{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TUTeMbMp2iqX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TUTeMbMp2iqX",
    "outputId": "1ec19e44-78c8-45c2-b88a-e44cbd984ca2"
   },
   "outputs": [],
   "source": [
    "!pip install qiskit\n",
    "!pip install qiskit-algorithms\n",
    "!pip install qiskit-machine-learning\n",
    "!pip install qiskit-Aer\n",
    "!pip install qiskit-qulacs\n",
    "!pip install matplotlib\n",
    "!pip install pylatexenc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "measured-liabilities",
   "metadata": {
    "id": "measured-liabilities"
   },
   "source": [
    "# Saving, Loading Qiskit Machine Learning Models and Continuous Training\n",
    "\n",
    "In the first part of this lecture we will cover how to:\n",
    "\n",
    "* Generate a simple dataset, split it into training/test datasets and plot them\n",
    "* Train and save a model\n",
    "* Load a saved model and resume training\n",
    "* Evaluate performance of models\n",
    "* PyTorch hybrid models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "speaking-glance",
   "metadata": {
    "id": "speaking-glance"
   },
   "source": [
    "First off, we start from the required imports. We'll heavily use SciKit-Learn on the data preparation step. In the next cell we also fix a random seed for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-cholesterol",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:50.850307318Z",
     "start_time": "2023-11-05T08:24:47.935479009Z"
    },
    "id": "exposed-cholesterol"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from qiskit.circuit.library import RealAmplitudes\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "algorithm_globals.random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-mileage",
   "metadata": {
    "id": "rural-mileage"
   },
   "source": [
    "We will be using two quantum simulators, in particular, two instances of the `Sampler` primitive. We'll start training on the first one, then will resume training on the second one. The approach shown in this tutorial can be used to train a model on a real hardware available on the cloud and then re-use the model for inference on a local simulator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-seating",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:51.153953565Z",
     "start_time": "2023-11-05T08:24:47.957105829Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "charming-seating",
    "outputId": "6e6469e7-1feb-4225-c1b5-25aec08670b7"
   },
   "outputs": [],
   "source": [
    "sampler1 = Sampler()\n",
    "\n",
    "sampler2 = Sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "careful-allowance",
   "metadata": {
    "id": "careful-allowance"
   },
   "source": [
    "## 1. Prepare a dataset\n",
    "\n",
    "Next step is to prepare a dataset. Here, we generate some data in the same way as in other tutorials. The difference is that we apply some transformations to the generated data. We generates `40` samples, each sample has `2` features, so our features is an array of shape `(40, 2)`. Labels are obtained by summing up features by columns and if the sum is more than `1` then this sample is labeled as `1` and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceramic-florida",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:51.283720991Z",
     "start_time": "2023-11-05T08:24:47.993471890Z"
    },
    "id": "ceramic-florida"
   },
   "outputs": [],
   "source": [
    "num_samples = 40\n",
    "num_features = 2\n",
    "features = 2 * algorithm_globals.random.random([num_samples, num_features]) - 1\n",
    "labels = 1 * (np.sum(features, axis=1) >= 0)  # in { 0,  1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-injury",
   "metadata": {
    "id": "reduced-injury"
   },
   "source": [
    "Then, we scale down our features into a range of `[0, 1]` by applying `MinMaxScaler` from SciKit-Learn. Model training convergence is better when this  transformation is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-director",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:51.413851559Z",
     "start_time": "2023-11-05T08:24:48.016851318Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dirty-director",
    "outputId": "d8bef8e9-44a1-4340-feb0-37abb472b373"
   },
   "outputs": [],
   "source": [
    "features = MinMaxScaler().fit_transform(features)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "julian-amount",
   "metadata": {
    "id": "julian-amount"
   },
   "source": [
    "Let's take a look at the features of the first `5` samples of our dataset after the transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-script",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:51.520038198Z",
     "start_time": "2023-11-05T08:24:48.039403019Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "thorough-script",
    "outputId": "2bc44cd4-27b8-4c49-c3fe-af0e3ecb13da"
   },
   "outputs": [],
   "source": [
    "features[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-aluminum",
   "metadata": {
    "id": "racial-aluminum"
   },
   "source": [
    "We choose `VQC` or Variational Quantum Classifier as a model we will train. This model, by default, takes one-hot encoded labels, so we have to transform the labels that are in the set of `{0, 1}` into one-hot representation. We employ SciKit-Learn for this transformation as well. Please note that the input array must be reshaped to `(num_samples, 1)` first. The `OneHotEncoder` encoder does not work with 1D arrays and our labels is a 1D array. In this case a user must decide either an array has only one feature(our case!) or has one sample. Also, by default the encoder returns sparse arrays, but for dataset plotting it is easier to have dense arrays, so we set `sparse` to `False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "understood-ukraine",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:51.680971565Z",
     "start_time": "2023-11-05T08:24:48.064216975Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "understood-ukraine",
    "outputId": "b232dba1-1aad-49b6-e63e-da7ba6141b29"
   },
   "outputs": [],
   "source": [
    "labels = OneHotEncoder(sparse_output=False).fit_transform(labels.reshape(-1, 1))\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-symbol",
   "metadata": {
    "id": "statewide-symbol"
   },
   "source": [
    "Let's take a look at the labels of the first `5` labels of the dataset. The labels should be one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "german-agreement",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:51.847895844Z",
     "start_time": "2023-11-05T08:24:48.086320894Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "german-agreement",
    "outputId": "49e35582-7957-4af6-8eac-ed3b80a83afe"
   },
   "outputs": [],
   "source": [
    "labels[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-toner",
   "metadata": {
    "id": "aquatic-toner"
   },
   "source": [
    "Now we split our dataset into two parts: a training dataset and a test one. As a rule of thumb, 80% of a full dataset should go into a training part and 20% into a test one. Our training dataset has `30` samples. The test dataset should be used only once, when a model is trained to verify how well the model behaves on unseen data. We employ `train_test_split` from SciKit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-ordinary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:51.981235065Z",
     "start_time": "2023-11-05T08:24:48.109926626Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "about-ordinary",
    "outputId": "b7dff033-5394-4d9d-cb7a-6623bb23259b"
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, train_size=30, random_state=algorithm_globals.random_seed\n",
    ")\n",
    "train_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-angel",
   "metadata": {
    "id": "critical-angel"
   },
   "source": [
    "Now it is time to see how our dataset looks like. Let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifty-scottish",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:53.978502816Z",
     "start_time": "2023-11-05T08:24:48.155068194Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "fifty-scottish",
    "outputId": "fbffc826-5ab7-4b9b-a918-2f89477f3d7a"
   },
   "outputs": [],
   "source": [
    "def plot_dataset():\n",
    "    plt.scatter(\n",
    "        train_features[np.where(train_labels[:, 0] == 0), 0],\n",
    "        train_features[np.where(train_labels[:, 0] == 0), 1],\n",
    "        marker=\"o\",\n",
    "        color=\"b\",\n",
    "        label=\"Label 0 train\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        train_features[np.where(train_labels[:, 0] == 1), 0],\n",
    "        train_features[np.where(train_labels[:, 0] == 1), 1],\n",
    "        marker=\"o\",\n",
    "        color=\"g\",\n",
    "        label=\"Label 1 train\",\n",
    "    )\n",
    "\n",
    "    plt.scatter(\n",
    "        test_features[np.where(test_labels[:, 0] == 0), 0],\n",
    "        test_features[np.where(test_labels[:, 0] == 0), 1],\n",
    "        marker=\"o\",\n",
    "        facecolors=\"w\",\n",
    "        edgecolors=\"b\",\n",
    "        label=\"Label 0 test\",\n",
    "    )\n",
    "    plt.scatter(\n",
    "        test_features[np.where(test_labels[:, 0] == 1), 0],\n",
    "        test_features[np.where(test_labels[:, 0] == 1), 1],\n",
    "        marker=\"o\",\n",
    "        facecolors=\"w\",\n",
    "        edgecolors=\"g\",\n",
    "        label=\"Label 1 test\",\n",
    "    )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "    plt.plot([1, 0], [0, 1], \"--\", color=\"black\")\n",
    "\n",
    "\n",
    "plot_dataset()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-depression",
   "metadata": {
    "id": "regulation-depression"
   },
   "source": [
    "On the plot above we see:\n",
    "\n",
    "* Solid <span style=\"color:blue\">blue</span> dots are the samples from the training dataset labeled as `0`\n",
    "* Empty <span style=\"color:blue\">blue</span> dots are the samples from the test dataset labeled as `0`\n",
    "* Solid <span style=\"color:green\">green</span> dots are the samples from the training dataset labeled as `1`\n",
    "* Empty <span style=\"color:green\">green</span> dots are the samples from the test dataset labeled as `1`\n",
    "\n",
    "We'll train our model using solid dots and verify it using empty dots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "egyptian-campaign",
   "metadata": {
    "id": "egyptian-campaign"
   },
   "source": [
    "## 2. Train a model and save it\n",
    "\n",
    "We'll train our model in two steps. On the first step we train our model in `20` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brief-lending",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:53.979960962Z",
     "start_time": "2023-11-05T08:24:48.448552843Z"
    },
    "id": "brief-lending"
   },
   "outputs": [],
   "source": [
    "maxiter = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-franklin",
   "metadata": {
    "id": "crude-franklin"
   },
   "source": [
    "Create an empty array for callback to store values of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integrated-palestinian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:53.980978327Z",
     "start_time": "2023-11-05T08:24:48.449290471Z"
    },
    "id": "integrated-palestinian"
   },
   "outputs": [],
   "source": [
    "objective_values = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-sherman",
   "metadata": {
    "id": "legendary-sherman"
   },
   "source": [
    "We re-use a callback function from the Neural Network Classifier & Regressor tutorial to plot iteration versus objective function value with some minor tweaks to plot objective values at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-apparel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:53.983171551Z",
     "start_time": "2023-11-05T08:24:48.449582707Z"
    },
    "id": "periodic-apparel"
   },
   "outputs": [],
   "source": [
    "# callback function that draws a live plot when the .fit() method is called\n",
    "def callback_graph(_, objective_value):\n",
    "    clear_output(wait=True)\n",
    "    objective_values.append(objective_value)\n",
    "\n",
    "    plt.title(\"Objective function value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "\n",
    "    stage1_len = np.min((len(objective_values), maxiter))\n",
    "    stage1_x = np.linspace(1, stage1_len, stage1_len)\n",
    "    stage1_y = objective_values[:stage1_len]\n",
    "\n",
    "    stage2_len = np.max((0, len(objective_values) - maxiter))\n",
    "    stage2_x = np.linspace(maxiter, maxiter + stage2_len - 1, stage2_len)\n",
    "    stage2_y = objective_values[maxiter : maxiter + stage2_len]\n",
    "\n",
    "    plt.plot(stage1_x, stage1_y, color=\"orange\")\n",
    "    plt.plot(stage2_x, stage2_y, color=\"purple\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "institutional-cyprus",
   "metadata": {
    "id": "institutional-cyprus"
   },
   "source": [
    "As mentioned above we train a `VQC` model and set `COBYLA` as an optimizer with a chosen value of the `maxiter` parameter. Then we evaluate performance of the model to see how well it was trained. Then we save this model for a file. On the second step we load this model and will continue to work with it.\n",
    "\n",
    "Here, we manually construct an ansatz to fix an initial point where to start optimization from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-impact",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:54.136747148Z",
     "start_time": "2023-11-05T08:24:48.486489150Z"
    },
    "id": "electronic-impact"
   },
   "outputs": [],
   "source": [
    "original_optimizer = COBYLA(maxiter=maxiter)\n",
    "\n",
    "ansatz = RealAmplitudes(num_features)\n",
    "initial_point = np.asarray([0.5] * ansatz.num_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-classroom",
   "metadata": {
    "id": "separated-classroom"
   },
   "source": [
    "We create a model and set a sampler to the first sampler we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-freeze",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:24:54.137343221Z",
     "start_time": "2023-11-05T08:24:48.486909482Z"
    },
    "id": "revolutionary-freeze"
   },
   "outputs": [],
   "source": [
    "original_classifier = VQC(\n",
    "    ansatz=ansatz, optimizer=original_optimizer, callback=callback_graph, sampler=sampler1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-mexican",
   "metadata": {
    "id": "minute-mexican"
   },
   "source": [
    "Now it is time to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-appointment",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "suited-appointment",
    "outputId": "2a892f09-405a-4148-e27b-1626b455dc63"
   },
   "outputs": [],
   "source": [
    "original_classifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-torture",
   "metadata": {
    "id": "revised-torture"
   },
   "source": [
    "Let's see how well our model performs after the first step of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-memphis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:01.951036644Z",
     "start_time": "2023-11-05T08:25:00.910455750Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "greek-memphis",
    "outputId": "f930031e-4b6f-4815-8076-bfce6652cbfe"
   },
   "outputs": [],
   "source": [
    "print(\"Train score\", original_classifier.score(train_features, train_labels))\n",
    "print(\"Test score \", original_classifier.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-moses",
   "metadata": {
    "id": "rental-moses"
   },
   "source": [
    "Next, we save the model. You may choose any file name you want. Please note that the `save` method does not append an extension if it is not specified in the file name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-interview",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:01.953154416Z",
     "start_time": "2023-11-05T08:25:01.131981222Z"
    },
    "id": "broadband-interview"
   },
   "outputs": [],
   "source": [
    "original_classifier.save(\"vqc_classifier.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-thread",
   "metadata": {
    "id": "sitting-thread"
   },
   "source": [
    "## 3. Load a model and continue training\n",
    "\n",
    "To load a model a user have to call a class method `load` of the corresponding model class. In our case it is `VQC`. We pass the same file name we used in the previous section where we saved our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-europe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:01.954871555Z",
     "start_time": "2023-11-05T08:25:01.158162917Z"
    },
    "id": "steady-europe"
   },
   "outputs": [],
   "source": [
    "loaded_classifier = VQC.load(\"vqc_classifier.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-shaft",
   "metadata": {
    "id": "reverse-shaft"
   },
   "source": [
    "Next, we want to alter the model in a way it can be trained further and on another simulator. To do so, we set the `warm_start` property. When it is set to `True` and `fit()` is called again the model uses weights from previous fit to start a new fit. We also set the `sampler` property of the underlying network to the second instance of the `Sampler` primitive we created in the beginning of the tutorial. Finally, we create and set a new optimizer with `maxiter` is set to `80`, so the total number of iterations is `100`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessible-cowboy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:01.955802287Z",
     "start_time": "2023-11-05T08:25:01.166674987Z"
    },
    "id": "accessible-cowboy"
   },
   "outputs": [],
   "source": [
    "loaded_classifier.warm_start = True\n",
    "loaded_classifier.neural_network.sampler = sampler2\n",
    "loaded_classifier.optimizer = COBYLA(maxiter=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-bruce",
   "metadata": {
    "id": "revised-bruce"
   },
   "source": [
    "Now we continue training our model from the state we finished in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-cyprus",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "metric-cyprus",
    "nbsphinx-thumbnail": {
     "output-index": 0
    },
    "outputId": "fdc99ab3-aedf-4cdb-af3f-a3ea5f93bcd9"
   },
   "outputs": [],
   "source": [
    "loaded_classifier.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-spread",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:33.767404530Z",
     "start_time": "2023-11-05T08:25:33.546478120Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bronze-spread",
    "outputId": "6bf1ce0a-49c9-4fc0-e515-e33855362ca0"
   },
   "outputs": [],
   "source": [
    "print(\"Train score\", loaded_classifier.score(train_features, train_labels))\n",
    "print(\"Test score\", loaded_classifier.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apparent-bloom",
   "metadata": {
    "id": "apparent-bloom"
   },
   "source": [
    "Let's see which data points were misclassified. First, we call `predict` to infer predicted values from the training and test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "catholic-norway",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:33.951577909Z",
     "start_time": "2023-11-05T08:25:33.702704249Z"
    },
    "id": "catholic-norway"
   },
   "outputs": [],
   "source": [
    "train_predicts = loaded_classifier.predict(train_features)\n",
    "test_predicts = loaded_classifier.predict(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-croatia",
   "metadata": {
    "id": "guided-croatia"
   },
   "source": [
    "Plot the whole dataset and the highlight the points that were classified incorrectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-handling",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:34.392010026Z",
     "start_time": "2023-11-05T08:25:33.893204544Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "tested-handling",
    "outputId": "25320f3b-2bd0-43d1-d66a-04a30f15d963"
   },
   "outputs": [],
   "source": [
    "# return plot to default figsize\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "plot_dataset()\n",
    "\n",
    "# plot misclassified data points\n",
    "plt.scatter(\n",
    "    train_features[np.all(train_labels != train_predicts, axis=1), 0],\n",
    "    train_features[np.all(train_labels != train_predicts, axis=1), 1],\n",
    "    s=200,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"r\",\n",
    "    linewidths=2,\n",
    ")\n",
    "plt.scatter(\n",
    "    test_features[np.all(test_labels != test_predicts, axis=1), 0],\n",
    "    test_features[np.all(test_labels != test_predicts, axis=1), 1],\n",
    "    s=200,\n",
    "    facecolors=\"none\",\n",
    "    edgecolors=\"r\",\n",
    "    linewidths=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-preference",
   "metadata": {
    "id": "genuine-preference"
   },
   "source": [
    "So, if you have a large dataset or a large model you can train it in multiple steps as shown in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac796a77fbe88859",
   "metadata": {
    "id": "ac796a77fbe88859"
   },
   "source": [
    "# PyTorch Workflow\n",
    "\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%203/01_a_pytorch_workflow.png?raw=1)\n",
    "\n",
    "You can have a more detailed look at PyTorch workflow [here](https://www.learnpytorch.io/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secondary-copying",
   "metadata": {
    "id": "secondary-copying"
   },
   "source": [
    "# Torch Connector and Hybrid QNNs\n",
    "\n",
    "This part introduces the `TorchConnector` class, and demonstrates how it allows for a natural integration of any `NeuralNetwork` from Qiskit Machine Learning into a PyTorch workflow. `TorchConnector` takes a `NeuralNetwork` and makes it available as a PyTorch `Module`. The resulting module can be seamlessly incorporated into PyTorch classical architectures and trained jointly without additional considerations, enabling the development and testing of novel **hybrid quantum-classical** machine learning architectures.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banned-helicopter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:34.445453142Z",
     "start_time": "2023-11-05T08:25:34.394712442Z"
    },
    "id": "banned-helicopter"
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear, CrossEntropyLoss, MSELoss\n",
    "from torch.optim import LBFGS\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "# Set seed for random generators\n",
    "algorithm_globals.random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-snapshot",
   "metadata": {
    "id": "unique-snapshot"
   },
   "source": [
    "## Part 1: Simple Classification & Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surgical-penetration",
   "metadata": {
    "id": "surgical-penetration"
   },
   "source": [
    "### 1. Classification\n",
    "\n",
    "First, we show how `TorchConnector` allows to train a Quantum `NeuralNetwork` to solve a classification tasks using PyTorch's automatic differentiation engine. In order to illustrate this, we will perform **binary classification** on a randomly generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-tragedy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:34.702110906Z",
     "start_time": "2023-11-05T08:25:34.399596108Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "secure-tragedy",
    "outputId": "8df3887a-d625-46e2-dfed-d1517f1ec3f6"
   },
   "outputs": [],
   "source": [
    "# Generate random dataset\n",
    "\n",
    "# Select dataset dimension (num_inputs) and size (num_samples)\n",
    "num_inputs = 2\n",
    "num_samples = 20\n",
    "\n",
    "# Generate random input coordinates (X) and binary labels (y)\n",
    "X = 2 * algorithm_globals.random.random([num_samples, num_inputs]) - 1\n",
    "y01 = 1 * (np.sum(X, axis=1) >= 0)  # in { 0,  1}, y01 will be used for SamplerQNN example\n",
    "y = 2 * y01 - 1  # in {-1, +1}, y will be used for EstimatorQNN example\n",
    "\n",
    "# Convert to torch Tensors\n",
    "X_ = Tensor(X)\n",
    "y01_ = Tensor(y01).reshape(len(y)).long()\n",
    "y_ = Tensor(y).reshape(len(y), 1)\n",
    "\n",
    "# Plot dataset\n",
    "for x, y_target in zip(X, y):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[1], \"bo\")\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], \"go\")\n",
    "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-rehabilitation",
   "metadata": {
    "id": "hazardous-rehabilitation"
   },
   "source": [
    "#### A. Classification with PyTorch and  `EstimatorQNN`\n",
    "\n",
    "Linking an `EstimatorQNN` to PyTorch is relatively straightforward. Here we illustrate this by using the `EstimatorQNN` constructed from a feature map and an ansatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-desperate",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:34.964484876Z",
     "start_time": "2023-11-05T08:25:34.710719261Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "fewer-desperate",
    "outputId": "ef1f3672-dde4-473f-f523-3abef8821fa0"
   },
   "outputs": [],
   "source": [
    "# Set up a circuit\n",
    "feature_map = ZZFeatureMap(num_inputs)\n",
    "ansatz = RealAmplitudes(num_inputs)\n",
    "qc = QuantumCircuit(num_inputs)\n",
    "qc.compose(feature_map, inplace=True)\n",
    "qc.compose(ansatz, inplace=True)\n",
    "qc.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-flavor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:35.000193199Z",
     "start_time": "2023-11-05T08:25:34.963939515Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "humanitarian-flavor",
    "outputId": "3e358b09-3bcd-4415-d33e-4c8e4bed87ab"
   },
   "outputs": [],
   "source": [
    "# Setup QNN\n",
    "qnn1 = EstimatorQNN(\n",
    "    circuit=qc, input_params=feature_map.parameters, weight_params=ansatz.parameters\n",
    ")\n",
    "\n",
    "# Set up PyTorch module\n",
    "# Note: If we don't explicitly declare the initial weights\n",
    "# they are chosen uniformly at random from [-1, 1].\n",
    "initial_weights = 0.1 * (2 * algorithm_globals.random.random(qnn1.num_weights) - 1)\n",
    "model1 = TorchConnector(qnn1, initial_weights=initial_weights)\n",
    "print(\"Initial weights: \", initial_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-grace",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:35.077231460Z",
     "start_time": "2023-11-05T08:25:34.968981142Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "likely-grace",
    "outputId": "6c69b429-85cd-4437-b003-3c65525386ef"
   },
   "outputs": [],
   "source": [
    "# Test with a single input\n",
    "model1(X_[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gorgeous-segment",
   "metadata": {
    "id": "gorgeous-segment"
   },
   "source": [
    "##### Optimizer\n",
    "The choice of optimizer for training any machine learning model can be crucial in determining the success of our training's outcome. When using `TorchConnector`, we get access to all of the optimizer algorithms defined in the [`torch.optim`] package ([link](https://pytorch.org/docs/stable/optim.html)). Some of the most famous algorithms used in popular machine learning architectures include *Adam*, *SGD*, or *Adagrad*. However, for this tutorial we will be using the L-BFGS algorithm (`torch.optim.LBFGS`), one of the most well know second-order optimization algorithms for numerical optimization.\n",
    "\n",
    "##### Loss Function\n",
    "As for the loss function, we can also take advantage of PyTorch's pre-defined modules from `torch.nn`, such as the [Cross-Entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) or [Mean Squared Error](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html) losses.\n",
    "\n",
    "\n",
    "**💡 Clarification :**\n",
    "In classical machine learning, the general rule of thumb is to apply a Cross-Entropy loss to classification tasks, and MSE loss to regression tasks. However, this recommendation is given under the assumption that the output of the classification network is a class probability value in the $[0, 1]$ range (usually this is achieved  through a Softmax layer). Because the following example for `EstimatorQNN` does not include such layer, and we don't apply any mapping to the output (the following section shows an example of application of parity mapping with `SamplerQNN`s), the QNN's output can take any value in the range $[-1, 1]$. In case you were wondering, this is the reason why this particular example uses MSELoss for classification despite it not being the norm (but we encourage you to experiment with different loss functions and see how they can impact training results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-extension",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:57.366444342Z",
     "start_time": "2023-11-05T08:25:34.999033415Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "following-extension",
    "outputId": "2823b143-3eaa-41a0-b6ca-f50bfbc7d5f0"
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss\n",
    "optimizer = LBFGS(model1.parameters())\n",
    "f_loss = MSELoss(reduction=\"sum\")\n",
    "\n",
    "# Start training\n",
    "model1.train()  # set model to training mode\n",
    "\n",
    "\n",
    "# Note from (https://pytorch.org/docs/stable/optim.html):\n",
    "# Some optimization algorithms such as LBFGS need to\n",
    "# reevaluate the function multiple times, so you have to\n",
    "# pass in a closure that allows them to recompute your model.\n",
    "# The closure should clear the gradients, compute the loss,\n",
    "# and return it.\n",
    "def closure():\n",
    "    optimizer.zero_grad()  # Initialize/clear gradients\n",
    "    loss = f_loss(model1(X_), y_)  # Evaluate loss function\n",
    "    loss.backward()  # Backward pass\n",
    "    print(loss.item())  # Print loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Run optimizer step4\n",
    "optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efficient-bangkok",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:57.786965909Z",
     "start_time": "2023-11-05T08:25:57.366197006Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "efficient-bangkok",
    "outputId": "d037d264-1b74-44c2-b2ba-99ba4b31b706"
   },
   "outputs": [],
   "source": [
    "# Evaluate model and compute accuracy\n",
    "model1.eval()\n",
    "y_predict = []\n",
    "for x, y_target in zip(X, y):\n",
    "    output = model1(Tensor(x))\n",
    "    y_predict += [np.sign(output.detach().numpy())[0]]\n",
    "\n",
    "print(\"Accuracy:\", sum(y_predict == y) / len(y))\n",
    "\n",
    "# Plot results\n",
    "# red == wrongly classified\n",
    "for x, y_target, y_p in zip(X, y, y_predict):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[1], \"bo\")\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], \"go\")\n",
    "    if y_target != y_p:\n",
    "        plt.scatter(x[0], x[1], s=200, facecolors=\"none\", edgecolors=\"r\", linewidths=2)\n",
    "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abstract-parish",
   "metadata": {
    "id": "abstract-parish"
   },
   "source": [
    "The red circles indicate wrongly classified data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-cross",
   "metadata": {
    "id": "typical-cross"
   },
   "source": [
    "#### B. Classification with PyTorch and `SamplerQNN`\n",
    "\n",
    "Linking a `SamplerQNN` to PyTorch requires a bit more attention than `EstimatorQNN`. Without the correct setup, backpropagation is not possible.\n",
    "\n",
    "In particular, we must make sure that we are returning a dense array of probabilities in the network's forward pass (`sparse=False`). This parameter is set up to `False` by default, so we just have to make sure that it has not been changed.\n",
    "\n",
    "**⚠️ Attention:**\n",
    "If we define a custom interpret function ( in the example: `parity`), we must remember to explicitly provide the desired output shape ( in the example: `2`). For more info on the initial parameter setup for `SamplerQNN`, please check out the [official qiskit documentation](https://qiskit.org/ecosystem/machine-learning/stubs/qiskit_machine_learning.neural_networks.SamplerQNN.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-operator",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:25:57.924057115Z",
     "start_time": "2023-11-05T08:25:57.800048212Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "present-operator",
    "outputId": "fe91a5e6-92f5-4a41-d06c-54d1a1162ba5"
   },
   "outputs": [],
   "source": [
    "# Define feature map and ansatz\n",
    "feature_map = ZZFeatureMap(num_inputs)\n",
    "ansatz = RealAmplitudes(num_inputs, entanglement=\"linear\", reps=1)\n",
    "\n",
    "# Define quantum circuit of num_qubits = input dim\n",
    "# Append feature map and ansatz\n",
    "qc = QuantumCircuit(num_inputs)\n",
    "qc.compose(feature_map, inplace=True)\n",
    "qc.compose(ansatz, inplace=True)\n",
    "\n",
    "# Define SamplerQNN and initial setup\n",
    "parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2  # optional interpret function\n",
    "output_shape = 2  # parity = 0, 1\n",
    "qnn2 = SamplerQNN(\n",
    "    circuit=qc,\n",
    "    input_params=feature_map.parameters,\n",
    "    weight_params=ansatz.parameters,\n",
    "    interpret=parity,\n",
    "    output_shape=output_shape,\n",
    ")\n",
    "\n",
    "# Set up PyTorch module\n",
    "# Reminder: If we don't explicitly declare the initial weights\n",
    "# they are chosen uniformly at random from [-1, 1].\n",
    "initial_weights = 0.1 * (2 * algorithm_globals.random.random(qnn2.num_weights) - 1)\n",
    "print(\"Initial weights: \", initial_weights)\n",
    "model2 = TorchConnector(qnn2, initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-reviewer",
   "metadata": {
    "id": "liquid-reviewer"
   },
   "source": [
    "For a reminder on optimizer and loss function choices, you can go back to [this section](#Optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-harvest",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:08.893832439Z",
     "start_time": "2023-11-05T08:25:57.833208374Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "marked-harvest",
    "outputId": "7abb165b-faa7-489d-d9a2-ae5e127d46e5"
   },
   "outputs": [],
   "source": [
    "# Define model, optimizer, and loss\n",
    "optimizer = LBFGS(model2.parameters())\n",
    "f_loss = CrossEntropyLoss()  # Our output will be in the [0,1] range\n",
    "\n",
    "# Start training\n",
    "model2.train()\n",
    "\n",
    "# Define LBFGS closure method (explained in previous section)\n",
    "def closure():\n",
    "    optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "    loss = f_loss(model2(X_), y01_)  # Calculate loss\n",
    "    loss.backward()  # Backward pass\n",
    "\n",
    "    print(loss.item())  # Print loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Run optimizer (LBFGS requires closure)\n",
    "optimizer.step(closure);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-electronics",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:09.328935177Z",
     "start_time": "2023-11-05T08:26:08.895707783Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 385
    },
    "id": "falling-electronics",
    "outputId": "ed98fc9f-13f5-4f3c-c591-5a5fa4e89d64"
   },
   "outputs": [],
   "source": [
    "# Evaluate model and compute accuracy\n",
    "model2.eval()\n",
    "y_predict = []\n",
    "for x in X:\n",
    "    output = model2(Tensor(x))\n",
    "    y_predict += [np.argmax(output.detach().numpy())]\n",
    "\n",
    "print(\"Accuracy:\", sum(y_predict == y01) / len(y01))\n",
    "\n",
    "# plot results\n",
    "# red == wrongly classified\n",
    "for x, y_target, y_ in zip(X, y01, y_predict):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[1], \"bo\")\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], \"go\")\n",
    "    if y_target != y_:\n",
    "        plt.scatter(x[0], x[1], s=200, facecolors=\"none\", edgecolors=\"r\", linewidths=2)\n",
    "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aboriginal-white",
   "metadata": {
    "id": "aboriginal-white"
   },
   "source": [
    "The red circles indicate wrongly classified data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-nicaragua",
   "metadata": {
    "id": "scheduled-nicaragua"
   },
   "source": [
    "### 2. Regression\n",
    "\n",
    "We use a model based on the `EstimatorQNN` to also illustrate how to perform a regression task. The chosen dataset in this case is randomly generated following a sine wave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-dubai",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:09.538259899Z",
     "start_time": "2023-11-05T08:26:09.311346346Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "amateur-dubai",
    "outputId": "b98e634a-88c9-47ee-936b-90c5772f437e"
   },
   "outputs": [],
   "source": [
    "# Generate random dataset\n",
    "\n",
    "num_samples = 20\n",
    "eps = 0.2\n",
    "lb, ub = -np.pi, np.pi\n",
    "f = lambda x: np.sin(x)\n",
    "\n",
    "X = (ub - lb) * algorithm_globals.random.random([num_samples, 1]) + lb\n",
    "y = f(X) + eps * (2 * algorithm_globals.random.random([num_samples, 1]) - 1)\n",
    "plt.plot(np.linspace(lb, ub), f(np.linspace(lb, ub)), \"r--\")\n",
    "plt.plot(X, y, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protected-genre",
   "metadata": {
    "id": "protected-genre"
   },
   "source": [
    "#### A. Regression with PyTorch and `EstimatorQNN`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-semiconductor",
   "metadata": {
    "id": "lovely-semiconductor"
   },
   "source": [
    "The network definition and training loop will be analogous to those of the classification task using `EstimatorQNN`. In this case, we define our own feature map and ansatz, but let's do it a little different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-adapter",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:09.589907699Z",
     "start_time": "2023-11-05T08:26:09.537991690Z"
    },
    "id": "brazilian-adapter"
   },
   "outputs": [],
   "source": [
    "# Construct simple feature map\n",
    "param_x = Parameter(\"x\")\n",
    "feature_map = QuantumCircuit(1, name=\"fm\")\n",
    "feature_map.ry(param_x, 0)\n",
    "\n",
    "# Construct simple parameterized ansatz\n",
    "param_y = Parameter(\"y\")\n",
    "ansatz = QuantumCircuit(1, name=\"vf\")\n",
    "ansatz.ry(param_y, 0)\n",
    "\n",
    "qc = QuantumCircuit(1)\n",
    "qc.compose(feature_map, inplace=True)\n",
    "qc.compose(ansatz, inplace=True)\n",
    "\n",
    "# Construct QNN\n",
    "qnn3 = EstimatorQNN(circuit=qc, input_params=[param_x], weight_params=[param_y])\n",
    "\n",
    "# Set up PyTorch module\n",
    "# Reminder: If we don't explicitly declare the initial weights\n",
    "# they are chosen uniformly at random from [-1, 1].\n",
    "initial_weights = 0.1 * (2 * algorithm_globals.random.random(qnn3.num_weights) - 1)\n",
    "model3 = TorchConnector(qnn3, initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waiting-competition",
   "metadata": {
    "id": "waiting-competition"
   },
   "source": [
    "For a reminder on optimizer and loss function choices, you can go back to [this section](#Optimizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-consciousness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:09.895641260Z",
     "start_time": "2023-11-05T08:26:09.554300810Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bibliographic-consciousness",
    "outputId": "937a097c-15d5-43c1-b97a-984ac19b09f2"
   },
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optimizer = LBFGS(model3.parameters())\n",
    "f_loss = MSELoss(reduction=\"sum\")\n",
    "\n",
    "# Start training\n",
    "model3.train()  # set model to training mode\n",
    "\n",
    "# Define objective function\n",
    "def closure():\n",
    "    optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "    loss = f_loss(model3(Tensor(X)), Tensor(y))  # Compute batch loss\n",
    "    loss.backward()  # Backward pass\n",
    "    print(loss.item())  # Print loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Run optimizer\n",
    "optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-happiness",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:10.207922098Z",
     "start_time": "2023-11-05T08:26:09.848942395Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "timely-happiness",
    "outputId": "99d8968e-c736-44f2-e90f-97ec929d958c"
   },
   "outputs": [],
   "source": [
    "# Plot target function\n",
    "plt.plot(np.linspace(lb, ub), f(np.linspace(lb, ub)), \"r--\")\n",
    "\n",
    "# Plot data\n",
    "plt.plot(X, y, \"bo\")\n",
    "\n",
    "# Plot fitted line\n",
    "model3.eval()\n",
    "y_ = []\n",
    "for x in np.linspace(lb, ub):\n",
    "    output = model3(Tensor([x]))\n",
    "    y_ += [output.detach().numpy()[0]]\n",
    "plt.plot(np.linspace(lb, ub), y_, \"g-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-georgia",
   "metadata": {
    "id": "individual-georgia"
   },
   "source": [
    "***\n",
    "\n",
    "## Part 2: MNIST Classification, Hybrid QNNs\n",
    "\n",
    "In this second part, we show how to leverage a hybrid quantum-classical neural network using `TorchConnector`, to perform a more complex image classification task on the MNIST handwritten digits dataset.\n",
    "\n",
    "For a more detailed (pre-`TorchConnector`) explanation on hybrid quantum-classical neural networks, you can check out the corresponding section in the [Qiskit Textbook](https://qiskit.org/textbook/ch-machine-learning/machine-learning-qiskit-pytorch.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "otherwise-military",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:10.238304125Z",
     "start_time": "2023-11-05T08:26:10.196454486Z"
    },
    "id": "otherwise-military"
   },
   "outputs": [],
   "source": [
    "# Additional torch-related imports\n",
    "import torch\n",
    "from torch import cat, no_grad, manual_seed\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from torch.nn import (\n",
    "    Module,\n",
    "    Conv2d,\n",
    "    Linear,\n",
    "    Dropout2d,\n",
    "    NLLLoss,\n",
    "    MaxPool2d,\n",
    "    Flatten,\n",
    "    Sequential,\n",
    "    ReLU,\n",
    ")\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-encounter",
   "metadata": {
    "id": "bronze-encounter"
   },
   "source": [
    "### Step 1: Defining Data-loaders for train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parliamentary-middle",
   "metadata": {
    "id": "parliamentary-middle"
   },
   "source": [
    "We take advantage of the `torchvision` [API](https://pytorch.org/vision/stable/datasets.html) to directly load a subset of the [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) and define torch `DataLoader`s ([link](https://pytorch.org/docs/stable/data.html)) for train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-charlotte",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:54.800166468Z",
     "start_time": "2023-11-05T08:26:10.201309927Z"
    },
    "id": "worthy-charlotte"
   },
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "# Set train shuffle seed (for reproducibility)\n",
    "manual_seed(42)\n",
    "\n",
    "batch_size = 1\n",
    "n_samples = 100  # We will concentrate on the first 100 samples\n",
    "\n",
    "# Use pre-defined torchvision function to load MNIST train data\n",
    "X_train = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "# Filter out labels (originally 0-9), leaving only labels 0 and 1\n",
    "idx = np.append(\n",
    "    np.where(X_train.targets == 0)[0][:n_samples], np.where(X_train.targets == 1)[0][:n_samples]\n",
    ")\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = X_train.targets[idx]\n",
    "\n",
    "# Define torch dataloader with filtered data\n",
    "train_loader = DataLoader(X_train, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "completed-spring",
   "metadata": {
    "id": "completed-spring"
   },
   "source": [
    "If we perform a quick visualization we can see that the train dataset consists of images of handwritten 0s and 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-bibliography",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:55.149342773Z",
     "start_time": "2023-11-05T08:26:54.747535622Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "medieval-bibliography",
    "outputId": "9a5bfce8-dc76-4d42-e1dc-ab3cc065374d"
   },
   "outputs": [],
   "source": [
    "n_samples_show = 6\n",
    "\n",
    "data_iter = iter(train_loader)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "while n_samples_show > 0:\n",
    "    images, targets = data_iter.__next__()\n",
    "\n",
    "    axes[n_samples_show - 1].imshow(images[0, 0].numpy().squeeze(), cmap=\"gray\")\n",
    "    axes[n_samples_show - 1].set_xticks([])\n",
    "    axes[n_samples_show - 1].set_yticks([])\n",
    "    axes[n_samples_show - 1].set_title(\"Labeled: {}\".format(targets[0].item()))\n",
    "\n",
    "    n_samples_show -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-chuck",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:55.171036308Z",
     "start_time": "2023-11-05T08:26:55.151683758Z"
    },
    "id": "structural-chuck"
   },
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "# -------------\n",
    "\n",
    "# Set test shuffle seed (for reproducibility)\n",
    "# manual_seed(5)\n",
    "\n",
    "n_samples = 50\n",
    "\n",
    "# Use pre-defined torchvision function to load MNIST test data\n",
    "X_test = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "\n",
    "# Filter out labels (originally 0-9), leaving only labels 0 and 1\n",
    "idx = np.append(\n",
    "    np.where(X_test.targets == 0)[0][:n_samples], np.where(X_test.targets == 1)[0][:n_samples]\n",
    ")\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = X_test.targets[idx]\n",
    "\n",
    "# Define torch dataloader with filtered data\n",
    "test_loader = DataLoader(X_test, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-morris",
   "metadata": {
    "id": "abroad-morris"
   },
   "source": [
    "### Step 2: Defining the QNN and Hybrid Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-tokyo",
   "metadata": {
    "id": "super-tokyo"
   },
   "source": [
    "This second step shows the power of the `TorchConnector`. After defining our quantum neural network layer (in this case, a `EstimatorQNN`), we can embed it into a layer in our torch `Module` by initializing a torch connector as `TorchConnector(qnn)`.\n",
    "\n",
    "**⚠️ Attention:**\n",
    "In order to have an adequate gradient backpropagation in hybrid models,  we MUST set the initial parameter `input_gradients` to TRUE during the qnn initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-purse",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:55.260568697Z",
     "start_time": "2023-11-05T08:26:55.162578076Z"
    },
    "id": "urban-purse"
   },
   "outputs": [],
   "source": [
    "# Define and create QNN\n",
    "def create_qnn():\n",
    "    feature_map = ZZFeatureMap(2)\n",
    "    ansatz = RealAmplitudes(2, reps=1)\n",
    "    qc = QuantumCircuit(2)\n",
    "    qc.compose(feature_map, inplace=True)\n",
    "    qc.compose(ansatz, inplace=True)\n",
    "\n",
    "    # REMEMBER TO SET input_gradients=True FOR ENABLING HYBRID GRADIENT BACKPROP\n",
    "    qnn = EstimatorQNN(\n",
    "        circuit=qc,\n",
    "        input_params=feature_map.parameters,\n",
    "        weight_params=ansatz.parameters,\n",
    "        input_gradients=True,\n",
    "    )\n",
    "    return qnn\n",
    "\n",
    "\n",
    "qnn4 = create_qnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-productivity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:26:55.262850682Z",
     "start_time": "2023-11-05T08:26:55.189695839Z"
    },
    "id": "exclusive-productivity"
   },
   "outputs": [],
   "source": [
    "# Define torch NN module\n",
    "\n",
    "\n",
    "class Net(Module):\n",
    "    def __init__(self, qnn):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2d(1, 2, kernel_size=5)\n",
    "        self.conv2 = Conv2d(2, 16, kernel_size=5)\n",
    "        self.dropout = Dropout2d()\n",
    "        self.fc1 = Linear(256, 64)\n",
    "        self.fc2 = Linear(64, 2)  # 2-dimensional input to QNN\n",
    "        self.qnn = TorchConnector(qnn)  # Apply torch connector, weights chosen\n",
    "        # uniformly at random from interval [-1,1].\n",
    "        self.fc3 = Linear(1, 1)  # 1-dimensional output from QNN\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = self.qnn(x)  # apply QNN\n",
    "        x = self.fc3(x)\n",
    "        return cat((x, 1 - x), -1)\n",
    "\n",
    "\n",
    "model4 = Net(qnn4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-specific",
   "metadata": {
    "id": "academic-specific"
   },
   "source": [
    "### Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precious-career",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:37.968744291Z",
     "start_time": "2023-11-05T08:26:55.200555403Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "precious-career",
    "outputId": "6ea07b02-a0a2-4e61-cca3-04020a0500cd"
   },
   "outputs": [],
   "source": [
    "# Define model, optimizer, and loss function\n",
    "optimizer = optim.Adam(model4.parameters(), lr=0.001)\n",
    "loss_func = NLLLoss()\n",
    "\n",
    "# Start training\n",
    "epochs = 10  # Set number of epochs\n",
    "loss_list = []  # Store loss history\n",
    "model4.train()  # Set model to training mode\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad(set_to_none=True)  # Initialize gradient\n",
    "        output = model4(data)  # Forward pass\n",
    "        loss = loss_func(output, target)  # Calculate loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Optimize weights\n",
    "        total_loss.append(loss.item())  # Store loss\n",
    "    loss_list.append(sum(total_loss) / len(total_loss))\n",
    "    print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(100.0 * (epoch + 1) / epochs, loss_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-stationery",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:38.204778073Z",
     "start_time": "2023-11-05T08:28:37.981117763Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "spoken-stationery",
    "outputId": "acdd4f03-2a27-42ba-e0ca-b4f70855ca7d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot loss convergence\n",
    "plt.plot(loss_list)\n",
    "plt.title(\"Hybrid NN Training Convergence\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.ylabel(\"Neg. Log Likelihood Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-closure",
   "metadata": {
    "id": "physical-closure"
   },
   "source": [
    "Now we'll save the trained model, just to show how a hybrid model can be saved and re-used later for inference. To save and load hybrid models, when using the TorchConnector, follow the PyTorch recommendations of saving and loading the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-bread",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:38.235513075Z",
     "start_time": "2023-11-05T08:28:38.205650580Z"
    },
    "id": "regulation-bread"
   },
   "outputs": [],
   "source": [
    "torch.save(model4.state_dict(), \"model4.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pacific-flour",
   "metadata": {
    "id": "pacific-flour"
   },
   "source": [
    "### Step 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-tribe",
   "metadata": {
    "id": "fabulous-tribe"
   },
   "source": [
    "We start from recreating the model and loading the state from the previously saved file. You create a QNN layer using another simulator or a real hardware. So, you can train a model on real hardware available on the cloud and then for inference use a simulator or vice verse. For a sake of simplicity we create a new quantum neural network in the same way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prospective-flooring",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:38.495156006Z",
     "start_time": "2023-11-05T08:28:38.214007977Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "prospective-flooring",
    "outputId": "69ee3254-3a84-4469-a8e5-5d66a5917d31"
   },
   "outputs": [],
   "source": [
    "qnn5 = create_qnn()\n",
    "model5 = Net(qnn5)\n",
    "model5.load_state_dict(torch.load(\"model4.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spectacular-conservative",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:39.335914399Z",
     "start_time": "2023-11-05T08:28:38.255400567Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spectacular-conservative",
    "outputId": "cffbcd63-e13d-407a-8fea-fdb50be18723"
   },
   "outputs": [],
   "source": [
    "model5.eval()  # set model to evaluation mode\n",
    "with no_grad():\n",
    "\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        output = model5(data)\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        loss = loss_func(output, target)\n",
    "        total_loss.append(loss.item())\n",
    "\n",
    "    print(\n",
    "        \"Performance on test data:\\n\\tLoss: {:.4f}\\n\\tAccuracy: {:.1f}%\".format(\n",
    "            sum(total_loss) / len(total_loss), correct / len(test_loader) / batch_size * 100\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-brave",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:39.771048096Z",
     "start_time": "2023-11-05T08:28:39.337562899Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "color-brave",
    "outputId": "367c6535-b969-403c-ca8d-eb70587c46fa",
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot predicted labels\n",
    "\n",
    "n_samples_show = 6\n",
    "count = 0\n",
    "fig, axes = plt.subplots(nrows=1, ncols=n_samples_show, figsize=(10, 3))\n",
    "\n",
    "model5.eval()\n",
    "with no_grad():\n",
    "    for batch_idx, (data, target) in enumerate(test_loader):\n",
    "        if count == n_samples_show:\n",
    "            break\n",
    "        output = model5(data[0:1])\n",
    "        if len(output.shape) == 1:\n",
    "            output = output.reshape(1, *output.shape)\n",
    "\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        axes[count].imshow(data[0].numpy().squeeze(), cmap=\"gray\")\n",
    "\n",
    "        axes[count].set_xticks([])\n",
    "        axes[count].set_yticks([])\n",
    "        axes[count].set_title(\"Predicted {}\".format(pred.item()))\n",
    "\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-visibility",
   "metadata": {
    "id": "prompt-visibility"
   },
   "source": [
    "🎉🎉🎉🎉\n",
    "**You are now able to experiment with your own hybrid datasets and architectures using Qiskit Machine Learning.**\n",
    "**Good Luck!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d1776820d2b89",
   "metadata": {
    "id": "104d1776820d2b89"
   },
   "source": [
    "# PyTorch qGAN Implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e73b72b",
   "metadata": {
    "id": "7e73b72b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "A generative adversarial network (GAN) has two parts:\n",
    "\n",
    "* The generator learns to generate plausible data. The generated instances become negative training examples for the discriminator.\n",
    "* The discriminator learns to distinguish the generator's fake data from real data. The discriminator penalizes the generator for producing implausible results.\n",
    "\n",
    "When training begins, the generator produces obviously fake data, and the discriminator quickly learns to tell that it's fake:\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%203/bad_gan.svg?raw=1)\n",
    "As training progresses, the generator gets closer to producing output that can fool the discriminator:\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%203/ok_gan.svg?raw=1)\n",
    "Finally, if generator training goes well, the discriminator gets worse at telling the difference between real and fake. It starts to classify fake data as real, and its accuracy decreases.\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%203/good_gan.svg?raw=1)\n",
    "Here's a picture of the whole system:\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%203/gan_diagram.svg?raw=1)\n",
    "\n",
    "Both the generator and the discriminator are neural networks. The generator output is connected directly to the discriminator input. Through backpropagation, the discriminator's classification provides a signal that the generator uses to update its weights.\n",
    "\n",
    "Let's explain the pieces of this system in greater detail.\n",
    "\n",
    "### The Discriminator\n",
    "\n",
    "The discriminator in a GAN is simply a classifier. It tries to distinguish real data from the data created by the generator. It could use any network architecture appropriate to the type of data it's classifying.\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%203/gan_diagram_discriminator.svg?raw=1)\n",
    "\n",
    "The discriminator's training data comes from two sources:\n",
    "\n",
    "* **Real data** instances, such as real pictures of people. The discriminator uses these instances as positive examples during training.\n",
    "* **Fake data** instances created by the generator. The discriminator uses these instances as negative examples during training.\n",
    "\n",
    "In the above figure, the two \"Sample\" boxes represent these two data sources feeding into the discriminator. During discriminator training the generator does not train. Its weights remain constant while it produces examples for the discriminator to train on.\n",
    "\n",
    "#### Training the Discriminator\n",
    "\n",
    "The discriminator connects to two loss functions. During discriminator training, the discriminator ignores the generator loss and just uses the discriminator loss. We use the generator loss during generator training, as described in the next section.\n",
    "\n",
    "During discriminator training:\n",
    "1. The discriminator classifies both real data and fake data from the generator.\n",
    "2. The discriminator loss penalizes the discriminator for misclassifying a real instance as fake or a fake instance as real.\n",
    "3. The discriminator updates its weights through backpropagation from the discriminator loss through the discriminator network.\n",
    "### The Generator\n",
    "The generator part of a GAN learns to create fake data by incorporating feedback from the discriminator. It learns to make the discriminator classify its output as real.\n",
    "\n",
    "Generator training requires tighter integration between the generator and the discriminator than discriminator training requires. The portion of the GAN that trains the generator includes:\n",
    "* random input\n",
    "* generator network, which transforms the random input into a data instance\n",
    "* discriminator network, which classifies the generated data\n",
    "* discriminator output\n",
    "* generator loss, which penalizes the generator for failing to fool the discriminator\n",
    "\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%203/gan_diagram_generator.svg?raw=1)\n",
    "\n",
    "Neural networks need some form of input. Normally we input data that we want to do something with, like an instance that we want to classify or make a prediction about. But what do we use as input for a network that outputs entirely new data instances?\n",
    "\n",
    "In its most basic form, a GAN takes random noise as its input. The generator then transforms this noise into a meaningful output. By introducing noise, we can get the GAN to produce a wide variety of data, sampling from different places in the target distribution.\n",
    "\n",
    "Experiments suggest that the distribution of the noise doesn't matter much, so we can choose something that's easy to sample from, like a uniform distribution. For convenience the space from which the noise is sampled is usually of smaller dimension than the dimensionality of the output space.\n",
    "\n",
    "#### Using the Discriminator to Train the Generator\n",
    "\n",
    "To train a neural net, we alter the net's weights to reduce the error or loss of its output. In our GAN, however, the generator is not directly connected to the loss that we're trying to affect. The generator feeds into the discriminator net, and the discriminator produces the output we're trying to affect. The generator loss penalizes the generator for producing a sample that the discriminator network classifies as fake.\n",
    "\n",
    "This extra chunk of network must be included in backpropagation. Backpropagation adjusts each weight in the right direction by calculating the weight's impact on the output — how the output would change if you changed the weight. But the impact of a generator weight depends on the impact of the discriminator weights it feeds into. So backpropagation starts at the output and flows back through the discriminator into the generator.\n",
    "\n",
    "At the same time, we don't want the discriminator to change during generator training. Trying to hit a moving target would make a hard problem even harder for the generator.\n",
    "\n",
    "So we train the generator with the following procedure:\n",
    "1. Sample random noise.\n",
    "2. Produce generator output from sampled random noise.\n",
    "3. Get discriminator \"Real\" or \"Fake\" classification for generator output.\n",
    "4. Calculate loss from discriminator classification.\n",
    "5. Backpropagate through both the discriminator and generator to obtain gradients.\n",
    "6. Use gradients to change only the generator weights.\n",
    "\n",
    "This is one iteration of generator training. In the next section we'll see how to juggle the training of both the generator and the discriminator.\n",
    "\n",
    "### GAN Training\n",
    "\n",
    "Because a GAN contains two separately trained networks, its training algorithm must address two complications:\n",
    "* GANs must juggle two different kinds of training (generator and discriminator).\n",
    "* GAN convergence is hard to identify.\n",
    "\n",
    "#### Alternating Training\n",
    "\n",
    "The generator and the discriminator have different training processes. So how do we train the GAN as a whole?\n",
    "\n",
    "GAN training proceeds in alternating periods:\n",
    "1. The discriminator trains for one or more epochs.\n",
    "2. The generator trains for one or more epochs.\n",
    "3. Repeat steps 1 and 2 to continue to train the generator and discriminator networks.\n",
    "\n",
    "We keep the generator constant during the discriminator training phase. As discriminator training tries to figure out how to distinguish real data from fake, it has to learn how to recognize the generator's flaws. That's a different problem for a thoroughly trained generator than it is for an untrained generator that produces random output.\n",
    "\n",
    "Similarly, we keep the discriminator constant during the generator training phase. Otherwise the generator would be trying to hit a moving target and might never converge.\n",
    "\n",
    "It's this back and forth that allows GANs to tackle otherwise intractable generative problems. We get a toehold in the difficult generative problem by starting with a much simpler classification problem. Conversely, if you can't train a classifier to tell the difference between real and generated data even for the initial random generator output, you can't get the GAN training started.\n",
    "\n",
    "#### Convergence\n",
    "\n",
    "As the generator improves with training, the discriminator performance gets worse because the discriminator can't easily tell the difference between real and fake. If the generator succeeds perfectly, then the discriminator has a 50% accuracy. In effect, the discriminator flips a coin to make its prediction.\n",
    "\n",
    "This progression poses a problem for convergence of the GAN as a whole: the discriminator feedback gets less meaningful over time. If the GAN continues training past the point when the discriminator is giving completely random feedback, then the generator starts to train on junk feedback, and its own quality may collapse.\n",
    "\n",
    "For a GAN, convergence is often a fleeting, rather than stable, state.\n",
    "\n",
    "### hybird qGAN\n",
    "\n",
    "The first model we will consider \\[1\\] is a hybrid quantum-classical algorithm used for generative modeling tasks. The algorithm uses the interplay of a quantum generator $G_{\\theta}$, i.e., an ansatz (parametrized quantum circuit), and a classical discriminator $D_{\\phi}$, a neural network, to learn the underlying probability distribution given training data.\n",
    "\n",
    "The generator and discriminator are trained in alternating optimization steps, where the generator aims at generating probabilities that will be classified by the discriminator as training data values (i.e, probabilities from the real training distribution), and the discriminator tries to differentiate between original distribution and probabilities from the generator (in other words, telling apart the real and generated distributions). The final goal is for the quantum generator to learn a representation for the target probability distribution.\n",
    "The trained quantum generator can, thus, be used to load a quantum state which is an approximate model of the target distribution.\n",
    "\n",
    "**References:**\n",
    "\n",
    "\\[1\\] Zoufal et al., [Quantum Generative Adversarial Networks for learning and loading random distributions](https://www.nature.com/articles/s41534-019-0223-2)\n",
    "\n",
    "### 1.1. qGANs for Loading Random Distributions\n",
    "\n",
    "Given $k$-dimensional data samples, we employ a quantum Generative Adversarial Network (qGAN) to learn a random distribution and to load it directly into a quantum state:\n",
    "\n",
    "$$ \\big| g_{\\theta}\\rangle = \\sum_{j=0}^{2^n-1} \\sqrt{p_{\\theta}^{j}}\\big| j \\rangle $$\n",
    "\n",
    "where $p_{\\theta}^{j}$ describe the occurrence probabilities of the basis states $\\big| j\\rangle$.\n",
    "\n",
    "The aim of the qGAN training is to generate a state $\\big| g_{\\theta}\\rangle$ where $p_{\\theta}^{j}$, for $j\\in \\left\\{0, \\ldots, {2^n-1} \\right\\}$, describe a probability distribution that is close to the distribution underlying the training data $X=\\left\\{x^0, \\ldots, x^{k-1} \\right\\}$.\n",
    "\n",
    "For further details please refer to [Quantum Generative Adversarial Networks for Learning and Loading Random Distributions](https://arxiv.org/abs/1904.00043) _Zoufal, Lucchi, Woerner_ \\[2019\\].\n",
    "\n",
    "For an example of how to use a trained qGAN in an application, the pricing of financial derivatives, please see the\n",
    "[Option Pricing with qGANs](https://qiskit.org/ecosystem/finance/tutorials/10_qgan_option_pricing.html) tutorial.\n",
    "\n",
    "## 2. Data and Representation\n",
    "\n",
    "First, we need to load our training data $X$.\n",
    "\n",
    "In this tutorial, the training data is given by a 2D multivariate normal distribution.\n",
    "\n",
    "The goal of the generator is to learn how to represent such distribution, and the trained generator should correspond to an $n$-qubit quantum state\n",
    "\\begin{equation}\n",
    "|g_{\\text{trained}}\\rangle=\\sum\\limits_{j=0}^{k-1}\\sqrt{p_{j}}|x_{j}\\rangle,\n",
    "\\end{equation}\n",
    "where the basis states $|x_{j}\\rangle$ represent the data items in the training data set\n",
    "$X={x_0, \\ldots, x_{k-1}}$ with $k\\leq 2^n$ and $p_j$ refers to the sampling probability\n",
    "of $|x_{j}\\rangle$.\n",
    "\n",
    "To facilitate this representation, we need to map the samples from the multivariate\n",
    "normal distribution to discrete values. The number of values that can be represented\n",
    "depends on the number of qubits used for the mapping.\n",
    "Hence, the data resolution is defined by the number of qubits.\n",
    "If we use $3$ qubits to represent one feature, we have $2^3 = 8$ discrete values.\n",
    "\n",
    "We first begin by fixing seeds in the random number generators for reproducibility of the outcome in this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c320ae41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:39.796417593Z",
     "start_time": "2023-11-05T08:28:39.770104673Z"
    },
    "id": "c320ae41",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "\n",
    "algorithm_globals.random_seed = 123456\n",
    "_ = torch.manual_seed(123456)  # suppress output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c20a1",
   "metadata": {
    "id": "984c20a1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We fix the number of dimensions, the discretization number and compute the number of qubits required as $2^3 = 8$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3113afa3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:39.799433160Z",
     "start_time": "2023-11-05T08:28:39.774899336Z"
    },
    "id": "3113afa3"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_dim = 2\n",
    "num_discrete_values = 8\n",
    "num_qubits = num_dim * int(np.log2(num_discrete_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80f3106",
   "metadata": {
    "id": "a80f3106"
   },
   "source": [
    "Then, we prepare a discrete distribution from the continuous 2D normal distribution. We evaluate the continuous probability density function (PDF) on the grid $(-2, 2)^2$ with a discretization of $8$ values per feature. Thus, we have $64$ values of the PDF. Since this will be a discrete distribution we normalize the obtained probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86aa64bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:39.886333305Z",
     "start_time": "2023-11-05T08:28:39.795055053Z"
    },
    "id": "86aa64bc"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "coords = np.linspace(-2, 2, num_discrete_values)\n",
    "rv = multivariate_normal(mean=[0.0, 0.0], cov=[[1, 0], [0, 1]], seed=algorithm_globals.random_seed)\n",
    "grid_elements = np.transpose([np.tile(coords, len(coords)), np.repeat(coords, len(coords))])\n",
    "prob_data = rv.pdf(grid_elements)\n",
    "prob_data = prob_data / np.sum(prob_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faaf81b",
   "metadata": {
    "id": "4faaf81b"
   },
   "source": [
    "Let's visualize our distribution. It is a nice bell-shaped bivariate normal distribution on a discrete grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c40ba1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:40.110240745Z",
     "start_time": "2023-11-05T08:28:39.856568339Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "0c40ba1b",
    "outputId": "1fbab701-a425-44d8-a799-0c969e0159b8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "mesh_x, mesh_y = np.meshgrid(coords, coords)\n",
    "grid_shape = (num_discrete_values, num_discrete_values)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 9), subplot_kw={\"projection\": \"3d\"})\n",
    "prob_grid = np.reshape(prob_data, grid_shape)\n",
    "surf = ax.plot_surface(mesh_x, mesh_y, prob_grid, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53f689a",
   "metadata": {
    "id": "c53f689a",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Definitions of the Neural Networks\n",
    "In this section we define two neural networks as described above:\n",
    "\n",
    "- A quantum generator as a quantum neural network.\n",
    "- A classical discriminator as a PyTorch-based neural network.\n",
    "\n",
    "### 3.1. Definition of the quantum neural network ansatz\n",
    "\n",
    "Now, we define the parameterized quantum circuit $G\\left(\\boldsymbol{\\theta}\\right)$ with $\\boldsymbol{\\theta} = {\\theta_1, ..., \\theta_k}$ which will be used in our quantum generator.\n",
    "\n",
    "To implement the quantum generator, we choose a hardware efficient ansatz with $6$ repetitions. The ansatz implements $R_Y$, $R_Z$ rotations and $CX$ gates which takes a uniform distribution as an input state. Notably, for $k>1$ the generator's parameters must be chosen carefully. For example, the circuit depth should be more than $1$ because higher circuit depths enable the representation of more complex structures. Here, we construct quite a deep circuit with a large number of parameters to be able to adequately capture and represent the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d05c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:40.237628798Z",
     "start_time": "2023-11-05T08:28:40.113877134Z"
    },
    "id": "73d05c8e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit.library import EfficientSU2\n",
    "\n",
    "qc = QuantumCircuit(num_qubits)\n",
    "qc.h(qc.qubits)\n",
    "\n",
    "ansatz = EfficientSU2(num_qubits, reps=6)\n",
    "qc.compose(ansatz, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59eedc8",
   "metadata": {
    "id": "c59eedc8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's draw our circuit and see what it looks like. On the plot we may notice a pattern that appears $6$ times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fa1f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:42.418799089Z",
     "start_time": "2023-11-05T08:28:40.157906781Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    },
    "id": "fd8fa1f1",
    "outputId": "901f2b87-6a7e-43fb-b7fe-8f81c2c00df6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "qc.decompose().draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40748c2d",
   "metadata": {
    "id": "40748c2d"
   },
   "source": [
    "Let's print the number of trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724cbe0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:42.428788241Z",
     "start_time": "2023-11-05T08:28:42.420647193Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0724cbe0",
    "outputId": "889bb44c-ddc0-4702-9093-d12ef0a10060"
   },
   "outputs": [],
   "source": [
    "qc.num_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648bcf4",
   "metadata": {
    "id": "e648bcf4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.2. Definition of the quantum generator\n",
    "\n",
    "We start defining the generator by creating a sampler for the ansatz. The reference implementation is a statevector-based implementation, thus it returns exact probabilities as a result of circuit execution. We add the `shots` parameter to add some noise to the results. In this case the implementation samples probabilities from the multinomial distribution constructed from the measured quasi probabilities. And as usual we fix the seed for reproducibility purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5cf2c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:42.451975923Z",
     "start_time": "2023-11-05T08:28:42.425859537Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fd5cf2c6",
    "outputId": "3fc0cee6-de3b-470e-ee11-0587915d42f7"
   },
   "outputs": [],
   "source": [
    "from qiskit.primitives import Sampler\n",
    "\n",
    "shots = 10000\n",
    "sampler = Sampler(options={\"shots\": shots, \"seed\": algorithm_globals.random_seed})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ebf733",
   "metadata": {
    "id": "72ebf733"
   },
   "source": [
    "Next, we define a function that creates the quantum generator from a given parameterized quantum circuit. Inside this function we create a neural network that returns the quasi probability distribution evaluated by the underlying Sampler. We fix `initial_weights` for reproducibility purposes. In the end we wrap the created quantum neural network in `TorchConnector` to make use of PyTorch-based training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98adb765",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:42.808830041Z",
     "start_time": "2023-11-05T08:28:42.438574072Z"
    },
    "id": "98adb765",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "\n",
    "\n",
    "def create_generator() -> TorchConnector:\n",
    "    qnn = SamplerQNN(\n",
    "        circuit=qc,\n",
    "        sampler=sampler,\n",
    "        input_params=[],\n",
    "        weight_params=qc.parameters,\n",
    "        sparse=False,\n",
    "    )\n",
    "\n",
    "    initial_weights = algorithm_globals.random.random(qc.num_parameters)\n",
    "    return TorchConnector(qnn, initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dade79",
   "metadata": {
    "id": "41dade79",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 3.3. Definition of the classical discriminator\n",
    "\n",
    "Next, we define a PyTorch-based classical neural network that represents the classical discriminator. The underlying gradients can be automatically computed with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afca2050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:42.814035803Z",
     "start_time": "2023-11-05T08:28:42.446007750Z"
    },
    "id": "afca2050",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.linear_input = nn.Linear(input_size, 20)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2)\n",
    "        self.linear20 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear_input(input)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.linear20(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61674109",
   "metadata": {
    "id": "61674109"
   },
   "source": [
    "### 3.4. Create a generator and a discriminator\n",
    "\n",
    "Now we create a generator and a discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501cac6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:42.859047950Z",
     "start_time": "2023-11-05T08:28:42.487286389Z"
    },
    "id": "501cac6e"
   },
   "outputs": [],
   "source": [
    "generator = create_generator()\n",
    "discriminator = Discriminator(num_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87998e",
   "metadata": {
    "id": "4b87998e",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Setting up the Training Loop\n",
    "In this section we set up:\n",
    "\n",
    "- A loss function for the generator and discriminator.\n",
    "- Optimizers for both.\n",
    "- A utility plotting function to visualize training process.\n",
    "\n",
    "### 4.1. Definition of the loss functions\n",
    "We want to train the generator and the discriminator with binary cross entropy as the loss function:\n",
    "$$L\\left(\\boldsymbol{\\theta}\\right)=\\sum_jp_j\\left(\\boldsymbol{\\theta}\\right)\\left[y_j\\log(x_j) + (1-y_j)\\log(1-x_j)\\right],$$\n",
    "where $x_j$ refers to a data sample and $y_j$ to the corresponding label.\n",
    "\n",
    "Since PyTorch's `binary_cross_entropy` is not differentiable with respect to weights, we implement the loss function manually to be able to evaluate gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ebd973",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:42.865071785Z",
     "start_time": "2023-11-05T08:28:42.487718299Z"
    },
    "id": "d4ebd973",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def adversarial_loss(input, target, w):\n",
    "    bce_loss = target * torch.log(input) + (1 - target) * torch.log(1 - input)\n",
    "    weighted_loss = w * bce_loss\n",
    "    total_loss = -torch.sum(weighted_loss)\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563f191d",
   "metadata": {
    "id": "563f191d",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.2. Definition of the optimizers\n",
    "In order to train the generator and discriminator, we need to define optimization schemes. In the following, we employ a momentum based optimizer called Adam, see [Kingma et al., Adam: A method for stochastic optimization](https://arxiv.org/abs/1412.6980) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545843d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:42.865395698Z",
     "start_time": "2023-11-05T08:28:42.488049189Z"
    },
    "id": "545843d8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.01  # learning rate\n",
    "b1 = 0.7  # first momentum parameter\n",
    "b2 = 0.999  # second momentum parameter\n",
    "\n",
    "generator_optimizer = Adam(generator.parameters(), lr=lr, betas=(b1, b2), weight_decay=0.005)\n",
    "discriminator_optimizer = Adam(\n",
    "    discriminator.parameters(), lr=lr, betas=(b1, b2), weight_decay=0.005\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0e376b",
   "metadata": {
    "id": "db0e376b",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4.3. Visualization of the training process\n",
    "We will visualize what is happening during the training by plotting the evolution of the generator's and the discriminator's loss functions during the training, as well as the progress in the relative entropy between the trained and the target distribution. We define a function that plots the loss functions and relative entropy. We call this function once an epoch of training is complete.\n",
    "\n",
    "Visualization of the training process begins when training data is collected across two epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad726df3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:28:42.865578055Z",
     "start_time": "2023-11-05T08:28:42.488353461Z"
    },
    "id": "ad726df3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "def plot_training_progress():\n",
    "    # we don't plot if we don't have enough data\n",
    "    if len(generator_loss_values) < 2:\n",
    "        return\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\n",
    "\n",
    "    # Generator Loss\n",
    "    ax1.set_title(\"Loss\")\n",
    "    ax1.plot(generator_loss_values, label=\"generator loss\", color=\"royalblue\")\n",
    "    ax1.plot(discriminator_loss_values, label=\"discriminator loss\", color=\"magenta\")\n",
    "    ax1.legend(loc=\"best\")\n",
    "    ax1.set_xlabel(\"Iteration\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.grid()\n",
    "\n",
    "    # Relative Entropy\n",
    "    ax2.set_title(\"Relative entropy\")\n",
    "    ax2.plot(entropy_values)\n",
    "    ax2.set_xlabel(\"Iteration\")\n",
    "    ax2.set_ylabel(\"Relative entropy\")\n",
    "    ax2.grid()\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fed81fd",
   "metadata": {
    "id": "4fed81fd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Model Training\n",
    "In the training loop we monitor not only loss functions, but relative entropy as well. The relative entropy describes a distance metric for distributions. Hence, we can use it to benchmark how close/far away the trained distribution is from the target distribution.\n",
    "\n",
    "Now, we are ready to train our model. It may take some time to train the model so be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66f2aaa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 812
    },
    "id": "b66f2aaa",
    "outputId": "e0f07fed-6f49-49fa-efc2-a52c82540340",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy.stats import multivariate_normal, entropy\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "num_qnn_outputs = num_discrete_values**num_dim\n",
    "\n",
    "generator_loss_values = []\n",
    "discriminator_loss_values = []\n",
    "entropy_values = []\n",
    "\n",
    "start = time.time()\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    valid = torch.ones(num_qnn_outputs, 1, dtype=torch.float)\n",
    "    fake = torch.zeros(num_qnn_outputs, 1, dtype=torch.float)\n",
    "\n",
    "    # Configure input\n",
    "    real_dist = torch.tensor(prob_data, dtype=torch.float).reshape(-1, 1)\n",
    "\n",
    "    # Configure samples\n",
    "    samples = torch.tensor(grid_elements, dtype=torch.float)\n",
    "    disc_value = discriminator(samples)\n",
    "\n",
    "    # Generate data\n",
    "    gen_dist = generator(torch.tensor([])).reshape(-1, 1)\n",
    "\n",
    "    # Train generator\n",
    "    generator_optimizer.zero_grad()\n",
    "    generator_loss = adversarial_loss(disc_value, valid, gen_dist)\n",
    "\n",
    "    # store for plotting\n",
    "    generator_loss_values.append(generator_loss.detach().item())\n",
    "\n",
    "    generator_loss.backward(retain_graph=True)\n",
    "    generator_optimizer.step()\n",
    "\n",
    "    # Train Discriminator\n",
    "    discriminator_optimizer.zero_grad()\n",
    "\n",
    "    real_loss = adversarial_loss(disc_value, valid, real_dist)\n",
    "    fake_loss = adversarial_loss(disc_value, fake, gen_dist.detach())\n",
    "    discriminator_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "    # Store for plotting\n",
    "    discriminator_loss_values.append(discriminator_loss.detach().item())\n",
    "\n",
    "    discriminator_loss.backward()\n",
    "    discriminator_optimizer.step()\n",
    "\n",
    "    entropy_value = entropy(gen_dist.detach().squeeze().numpy(), prob_data)\n",
    "    entropy_values.append(entropy_value)\n",
    "\n",
    "    plot_training_progress()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "print(f\"Fit in {elapsed:0.2f} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73486186",
   "metadata": {
    "id": "73486186",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Results: Cumulative Density Functions\n",
    "In this section we compare the cumulative distribution function (CDF) of the trained distribution to the CDF of the target distribution.\n",
    "\n",
    "First, we generate a new probability distribution with PyTorch autograd turned off as we are not going to train the model anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4befb219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:30:44.458844182Z",
     "start_time": "2023-11-05T08:30:44.219154056Z"
    },
    "id": "4befb219"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    generated_probabilities = generator().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345b5165",
   "metadata": {
    "id": "345b5165"
   },
   "source": [
    "And then, we plot the cumulative distribution functions of the generated distribution, original distribution, and the difference between them. Please, be careful, the scale on the third plot **is not the same** as on the first and second plot, and the actual difference between the two plotted CDFs is pretty small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ef8dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-05T08:30:44.807425440Z",
     "start_time": "2023-11-05T08:30:44.249756264Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 475
    },
    "id": "9a9ef8dd",
    "outputId": "0da8d132-d301-45f5-ebd7-06e2bda52dcd",
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 9))\n",
    "\n",
    "# Generated CDF\n",
    "gen_prob_grid = np.reshape(np.cumsum(generated_probabilities), grid_shape)\n",
    "\n",
    "ax1 = fig.add_subplot(1, 3, 1, projection=\"3d\")\n",
    "ax1.set_title(\"Generated CDF\")\n",
    "ax1.plot_surface(mesh_x, mesh_y, gen_prob_grid, linewidth=0, antialiased=False, cmap=cm.coolwarm)\n",
    "ax1.set_zlim(-0.05, 1.05)\n",
    "\n",
    "# Real CDF\n",
    "real_prob_grid = np.reshape(np.cumsum(prob_data), grid_shape)\n",
    "\n",
    "ax2 = fig.add_subplot(1, 3, 2, projection=\"3d\")\n",
    "ax2.set_title(\"True CDF\")\n",
    "ax2.plot_surface(mesh_x, mesh_y, real_prob_grid, linewidth=0, antialiased=False, cmap=cm.coolwarm)\n",
    "ax2.set_zlim(-0.05, 1.05)\n",
    "\n",
    "# Difference\n",
    "ax3 = fig.add_subplot(1, 3, 3, projection=\"3d\")\n",
    "ax3.set_title(\"Difference between CDFs\")\n",
    "ax3.plot_surface(\n",
    "    mesh_x, mesh_y, real_prob_grid - gen_prob_grid, linewidth=2, antialiased=False, cmap=cm.coolwarm\n",
    ")\n",
    "ax3.set_zlim(-0.05, 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f28b1a3",
   "metadata": {
    "id": "7f28b1a3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "Quantum generative adversarial networks employ the interplay of a generator and discriminator to map an approximate representation of a probability distribution underlying given data samples into a quantum channel. This tutorial presents a self-standing PyTorch-based qGAN implementation where the generator is given by a quantum channel, i.e., a variational quantum circuit, and the discriminator by a classical neural network, and discusses the application of efficient learning and loading of generic probability distributions into quantum states. The loading requires $\\mathscr{O}\\left(poly\\left(n\\right)\\right)$ gates and can thus enable the use of potentially advantageous quantum algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd00a7d665db7a4",
   "metadata": {
    "id": "9dd00a7d665db7a4"
   },
   "source": [
    "# Do it yourself\n",
    "\n",
    " You can also create a qGAN solely on quantum hardware. The ability of quantum information processors to represent vectors in N-dimensional spaces using logN qubits, and to perform manipulations of sparse and low-rank matrices in time O(poly(logN)) implies that QGANs exhibit a potential exponential advantage over classical GANs when the object of the game is to reproduce the statistics of measurements made on very high-dimensional data sets. In other words, QGANs have a potential exponential speedup when generating data made on very high-dimensional data sets.\n",
    "\n",
    "Construct a Quantum Generative Adversarial Network (QGAN) from (Lloyd and Weedbrook (2018), Dallaire-Demers and Killoran (2018)) using two subcircuits, a generator and a discriminator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9922525e9f0877",
   "metadata": {
    "id": "bb9922525e9f0877"
   },
   "source": [
    "## Generator and discriminator\n",
    "In this QGAN example, we will use a quantum circuit to generate the real data.\n",
    "\n",
    "For this simple example, our real data will be a qubit that has been rotated (from the starting state $|0⟩$) to some arbitrary, but fixed, state.\n",
    "\n",
    "Create a three qubit circuit, create a function that rotates the first qubit to a fixed angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8368c6137d681192",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "8368c6137d681192",
    "outputId": "758999d4-d23e-495d-de51-d957583e9dfc"
   },
   "outputs": [],
   "source": [
    "def real(angles):\n",
    "    ## Hadamard + Rotation on wire 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a670055fd35162db",
   "metadata": {
    "id": "a670055fd35162db"
   },
   "source": [
    "For the generator and discriminator, choose the basic two-wire layer circuit structure, acting on different wires.\n",
    "\n",
    "Both the real data circuit and the generator should output on wire 0, which will be connected as an input to the discriminator. Wire 1 will provide a workspace for the generator, while the discriminator’s output shall be on wire 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512b49f8e8a0277",
   "metadata": {
    "id": "d512b49f8e8a0277"
   },
   "outputs": [],
   "source": [
    "def generator(w):\n",
    "    # A 3 layer basic NN or similar acting on wires 0 and 1\n",
    "\n",
    "def discriminator(w):\n",
    "    # A 3 layer basic NN or similar acting on wires 0 and 2\n",
    "\n",
    "def real_disc_circuit(phi, theta, omega, disc_weights):\n",
    "    real([phi, theta, omega])\n",
    "    discriminator(disc_weights)\n",
    "    ### Return the expectation value of Pauli Z on wire 2\n",
    "\n",
    "def gen_disc_circuit(gen_weights, disc_weights):\n",
    "    generator(gen_weights)\n",
    "    discriminator(disc_weights)\n",
    "    ### Return the expectation value of Pauli Z on wire 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46f8e0652a1fd4",
   "metadata": {
    "id": "9d46f8e0652a1fd4"
   },
   "source": [
    "## QGAN cost functions\n",
    "\n",
    "There are two cost functions of interest, corresponding to the two stages of QGAN training. These cost functions are built from two pieces: the first piece is the probability that the discriminator correctly classifies real data as real. The second piece is the probability that the discriminator classifies fake data (i.e., a state prepared by the generator) as real.\n",
    "\n",
    "The discriminator is trained to maximize the probability of correctly classifying real data, while minimizing the probability of mistakenly classifying fake data.\n",
    "$$\n",
    "Cost_D = \\mathrm{Pr}(real|\\mathrm{fake}) - \\mathrm{Pr}(real|\\mathrm{real})\n",
    "$$\n",
    "The generator is trained to maximize the probability that the discriminator accepts fake data as real.\n",
    "$$\n",
    "Cost_G = - \\mathrm{Pr}(real|\\mathrm{fake})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19e5a0a2313155",
   "metadata": {
    "id": "ba19e5a0a2313155"
   },
   "outputs": [],
   "source": [
    "def prob_real_true(disc_weights):\n",
    "    ## code me\n",
    "\n",
    "def prob_fake_true(gen_weights, disc_weights):\n",
    "    ## code me\n",
    "\n",
    "\n",
    "def disc_cost(disc_weights):\n",
    "    ## code me\n",
    "\n",
    "\n",
    "def gen_cost(gen_weights):\n",
    "    ## code me\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25e45d696d273e",
   "metadata": {
    "id": "5c25e45d696d273e"
   },
   "source": [
    "In the first stage of training, optimize the discriminator while keeping the generator parameters fixed. At the discriminator’s optimum, the probability for the discriminator to correctly classify the real data should be close to one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae53c71d2a08f11",
   "metadata": {
    "id": "5ae53c71d2a08f11"
   },
   "outputs": [],
   "source": [
    "# code me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b5287b76a9dff1",
   "metadata": {
    "id": "84b5287b76a9dff1"
   },
   "source": [
    "In the adversarial game we now have to train the generator to better fool the discriminator.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc7e2f5ae59bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T01:36:14.061525655Z",
     "start_time": "2023-11-09T01:36:14.055150575Z"
    },
    "id": "87dc7e2f5ae59bb5"
   },
   "outputs": [],
   "source": [
    "# code me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e98b6cca85328c5",
   "metadata": {
    "id": "5e98b6cca85328c5"
   },
   "source": [
    "At the optimum of the generator, the probability for the discriminator to be fooled should be close to 1.\n",
    "At the joint optimum the discriminator cost will be close to zero, indicating that the discriminator assigns equal probability to both real and generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea0e5d0bee46cf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T01:37:18.608746385Z",
     "start_time": "2023-11-09T01:37:18.489959873Z"
    },
    "id": "eea0e5d0bee46cf2"
   },
   "outputs": [],
   "source": [
    "#code me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958cca8cc63cdaf",
   "metadata": {
    "id": "2958cca8cc63cdaf"
   },
   "source": [
    "Compare the states of the real data circuit and the generator. We expect the generator to have learned to be in a state that is very close to the one prepared in the real data circuit. An easy way to access the state of the first qubit is through its Bloch sphere representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17319167b34062c2",
   "metadata": {
    "id": "17319167b34062c2"
   },
   "outputs": [],
   "source": [
    "#code me"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
