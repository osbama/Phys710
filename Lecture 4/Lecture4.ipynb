{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:33:35.365698465Z",
     "start_time": "2023-11-16T04:33:34.546712866Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "laxw7EwrTvwP",
    "outputId": "b122e462-7d61-475a-e125-24c41dadca41"
   },
   "outputs": [],
   "source": [
    "!pip install qiskit-algorithms\n",
    "!pip install qiskit-machine-learning\n",
    "!pip install qiskit-Aer\n",
    "!pip install qiskit-qulacs\n",
    "!pip install matplotlib\n",
    "!pip install pylatexenc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "90qsOyEITvwR",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Quantum Kernel Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JcD7HvF2TvwS"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1. Kernel Methods for Machine Learning\n",
    "\n",
    "Kernel methods are a collection of pattern analysis algorithms that use kernel functions to operate in a high-dimensional feature space. The best-known application of kernel methods is in **Support Vector Machines (SVMs)**, supervised learning algorithms commonly used for classification tasks. The main goal of SVMs is to find decision boundaries to separate a given set of data points into classes. When these data spaces are not linearly separable, SVMs can benefit from the use of kernels to find these boundaries.\n",
    "\n",
    "Formally, decision boundaries are hyperplanes in a high dimensional space. The kernel function implicitly maps input data into this higher dimensional space, where it can be easier to solve the initial problem. In other words, kernels may allow data distributions that were originally non-linearly separable to become a linearly separable problem. This is an effect known as the \"kernel trick\".\n",
    "\n",
    "There are use-cases for kernel-based unsupervised algorithms too, for example, in the context of clustering. **Spectral Clustering** is a technique where data points are treated as nodes of a graph, and the clustering task is viewed as a graph partitioning problem where nodes are mapped to a space where they can be easily segregated to form clusters.\n",
    "\n",
    "### 1.2. Kernel Functions\n",
    "\n",
    "Mathematically, kernel functions follow:\n",
    "\n",
    "$k(\\vec{x}_i, \\vec{x}_j) = \\langle f(\\vec{x}_i), f(\\vec{x}_j) \\rangle$\n",
    "\n",
    "where\n",
    "* $k$ is the kernel function\n",
    "* $\\vec{x}_i, \\vec{x}_j$ are $n$ dimensional inputs\n",
    "* $f$ is a map from $n$-dimension to $m$-dimension space and\n",
    "* $\\langle a,b \\rangle$ denotes the inner product\n",
    "\n",
    "When considering finite data, a kernel function can be represented as a matrix:\n",
    "\n",
    "$K_{ij} = k(\\vec{x}_i,\\vec{x}_j)$.\n",
    "\n",
    "### 1.3. Quantum Kernels\n",
    "\n",
    "The main idea behind quantum kernel machine learning is to leverage quantum feature maps to perform the kernel trick. In this case, the quantum kernel is created by mapping a classical feature vector $\\vec{x}$ to a Hilbert space using a quantum feature map $\\phi(\\vec{x})$. Mathematically:\n",
    "\n",
    "$K_{ij} = \\left| \\langle \\phi(\\vec{x}_i)| \\phi(\\vec{x}_j) \\rangle \\right|^{2}$\n",
    "\n",
    "where\n",
    "* $K_{ij}$ is the kernel matrix\n",
    "* $\\vec{x}_i, \\vec{x}_j$ are $n$ dimensional inputs\n",
    "* $\\phi(\\vec{x})$ is the quantum feature map\n",
    "* $\\left| \\langle a|b \\rangle \\right|^{2}$ denotes the overlap of two quantum states $a$ and $b$\n",
    "\n",
    "Quantum kernels can be plugged into common classical kernel learning algorithms such as SVMs or clustering algorithms, as you will see in the examples below. They can also be leveraged in new quantum kernel methods like [QSVC](https://qiskit.org/ecosystem/machine-learning/stubs/qiskit_machine_learning.algorithms.QSVC.html) class  provided by `qiskit-machine-learning`.\n",
    "***\n",
    "\n",
    "Before introducing any example, we set up the global seed to ensure reproducibility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:33:49.214920923Z",
     "start_time": "2023-11-16T04:33:48.709582484Z"
    },
    "id": "WVCPzur1TvwS"
   },
   "outputs": [],
   "source": [
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "\n",
    "algorithm_globals.random_seed = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tgD4pYZTvwT"
   },
   "source": [
    "## 2. Classification\n",
    "\n",
    "This section illustrates a quantum kernel classification workflow using `qiskit-machine-learning`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHQhGmFOTvwT"
   },
   "source": [
    "### 2.1. Defining the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zDnRDUu_TvwT"
   },
   "source": [
    "For this example, we will use the _ad hoc dataset_ as described in the reference [paper](https://arxiv.org/pdf/1804.11326.pdf).\n",
    "\n",
    "We can define the dataset dimension and get our train and test subsets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:33:52.350918339Z",
     "start_time": "2023-11-16T04:33:51.275089269Z"
    },
    "id": "iSIRMTSvTvwT"
   },
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "\n",
    "adhoc_dimension = 2\n",
    "train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
    "    training_size=20,\n",
    "    test_size=5,\n",
    "    n=adhoc_dimension,\n",
    "    gap=0.3,\n",
    "    plot_data=False,\n",
    "    one_hot=False,\n",
    "    include_sample_total=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFp0kyDwTvwT"
   },
   "source": [
    "This dataset is two-dimensional, the two features are represented by the $x$ and $y$ coordinates, and it has two class labels: A and B. We can plot it and see what the distribution looks like. We define utility functions to plot the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:33:53.962698594Z",
     "start_time": "2023-11-16T04:33:53.909681523Z"
    },
    "id": "ZDz3yaT2TvwT"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_features(ax, features, labels, class_label, marker, face, edge, label):\n",
    "    # A train plot\n",
    "    ax.scatter(\n",
    "        # x coordinate of labels where class is class_label\n",
    "        features[np.where(labels[:] == class_label), 0],\n",
    "        # y coordinate of labels where class is class_label\n",
    "        features[np.where(labels[:] == class_label), 1],\n",
    "        marker=marker,\n",
    "        facecolors=face,\n",
    "        edgecolors=edge,\n",
    "        label=label,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_dataset(train_features, train_labels, test_features, test_labels, adhoc_total):\n",
    "\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.ylim(0, 2 * np.pi)\n",
    "    plt.xlim(0, 2 * np.pi)\n",
    "    plt.imshow(\n",
    "        np.asmatrix(adhoc_total).T,\n",
    "        interpolation=\"nearest\",\n",
    "        origin=\"lower\",\n",
    "        cmap=\"RdBu\",\n",
    "        extent=[0, 2 * np.pi, 0, 2 * np.pi],\n",
    "    )\n",
    "\n",
    "    # A train plot\n",
    "    plot_features(plt, train_features, train_labels, 0, \"s\", \"w\", \"b\", \"A train\")\n",
    "\n",
    "    # B train plot\n",
    "    plot_features(plt, train_features, train_labels, 1, \"o\", \"w\", \"r\", \"B train\")\n",
    "\n",
    "    # A test plot\n",
    "    plot_features(plt, test_features, test_labels, 0, \"s\", \"b\", \"w\", \"A test\")\n",
    "\n",
    "    # B test plot\n",
    "    plot_features(plt, test_features, test_labels, 1, \"o\", \"r\", \"w\", \"B test\")\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "    plt.title(\"Ad hoc dataset\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DM70DRPbTvwU"
   },
   "source": [
    "Now we actually plot the dataset for classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:33:56.146841173Z",
     "start_time": "2023-11-16T04:33:55.854360307Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "v958OjE8TvwU",
    "outputId": "7a257491-1f2d-4f18-e23d-29f2e6c1bb7e",
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "plot_dataset(train_features, train_labels, test_features, test_labels, adhoc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnE3Nit4TvwU"
   },
   "source": [
    "### 2.2. Defining the quantum kernel\n",
    "\n",
    "The next step is to create a quantum kernel instance that will help classify this data.\n",
    "\n",
    "We use the [FidelityQuantumKernel](https://qiskit.org/ecosystem/machine-learning/stubs/qiskit_machine_learning.kernels.FidelityQuantumKernel.html) class, and pass two input arguments to its constructor:\n",
    "\n",
    "1. `feature_map`: in this case, a two-qubit [ZZFeatureMap](https://qiskit.org/documentation/stubs/qiskit.circuit.library.ZZFeatureMap.html).\n",
    "\n",
    "2. `fidelity`: in this case, the [ComputeUncompute](https://qiskit.org/ecosystem/algorithms/stubs/qiskit_algorithms.state_fidelities.ComputeUncompute.html) fidelity subroutine that leverages the [Sampler](https://qiskit.org/documentation/stubs/qiskit.primitives.Sampler.html) primitive.\n",
    "\n",
    "**NOTE:** If you don't pass a `Sampler` or `Fidelity` instance, then the instances of the reference `Sampler` and `ComputeUncompute` classes (found in `qiskit.primitives`) will be created by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:33:58.023937734Z",
     "start_time": "2023-11-16T04:33:57.944903356Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ka6BR6u5TvwU",
    "outputId": "ba645b88-baba-4f5f-91e2-ec3f6e7dd154"
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.primitives import Sampler\n",
    "from qiskit_algorithms.state_fidelities import ComputeUncompute\n",
    "from qiskit_machine_learning.kernels import FidelityQuantumKernel\n",
    "\n",
    "adhoc_feature_map = ZZFeatureMap(feature_dimension=adhoc_dimension, reps=2, entanglement=\"linear\")\n",
    "\n",
    "sampler = Sampler()\n",
    "\n",
    "fidelity = ComputeUncompute(sampler=sampler)\n",
    "\n",
    "adhoc_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=adhoc_feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALX2p1ZRTvwU"
   },
   "source": [
    "### 2.3. Classification with SVC\n",
    "The quantum kernel can now be plugged into classical kernel methods, such as the [SVC](https://scikit-learn.org/stable/modules/svm.html) algorithm from `scikit-learn`. This algorithm allows us to define a [custom kernel](https://scikit-learn.org/stable/modules/svm.html#custom-kernels) in two ways:\n",
    "\n",
    "1. by providing the kernel as a **callable function**\n",
    "2. by precomputing the **kernel matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u31-oTnoTvwU"
   },
   "source": [
    "#### Kernel as a callable function\n",
    "\n",
    "We define a SVC model and directly pass the `evaluate` function of the quantum kernel as a callable. Once the model is created, we train it by calling the `fit` method on the training dataset and evaluate the model for accuracy with `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:07.092454971Z",
     "start_time": "2023-11-16T04:34:00.015310628Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BJDRA_XuTvwV",
    "outputId": "77781289-1b0e-43cf-97d6-3e6a32515dc8"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "adhoc_svc = SVC(kernel=adhoc_kernel.evaluate)\n",
    "\n",
    "adhoc_svc.fit(train_features, train_labels)\n",
    "\n",
    "adhoc_score_callable_function = adhoc_svc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Callable kernel classification test score: {adhoc_score_callable_function}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RtM9uNWsTvwV"
   },
   "source": [
    "#### Precomputed kernel matrix\n",
    "\n",
    "Instead of passing a function of the quantum kernel as a callable, we can also precompute training and testing kernel matrices before passing them to the `scikit-learn` `SVC` algorithm.\n",
    "\n",
    "To extract the train and test matrices, we can call `evaluate` on the previously defined kernel and visualize them graphically as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:14.328447958Z",
     "start_time": "2023-11-16T04:34:07.061402412Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "ctpqug_aTvwV",
    "outputId": "f4c93fc1-b7dc-46ea-c672-fe5e1be8e8f8"
   },
   "outputs": [],
   "source": [
    "adhoc_matrix_train = adhoc_kernel.evaluate(x_vec=train_features)\n",
    "adhoc_matrix_test = adhoc_kernel.evaluate(x_vec=test_features, y_vec=train_features)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "axs[0].imshow(\n",
    "    np.asmatrix(adhoc_matrix_train), interpolation=\"nearest\", origin=\"upper\", cmap=\"Blues\"\n",
    ")\n",
    "axs[0].set_title(\"Ad hoc training kernel matrix\")\n",
    "\n",
    "axs[1].imshow(np.asmatrix(adhoc_matrix_test), interpolation=\"nearest\", origin=\"upper\", cmap=\"Reds\")\n",
    "axs[1].set_title(\"Ad hoc testing kernel matrix\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev1o4OLFTvwV"
   },
   "source": [
    "To use these matrices, we set the `kernel` parameter of a new `SVC` instance to `\"precomputed\"`. We train the classifier by calling `fit` with the training matrix and training dataset. Once the model is trained, we evaluate it using the test matrix on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:14.499174113Z",
     "start_time": "2023-11-16T04:34:14.330188720Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcUkJSAuTvwW",
    "outputId": "16153d79-cbbd-498b-e075-8abb49d25df7"
   },
   "outputs": [],
   "source": [
    "adhoc_svc = SVC(kernel=\"precomputed\")\n",
    "\n",
    "adhoc_svc.fit(adhoc_matrix_train, train_labels)\n",
    "\n",
    "adhoc_score_precomputed_kernel = adhoc_svc.score(adhoc_matrix_test, test_labels)\n",
    "\n",
    "print(f\"Precomputed kernel classification test score: {adhoc_score_precomputed_kernel}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQWsDrPMTvwW"
   },
   "source": [
    "### 2.4. Classification with QSVC\n",
    "\n",
    "`QSVC` is an alternative training algorithm provided by `qiskit-machine-learning` for convenience. It is an extension of `SVC` that takes in a quantum kernel instead of the `kernel.evaluate` method shown before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:20.539859697Z",
     "start_time": "2023-11-16T04:34:14.374719851Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x2zPohtcTvwW",
    "outputId": "9ffc750a-e8ed-4713-b70d-b939cc577d62"
   },
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "\n",
    "qsvc = QSVC(quantum_kernel=adhoc_kernel)\n",
    "\n",
    "qsvc.fit(train_features, train_labels)\n",
    "\n",
    "qsvc_score = qsvc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"QSVC classification test score: {qsvc_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeYP44GpTvwW"
   },
   "source": [
    "### 2.5. Evaluation of models used for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:20.562274062Z",
     "start_time": "2023-11-16T04:34:20.541211110Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AZmJwdSATvwW",
    "outputId": "7772033b-3dc3-40a2-dd06-1faa6d6ebbf1"
   },
   "outputs": [],
   "source": [
    "print(f\"Classification Model                    | Accuracy Score\")\n",
    "print(f\"---------------------------------------------------------\")\n",
    "print(f\"SVC using kernel as a callable function | {adhoc_score_callable_function:10.2f}\")\n",
    "print(f\"SVC using precomputed kernel matrix     | {adhoc_score_precomputed_kernel:10.2f}\")\n",
    "print(f\"QSVC                                    | {qsvc_score:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqexZSDjTvwW"
   },
   "source": [
    "As the classification dataset is small, we find that the three models achieve 100% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WanNstQRTvwW"
   },
   "source": [
    "## 3. Clustering\n",
    "\n",
    "The second workflow in this tutorial focuses on a clustering task using `qiskit-machine-learning` and the spectral clustering algorithm from `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ELLYmc4TvwW"
   },
   "source": [
    "### 3.1. Defining the dataset\n",
    "\n",
    "We will once again use the _ad hoc dataset_, but now generated with a higher gap of `0.6` (previous example: `0.3`) between the two classes.\n",
    "\n",
    "Note that clustering falls under the category of unsupervised machine learning, so a test dataset is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:21.358594399Z",
     "start_time": "2023-11-16T04:34:20.545951906Z"
    },
    "id": "R9szoQtfTvwW"
   },
   "outputs": [],
   "source": [
    "adhoc_dimension = 2\n",
    "train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
    "    training_size=25,\n",
    "    test_size=0,\n",
    "    n=adhoc_dimension,\n",
    "    gap=0.6,\n",
    "    plot_data=False,\n",
    "    one_hot=False,\n",
    "    include_sample_total=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZgHiDfXTvwW"
   },
   "source": [
    " We plot the clustering dataset below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:21.668432189Z",
     "start_time": "2023-11-16T04:34:21.383794987Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "cBuIUiy0TvwX",
    "outputId": "e66cec1d-020f-495d-8935-3d4f7e4b79f6"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.ylim(0, 2 * np.pi)\n",
    "plt.xlim(0, 2 * np.pi)\n",
    "plt.imshow(\n",
    "    np.asmatrix(adhoc_total).T,\n",
    "    interpolation=\"nearest\",\n",
    "    origin=\"lower\",\n",
    "    cmap=\"RdBu\",\n",
    "    extent=[0, 2 * np.pi, 0, 2 * np.pi],\n",
    ")\n",
    "\n",
    "# A label plot\n",
    "plot_features(plt, train_features, train_labels, 0, \"s\", \"w\", \"b\", \"B\")\n",
    "\n",
    "# B label plot\n",
    "plot_features(plt, train_features, train_labels, 1, \"o\", \"w\", \"r\", \"B\")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "plt.title(\"Ad hoc dataset for clustering\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zMsy-gwGTvwX"
   },
   "source": [
    "### 3.2. Defining the Quantum Kernel\n",
    "We use an identical setup as in the classification example. We create another instance of the `FidelityQuantumKernel` class with a `ZZFeatureMap`, but you might notice that in this case we do not provide a `fidelity` instance. This is because the `ComputeUncompute` method provided in the previous case is instantiated by default when the fidelity instance is not provided explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:21.702429055Z",
     "start_time": "2023-11-16T04:34:21.671451899Z"
    },
    "id": "8k6ZR1PbTvwX"
   },
   "outputs": [],
   "source": [
    "adhoc_feature_map = ZZFeatureMap(feature_dimension=adhoc_dimension, reps=2, entanglement=\"linear\")\n",
    "\n",
    "adhoc_kernel = FidelityQuantumKernel(feature_map=adhoc_feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oktfqtUtTvwX"
   },
   "source": [
    "### 3.3. Clustering with the Spectral Clustering Model\n",
    "\n",
    "The `scikit-learn` spectral clustering algorithm allows us to define a custom kernel in two ways (just like `SVC`):\n",
    "\n",
    "1. by providing the kernel as a **callable function**\n",
    "2. by precomputing the **kernel matrix**.\n",
    "\n",
    "With the current `FidelityQuantumKernel` class in `qiskit-machine-learning`, we can only use the latter option, so we precompute the kernel matrix by calling `evaluate` and visualize it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:30.126436717Z",
     "start_time": "2023-11-16T04:34:21.686578975Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "-2QmReqITvwX",
    "outputId": "38409156-64ac-47a9-b99d-38d38b3b45bd"
   },
   "outputs": [],
   "source": [
    "adhoc_matrix = adhoc_kernel.evaluate(x_vec=train_features)\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(np.asmatrix(adhoc_matrix), interpolation=\"nearest\", origin=\"upper\", cmap=\"Greens\")\n",
    "plt.title(\"Ad hoc clustering kernel matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SG2AnvQTTvwX"
   },
   "source": [
    "Next, we define a spectral clustering model and fit it using the precomputed kernel. Further, we score the labels using normalized mutual information, since we know the class labels a priori (before hand)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:30.621885252Z",
     "start_time": "2023-11-16T04:34:30.128684646Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9jffViwdTvwX",
    "outputId": "f42445cc-7c3f-4f97-e563-d34ca3b02f2c"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "adhoc_spectral = SpectralClustering(2, affinity=\"precomputed\")\n",
    "\n",
    "cluster_labels = adhoc_spectral.fit_predict(adhoc_matrix)\n",
    "\n",
    "cluster_score = normalized_mutual_info_score(cluster_labels, train_labels)\n",
    "\n",
    "print(f\"Clustering score: {cluster_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uTkHK1lSTvwY"
   },
   "source": [
    "## 4. Kernel Principal Component Analysis\n",
    "\n",
    "This section focuses on a Principal Component Analysis task using a kernel PCA algorithm. We calculate a kernel matrix using a `ZZFeatureMap` and show that this approach translates the original features into a new space, where axes are chosen along principal components. In this space the classification task can be performed with a simpler model rather than an SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5fITKeNHTvwY"
   },
   "source": [
    "### 4.1. Defining the dataset\n",
    "\n",
    "We again use the _ad hoc dataset_ with a gap of `0.6` between the two classes. This dataset resembles the dataset we had in the clustering section, the difference is that in this case `test_size` is not zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:31.771407009Z",
     "start_time": "2023-11-16T04:34:30.415680628Z"
    },
    "id": "C3TrwSsbTvwY"
   },
   "outputs": [],
   "source": [
    "adhoc_dimension = 2\n",
    "train_features, train_labels, test_features, test_labels, adhoc_total = ad_hoc_data(\n",
    "    training_size=25,\n",
    "    test_size=10,\n",
    "    n=adhoc_dimension,\n",
    "    gap=0.6,\n",
    "    plot_data=False,\n",
    "    one_hot=False,\n",
    "    include_sample_total=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odtT1sQYTvwY"
   },
   "source": [
    "We plot the training and test datasets below. Our ultimate goal in this section is to construct new coordinates where the two classes can be linearly separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:32.187349389Z",
     "start_time": "2023-11-16T04:34:31.838207810Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "PqB8OgciTvwY",
    "outputId": "de8a4b95-b45e-4914-abfe-4f128e4f2b2b"
   },
   "outputs": [],
   "source": [
    "plot_dataset(train_features, train_labels, test_features, test_labels, adhoc_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPow3CL2TvwY"
   },
   "source": [
    "### 4.2. Defining the Quantum Kernel\n",
    "\n",
    "We proceed with the same kernel setup as it was in the classification task, namely a `ZZFeatureMap` circuit as a feature map and an instance of `FidelityQuantumKernel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:32.187803496Z",
     "start_time": "2023-11-16T04:34:32.171763065Z"
    },
    "id": "frA70q81TvwY"
   },
   "outputs": [],
   "source": [
    "feature_map = ZZFeatureMap(feature_dimension=2, reps=2, entanglement=\"linear\")\n",
    "qpca_kernel = FidelityQuantumKernel(fidelity=fidelity, feature_map=feature_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmNf2PvSTvwZ"
   },
   "source": [
    "Then, we evaluate kernel matrices for the training and test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:47.164104175Z",
     "start_time": "2023-11-16T04:34:32.177160989Z"
    },
    "id": "KIxjapmkTvwZ"
   },
   "outputs": [],
   "source": [
    "matrix_train = qpca_kernel.evaluate(x_vec=train_features)\n",
    "matrix_test = qpca_kernel.evaluate(x_vec=test_features, y_vec=train_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHs_dTfSTvwZ"
   },
   "source": [
    "### 4.3. Comparison of Kernel PCA on gaussian and quantum kernel\n",
    "\n",
    "In this section we use the `KernelPCA` implementation from `scikit-learn`, with the `kernel` parameter set to \"rbf\" for a gaussian kernel and \"precomputed\" for a quantum kernel. The former is very popular in classical machine learning models, whereas the latter allows using a quantum kernel defined as `qpca_kernel`.\n",
    "\n",
    "One can observe that the gaussian kernel based Kernel PCA model fails to make the dataset linearly separable, while the quantum kernel succeeds.\n",
    "\n",
    "While usually PCA is used to reduce the number of features in a dataset, or in other words to reduce dimensionality of a dataset, we don't do that here. Rather we keep the number of dimensions and employ the kernel PCA, mostly for visualization purposes, to show that classification on the transformed dataset becomes easily tractable by linear methods, like logistic regression. We use this method to separate two classes in the principal component space with a `LogisticRegression` model from `scikit-learn`. As usual we train it by calling the `fit` method on the training dataset and evaluate the model for accuracy with `score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:47.227285581Z",
     "start_time": "2023-11-16T04:34:47.210778601Z"
    },
    "id": "LSE8D-7DTvwZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kernel_pca_rbf = KernelPCA(n_components=2, kernel=\"rbf\")\n",
    "kernel_pca_rbf.fit(train_features)\n",
    "train_features_rbf = kernel_pca_rbf.transform(train_features)\n",
    "test_features_rbf = kernel_pca_rbf.transform(test_features)\n",
    "\n",
    "kernel_pca_q = KernelPCA(n_components=2, kernel=\"precomputed\")\n",
    "train_features_q = kernel_pca_q.fit_transform(matrix_train)\n",
    "test_features_q = kernel_pca_q.transform(matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WKmiMyLTTvwZ"
   },
   "source": [
    "Here we train and score a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:47.228530690Z",
     "start_time": "2023-11-16T04:34:47.211521165Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qz56SsATTvwa",
    "outputId": "10a296ab-2296-4f72-eba1-67d578fa790e"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logistic_regression = LogisticRegression()\n",
    "logistic_regression.fit(train_features_q, train_labels)\n",
    "\n",
    "logistic_score = logistic_regression.score(test_features_q, test_labels)\n",
    "print(f\"Logistic regression score: {logistic_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeen4N34Tvwa"
   },
   "source": [
    "Let's plot the results. First, we plot the transformed dataset we get with the quantum kernel. On the same plot we also add model results. Then, we plot the transformed dataset we get with the gaussian kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:47.724629075Z",
     "start_time": "2023-11-16T04:34:47.211991923Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "CUaLfV1bTvwa",
    "outputId": "887fb5d9-96cb-4cc6-d904-fa8f56307c7b"
   },
   "outputs": [],
   "source": [
    "fig, (q_ax, rbf_ax) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "\n",
    "plot_features(q_ax, train_features_q, train_labels, 0, \"s\", \"w\", \"b\", \"A train\")\n",
    "plot_features(q_ax, train_features_q, train_labels, 1, \"o\", \"w\", \"r\", \"B train\")\n",
    "\n",
    "plot_features(q_ax, test_features_q, test_labels, 0, \"s\", \"b\", \"w\", \"A test\")\n",
    "plot_features(q_ax, test_features_q, test_labels, 1, \"o\", \"r\", \"w\", \"A test\")\n",
    "\n",
    "q_ax.set_ylabel(\"Principal component #1\")\n",
    "q_ax.set_xlabel(\"Principal component #0\")\n",
    "q_ax.set_title(\"Projection of training and test data\\n using KPCA with Quantum Kernel\")\n",
    "\n",
    "# Plotting the linear separation\n",
    "h = 0.01  # step size in the mesh\n",
    "\n",
    "# create a mesh to plot in\n",
    "x_min, x_max = train_features_q[:, 0].min() - 1, train_features_q[:, 0].max() + 1\n",
    "y_min, y_max = train_features_q[:, 1].min() - 1, train_features_q[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "predictions = logistic_regression.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "predictions = predictions.reshape(xx.shape)\n",
    "q_ax.contourf(xx, yy, predictions, cmap=plt.cm.RdBu, alpha=0.2)\n",
    "\n",
    "plot_features(rbf_ax, train_features_rbf, train_labels, 0, \"s\", \"w\", \"b\", \"A train\")\n",
    "plot_features(rbf_ax, train_features_rbf, train_labels, 1, \"o\", \"w\", \"r\", \"B train\")\n",
    "plot_features(rbf_ax, test_features_rbf, test_labels, 0, \"s\", \"b\", \"w\", \"A test\")\n",
    "plot_features(rbf_ax, test_features_rbf, test_labels, 1, \"o\", \"r\", \"w\", \"A test\")\n",
    "\n",
    "rbf_ax.set_ylabel(\"Principal component #1\")\n",
    "rbf_ax.set_xlabel(\"Principal component #0\")\n",
    "rbf_ax.set_title(\"Projection of training data\\n using KernelPCA\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tFAM0fJTvwa"
   },
   "source": [
    "As we can see, the data points on the right figure are not separable, but they are on the left figure, hence in case of quantum kernel we can apply linear models on the transformed dataset and this is why SVM classifier works perfectly well on the _ad hoc_ dataset as we saw in the [classification section](#2.-Classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-I3D--fTvwa"
   },
   "source": [
    "For further reference, `scikit-learn` has other algorithms that can use a precomputed kernel matrix, such as:\n",
    "\n",
    "- [Agglomerative clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)\n",
    "- [Support vector regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)\n",
    "- [Ridge regression](https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html)\n",
    "- [Gaussian process regression](https://scikit-learn.org/stable/modules/gaussian_process.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4dkag77STvwa"
   },
   "source": [
    "# Quantum Kernel Training for Machine Learning Applications\n",
    "\n",
    "In this tutorial, we will train a quantum kernel on a labeled dataset for a machine learning application. To illustrate the basic steps, we will use Quantum Kernel Alignment (QKA) for a binary classification task. QKA is a technique that iteratively adapts a parametrized quantum kernel to a dataset while converging to the maximum SVM margin. More information about QKA can be found in the preprint, [\"Covariant quantum kernels for data with group structure.\"](https://arxiv.org/abs/2105.03406)\n",
    "\n",
    "\n",
    "The entry point to training a quantum kernel is the `QuantumKernelTrainer` class. The basic steps are:\n",
    "\n",
    "1. Prepare the dataset\n",
    "2. Define the quantum feature map\n",
    "3. Set up an instance of `TrainableKernel` and `QuantumKernelTrainer` objects\n",
    "4. Use the `QuantumKernelTrainer.fit` method to train the kernel parameters on the dataset\n",
    "5. Pass the trained quantum kernel to a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AF58w8KPTvwa"
   },
   "source": [
    "### Import Local, External, and Qiskit Packages and define a callback class for our optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:47.868274986Z",
     "start_time": "2023-11-16T04:34:47.734126469Z"
    },
    "id": "lYCMpSmJTvwb"
   },
   "outputs": [],
   "source": [
    "# External imports\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Qiskit imports\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit_algorithms.optimizers import SPSA\n",
    "from qiskit_machine_learning.kernels import TrainableFidelityQuantumKernel\n",
    "from qiskit_machine_learning.kernels.algorithms import QuantumKernelTrainer\n",
    "from qiskit_machine_learning.algorithms import QSVC\n",
    "from qiskit_machine_learning.datasets import ad_hoc_data\n",
    "\n",
    "\n",
    "class QKTCallback:\n",
    "    \"\"\"Callback wrapper class.\"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self._data = [[] for i in range(5)]\n",
    "\n",
    "    def callback(self, x0, x1=None, x2=None, x3=None, x4=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x0: number of function evaluations\n",
    "            x1: the parameters\n",
    "            x2: the function value\n",
    "            x3: the stepsize\n",
    "            x4: whether the step was accepted\n",
    "        \"\"\"\n",
    "        self._data[0].append(x0)\n",
    "        self._data[1].append(x1)\n",
    "        self._data[2].append(x2)\n",
    "        self._data[3].append(x3)\n",
    "        self._data[4].append(x4)\n",
    "\n",
    "    def get_callback_data(self):\n",
    "        return self._data\n",
    "\n",
    "    def clear_callback_data(self):\n",
    "        self._data = [[] for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mO93U4ztTvwb"
   },
   "source": [
    "### Prepare the Dataset\n",
    "\n",
    "In this guide, we will use Qiskit Machine Learning's `ad_hoc.py` dataset to demonstrate the kernel training process. See the documentation [here](https://qiskit.org/ecosystem/machine-learning/stubs/qiskit_machine_learning.datasets.ad_hoc_data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:48.812108741Z",
     "start_time": "2023-11-16T04:34:47.981702514Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 488
    },
    "id": "VsWJ3PyRTvwb",
    "outputId": "a0752430-acdf-4a08-8f37-82c63f7d1f2c"
   },
   "outputs": [],
   "source": [
    "adhoc_dimension = 2\n",
    "X_train, y_train, X_test, y_test, adhoc_total = ad_hoc_data(\n",
    "    training_size=20,\n",
    "    test_size=5,\n",
    "    n=adhoc_dimension,\n",
    "    gap=0.3,\n",
    "    plot_data=False,\n",
    "    one_hot=False,\n",
    "    include_sample_total=True,\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.ylim(0, 2 * np.pi)\n",
    "plt.xlim(0, 2 * np.pi)\n",
    "plt.imshow(\n",
    "    np.asmatrix(adhoc_total).T,\n",
    "    interpolation=\"nearest\",\n",
    "    origin=\"lower\",\n",
    "    cmap=\"RdBu\",\n",
    "    extent=[0, 2 * np.pi, 0, 2 * np.pi],\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    X_train[np.where(y_train[:] == 0), 0],\n",
    "    X_train[np.where(y_train[:] == 0), 1],\n",
    "    marker=\"s\",\n",
    "    facecolors=\"w\",\n",
    "    edgecolors=\"b\",\n",
    "    label=\"A train\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X_train[np.where(y_train[:] == 1), 0],\n",
    "    X_train[np.where(y_train[:] == 1), 1],\n",
    "    marker=\"o\",\n",
    "    facecolors=\"w\",\n",
    "    edgecolors=\"r\",\n",
    "    label=\"B train\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X_test[np.where(y_test[:] == 0), 0],\n",
    "    X_test[np.where(y_test[:] == 0), 1],\n",
    "    marker=\"s\",\n",
    "    facecolors=\"b\",\n",
    "    edgecolors=\"w\",\n",
    "    label=\"A test\",\n",
    ")\n",
    "plt.scatter(\n",
    "    X_test[np.where(y_test[:] == 1), 0],\n",
    "    X_test[np.where(y_test[:] == 1), 1],\n",
    "    marker=\"o\",\n",
    "    facecolors=\"r\",\n",
    "    edgecolors=\"w\",\n",
    "    label=\"B test\",\n",
    ")\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0.0)\n",
    "plt.title(\"Ad hoc dataset for classification\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eiUz2oJTvwb"
   },
   "source": [
    "### Define the Quantum Feature Map\n",
    "\n",
    "Next, we set up the quantum feature map, which encodes classical data into the quantum state space. Here, we use a `QuantumCircuit` to set up a trainable rotation layer and a `ZZFeatureMap` from `Qiskit` to represent the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:48.906850228Z",
     "start_time": "2023-11-16T04:34:48.809027820Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXZ_4F8fTvwb",
    "outputId": "fcf83fe6-1378-4810-9400-f16b586a839d"
   },
   "outputs": [],
   "source": [
    "# Create a rotational layer to train. We will rotate each qubit the same amount.\n",
    "training_params = ParameterVector(\"θ\", 1)\n",
    "fm0 = QuantumCircuit(2)\n",
    "fm0.ry(training_params[0], 0)\n",
    "fm0.ry(training_params[0], 1)\n",
    "\n",
    "# Use ZZFeatureMap to represent input data\n",
    "fm1 = ZZFeatureMap(2)\n",
    "\n",
    "# Create the feature map, composed of our two circuits\n",
    "fm = fm0.compose(fm1)\n",
    "\n",
    "fm.draw('mpl')\n",
    "print(f\"Trainable parameters: {training_params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fni03lZOTvwc"
   },
   "source": [
    "### Set Up the Quantum Kernel and Quantum Kernel Trainer\n",
    "\n",
    "To train the quantum kernel, we will use an instance of `TrainableFidelityQuantumKernel` (holds the feature map and its parameters) and `QuantumKernelTrainer` (manages the training process).\n",
    "\n",
    "We will train using the Quantum Kernel Alignment technique by selecting the kernel loss function, `SVCLoss`, as input to the `QuantumKernelTrainer`. Since this is a Qiskit-supported loss, we can use the string, `\"svc_loss\"`; however, note that default settings are used when passing the loss as a string. For custom settings, instantiate explicitly with the desired options, and pass the `KernelLoss` object to the `QuantumKernelTrainer`.\n",
    "\n",
    "We will select SPSA as the optimizer and initialize the trainable parameter with the `initial_point` argument. Note: The length of the list passed as the `initial_point` argument must equal the number of trainable parameters in the feature map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:34:48.924269171Z",
     "start_time": "2023-11-16T04:34:48.887632062Z"
    },
    "id": "PEdeDEo4Tvwc"
   },
   "outputs": [],
   "source": [
    "# Instantiate quantum kernel\n",
    "quant_kernel = TrainableFidelityQuantumKernel(feature_map=fm, training_parameters=training_params)\n",
    "\n",
    "# Set up the optimizer\n",
    "cb_qkt = QKTCallback()\n",
    "spsa_opt = SPSA(maxiter=10, callback=cb_qkt.callback, learning_rate=0.05, perturbation=0.05)\n",
    "\n",
    "# Instantiate a quantum kernel trainer.\n",
    "qkt = QuantumKernelTrainer(\n",
    "    quantum_kernel=quant_kernel, loss=\"svc_loss\", optimizer=spsa_opt, initial_point=[np.pi / 2]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPWCe6x9Tvwc"
   },
   "source": [
    "### Train the Quantum Kernel\n",
    "\n",
    "To train the quantum kernel on the dataset (samples and labels), we call the `fit` method of `QuantumKernelTrainer`.\n",
    "\n",
    "The output of `QuantumKernelTrainer.fit` is a `QuantumKernelTrainerResult` object. The results object contains the following class fields:\n",
    "\n",
    " - `optimal_parameters`: A dictionary containing {parameter: optimal value} pairs\n",
    " - `optimal_point`: The optimal parameter value found in training\n",
    " - `optimal_value`: The value of the loss function at the optimal point\n",
    " - `optimizer_evals`: The number of evaluations performed by the optimizer\n",
    " - `optimizer_time`: The amount of time taken to perform optimization\n",
    " - `quantum_kernel`: A `TrainableKernel` object with optimal values bound to the feature map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:04.452515867Z",
     "start_time": "2023-11-16T04:34:48.895839163Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVAa8TK1Tvwc",
    "outputId": "2a95fedb-dacc-454f-8810-38ec5ca4e716"
   },
   "outputs": [],
   "source": [
    "# Train the kernel using QKT directly\n",
    "qka_results = qkt.fit(X_train, y_train)\n",
    "optimized_kernel = qka_results.quantum_kernel\n",
    "print(qka_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05ruRz_9Tvwc"
   },
   "source": [
    "### Fit and Test the Model\n",
    "\n",
    "We can pass the trained quantum kernel to a machine learning model, then fit the model and test on new data. Here, we will use Qiskit Machine Learning's `QSVC` for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:09.782996890Z",
     "start_time": "2023-11-16T04:37:04.382482686Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fck27soQTvwc",
    "outputId": "fda0d679-0a38-4007-a8f4-288983abcaee"
   },
   "outputs": [],
   "source": [
    "# Use QSVC for classification\n",
    "qsvc = QSVC(quantum_kernel=optimized_kernel)\n",
    "\n",
    "# Fit the QSVC\n",
    "qsvc.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels\n",
    "labels_test = qsvc.predict(X_test)\n",
    "\n",
    "# Evalaute the test accuracy\n",
    "accuracy_test = metrics.balanced_accuracy_score(y_true=y_test, y_pred=labels_test)\n",
    "print(f\"accuracy test: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vg0iXpKTvwd"
   },
   "source": [
    "### Visualize the Kernel Training Process\n",
    "\n",
    "From the callback data, we can plot how the loss evolves during the training process. We see it converges rapidly and reaches high test accuracy on this dataset with our choice of inputs.\n",
    "\n",
    "We can also display the final kernel matrix, which is a measure of similarity between the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:14.582158746Z",
     "start_time": "2023-11-16T04:37:09.731551938Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "rIc3dymGTvwd",
    "outputId": "6ed02e77-e5a2-4d78-efc4-4a8a52bc74cb"
   },
   "outputs": [],
   "source": [
    "plot_data = cb_qkt.get_callback_data()  # callback data\n",
    "K = optimized_kernel.evaluate(X_train)  # kernel matrix evaluated on the training samples\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax[0].plot([i + 1 for i in range(len(plot_data[0]))], np.array(plot_data[2]), c=\"k\", marker=\"o\")\n",
    "ax[0].set_xlabel(\"Iterations\")\n",
    "ax[0].set_ylabel(\"Loss\")\n",
    "ax[1].imshow(K, cmap=matplotlib.colormaps[\"bwr\"])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLCHUIN5Tvwd"
   },
   "source": [
    "## How To Create Custom Kernel Loss Functions\n",
    "\n",
    "In this guide, we show how to create custom kernel loss functions for quantum kernel training. This is useful if we desire the resulting kernel exhibit certain properties such as low rank, or high generalization to unseen data when used for classification. For this purpose, one may wish to evaluate a kernel on some dataset, $X$, and compute properties of the resulting kernel matrix, $K$, or optimize quantities related to the model weights of a [Support Vector Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) that has been trained using $K$.\n",
    "\n",
    "In order to ensure compatibility with Qiskit's optimizers, it is neccesary that kernel loss functions follow the following input-output behavior:\n",
    "\n",
    "- A kernel loss function must take as input a single array of trainable parameter values\n",
    "- A kernel loss function must return a real number\n",
    "\n",
    "Despite these constraints, the data, labels, `TrainableFidelityQuantumKernel` object, and any other necessary inputs must be made available to the kernel loss's internal logic. Below we suggest two approaches:\n",
    "\n",
    "1. Provide all inputs to kernel loss as class variables\n",
    "2. Wrap kernel loss function using `partial`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:14.615404781Z",
     "start_time": "2023-11-16T04:37:14.587129584Z"
    },
    "id": "AHqdZohATvwd"
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import Qiskit packages\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit_machine_learning.kernels import TrainableFidelityQuantumKernel\n",
    "from qiskit_machine_learning.utils.loss_functions import KernelLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peKs0YahTvwd"
   },
   "source": [
    "Let's start by setting up a quantum kernel, data, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:14.800563386Z",
     "start_time": "2023-11-16T04:37:14.594487604Z"
    },
    "id": "n_EcO2k9Tvwe"
   },
   "outputs": [],
   "source": [
    "feature_map = ZZFeatureMap(3)\n",
    "user_params = [feature_map.parameters[1]]\n",
    "\n",
    "qkernel = TrainableFidelityQuantumKernel(\n",
    "    feature_map=feature_map,\n",
    "    training_parameters=user_params,\n",
    ")\n",
    "\n",
    "X_train = [[0.6, 0.2], [0.5, 0.3], [0.3, 0.7], [0.1, 0.5]]\n",
    "y_train = np.array([0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KLctZf1Tvwe"
   },
   "source": [
    "### Option 1: Provide all inputs to kernel loss as class variables\n",
    "\n",
    "The preferred option for building a custom kernel loss function is to mimic the structure of Qiskit's pre-defined kernel loss functions, such as `SVCLoss`.\n",
    "\n",
    "In order to match the behavior of existing kernel loss methods, your class should:\n",
    "- Extend `KernelLoss`\n",
    "- Accept `kwargs` in the constructor\n",
    "- Include a `evaluate` method that takes all neccesary inputs and returns the evaluated loss\n",
    "- Include a `get_variational_callable` method that wraps `evaluate` with all inputs except the trainable parameter values `user_param_values`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:14.830427547Z",
     "start_time": "2023-11-16T04:37:14.676606126Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pzxWhLj_Tvwe",
    "outputId": "cc552630-a691-43f6-b23e-dba5b3c63285"
   },
   "outputs": [],
   "source": [
    "class CustomKernelLoss(KernelLoss):\n",
    "    \"\"\"Example Kernel Loss class\"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    # Evaluate the Loss of a trainable quantum kernel\n",
    "    # at a particular setting of user_param_values on\n",
    "    # a particular dataset.\n",
    "    def evaluate(\n",
    "        self,\n",
    "        parameter_values: Sequence[float],\n",
    "        quantum_kernel: TrainableFidelityQuantumKernel,\n",
    "        data: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "    ):\n",
    "        # Bind the user parameter values\n",
    "        quantum_kernel.assign_training_parameters(parameter_values)\n",
    "        kernel_matrix = quantum_kernel.evaluate(data)\n",
    "        return labels.T @ kernel_matrix @ labels\n",
    "\n",
    "    # Wrap our evaluate method so to produce a callable\n",
    "    # which maps user_param_values to loss scores.\n",
    "    def get_variational_callable(\n",
    "        self,\n",
    "        quantum_kernel: TrainableFidelityQuantumKernel,\n",
    "        data: np.ndarray,\n",
    "        labels: np.ndarray,\n",
    "    ):\n",
    "        return partial(\n",
    "            self.evaluate, quantum_kernel=quantum_kernel, data=data, labels=labels\n",
    "        )\n",
    "\n",
    "\n",
    "kernel_loss = CustomKernelLoss().get_variational_callable(qkernel, X_train, y_train)\n",
    "print(kernel_loss([0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TerV-fjXTvwe"
   },
   "source": [
    "We now have a black box callable that assigns a numeric value to any set of user parameter bindings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:19.400388369Z",
     "start_time": "2023-11-16T04:37:14.762298941Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "Ea-1SSL0Tvwe",
    "outputId": "f2c92cd2-a04a-4017-f2d3-7e9baa808e62"
   },
   "outputs": [],
   "source": [
    "NUM_GRID_POINTS = 100\n",
    "loss_values = [kernel_loss([val]) for val in np.linspace(0, 2 * np.pi, NUM_GRID_POINTS)]\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 15\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.linspace(0, 2 * np.pi, NUM_GRID_POINTS), loss_values)\n",
    "plt.xlabel(\"θ\")\n",
    "plt.ylabel(\"Kernel Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yDSgrPjiTvwe"
   },
   "source": [
    "### Option 2: Define kernel loss as a method and wrap with `partial`\n",
    "If you do not wish to define a full class for your kernel loss (e.g., to minimize lines of code), you are always free to define your kernel loss as a standalone function that is then wrapped with `partial` to enforce the expected input/output behavior. Note that if your inputs are defined within scope of your kernel loss method, you can actually avoid using `partial`; however, this will often not be feasible in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:19.487030453Z",
     "start_time": "2023-11-16T04:37:19.421332517Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X5VlO2QpTvwe",
    "outputId": "2df90abc-90c4-47e9-af58-71f3e6c3f1a0"
   },
   "outputs": [],
   "source": [
    "def kernel_loss_full(training_param_values, kernel, data, labels):\n",
    "    kernel.assign_training_parameters(training_param_values)\n",
    "    kernel_matrix = kernel.evaluate(data)\n",
    "    return labels.T @ kernel_matrix @ labels\n",
    "\n",
    "\n",
    "kernel_loss = partial(kernel_loss_full, kernel=qkernel, data=X_train, labels=y_train)\n",
    "print(kernel_loss([0.75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EcOv0OixTvwf"
   },
   "source": [
    "## How To Create Custom Quantum Feature Maps\n",
    "\n",
    "In machine learning, a feature map represents a transformation of data into a higher-dimensional space. However, this can be an expensive computation. Instead, kernel functions can be used to implicitly encode this transformation through the pairwise inner products of data samples. Kernels are a similarity measure over the dataset and are a key component of many machine learning models, for example, support vector machines. A quantum computer can be used to encode classical data into the quantum state space. We call this a quantum feature map.\n",
    "\n",
    "In this guide, we will show how to create a custom quantum feature map with trainable parameters, which may be used as input to Qiskit machine learning algorithms such as `QSVC` and `QuantumKernelTrainer`. We will follow four basic steps:\n",
    "\n",
    "1. Import required Qiskit packages\n",
    "2. Design the circuit for the quantum feature map\n",
    "3. Build the circuit with Qiskit\n",
    "4. Implement the feature map as a `QuantumCircuit` child class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PysiZ0CTvwf"
   },
   "source": [
    "### Import Required Packages\n",
    "\n",
    "To create a quantum feature map with trainable parameters in Qiskit, there are two basic guidelines.<br>\n",
    "The quantum feature map should:\n",
    " - Be an extension of Qiskit's `QuantumCircuit` class\n",
    " - Contain some number of trainable user parameters, `θ`, in addition to parameters designated to input data, `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:19.487305654Z",
     "start_time": "2023-11-16T04:37:19.466931441Z"
    },
    "id": "YbZAddcATvwf"
   },
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOKZpslfTvwf"
   },
   "source": [
    "### Design the Circuit\n",
    "Similarly to classical feature engineering, creating a quantum feature map is a process that strongly depends on the learning problem at hand. In general, we cannot suggest an optimal feature map with no prior knowledge of the learning problem. Instead, we will focus on the basic steps to create a circuit using the Qiskit API. To illustrate, we will build a version of the [covariant feature map](https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/qkt/feature_maps/covariant_feature_map.py), which is tailored to a dataset with a particular structure. Check out [this guide](https://github.com/qiskit-community/prototype-quantum-kernel-training/blob/main/docs/background/qkernels_and_data_w_group_structure.ipynb) for more information on covariant quantum kernels.\n",
    "\n",
    "For this example, the feature map will be built from a circuit containing trainable parameters `θ` followed by a circuit encoding the input data `x`. The trainable parameter of the $i$th qubit corresponds to a rotation around the $y$-axis by an angle `θ[i]`. We follow this by an entanglement layer of controlled-$z$ gates. Finally, we encode two features `x[i], x[i+1]` per qubit using consecutive rotations around the $x$ and $z$ axes.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zzO89i3JTvwf"
   },
   "source": [
    "### Build the Circuit with Qiskit\n",
    "\n",
    "First, we instantiate a `QuantumCircuit`  and create the circuit layer with trainable parameters `θ[i]`. Here, we will assume we are given a dataset with 12 features and we encode two features per qubit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:19.811713506Z",
     "start_time": "2023-11-16T04:37:19.510375426Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "HOjL1e3qTvwf",
    "outputId": "e64ae9e3-f24e-4aaf-e95f-7a54c258b39e"
   },
   "outputs": [],
   "source": [
    "# For a dataset with 12 features; and 2 features per qubit\n",
    "FEATURE_DIMENSION = 12\n",
    "NUM_QUBITS = int(FEATURE_DIMENSION / 2)\n",
    "\n",
    "# Qiskit feature maps should generally be QuantumCircuits or extensions of QuantumCircuit\n",
    "feature_map = QuantumCircuit(NUM_QUBITS)\n",
    "user_params = ParameterVector(\"θ\", NUM_QUBITS)\n",
    "\n",
    "# Create circuit layer with trainable parameters\n",
    "for i in range(NUM_QUBITS):\n",
    "    feature_map.ry(user_params[i], feature_map.qubits[i])\n",
    "\n",
    "feature_map.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk4njHFaTvwg"
   },
   "source": [
    "Next, we will define an entanglement scheme (a linear map of controlled-$z$ gates) and create the entanglement layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:20.161915156Z",
     "start_time": "2023-11-16T04:37:19.814830607Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "XgzNdAkOTvwg",
    "outputId": "2c9efa60-51ce-4fcb-d8bb-c52ef95b5e4d"
   },
   "outputs": [],
   "source": [
    "# Linear entanglement\n",
    "entanglement = [[i, i + 1] for i in range(NUM_QUBITS - 1)]\n",
    "\n",
    "for source, target in entanglement:\n",
    "    feature_map.cz(feature_map.qubits[source], feature_map.qubits[target])\n",
    "\n",
    "feature_map.barrier()\n",
    "\n",
    "feature_map.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTRfeaZGTvwg"
   },
   "source": [
    "Finally, we encode two features `x[i], x[i+1]` per qubit using a layer of single-qubit rotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:20.632793115Z",
     "start_time": "2023-11-16T04:37:20.195773903Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "AJphT8DHTvwg",
    "outputId": "05f3fd91-cae6-4fe4-c2fe-da643708f806"
   },
   "outputs": [],
   "source": [
    "input_params = ParameterVector(\"x\", FEATURE_DIMENSION)\n",
    "for i in range(NUM_QUBITS):\n",
    "    feature_map.rz(input_params[2 * i + 1], feature_map.qubits[i])\n",
    "    feature_map.rx(input_params[2 * i], feature_map.qubits[i])\n",
    "\n",
    "feature_map.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7HPVsn-Tvwg"
   },
   "source": [
    "### Implement the Feature Map as a `QuantumCircuit` Child Class\n",
    "\n",
    "Most Qiskit algorithms that take feature maps as input require the feature map be a class extension of a `QuantumCircuit`. While there are many ways to do this, we suggest the following approach illustrated with `ExampleFeatureMap` that extends `QuantumCircuit`:\n",
    "\n",
    "The feature map circuit is created upon instantiation such that\n",
    " - Parameters such as feature dimension and entanglement scheme should be specified during initialization\n",
    " - In the initialization, `QuantumCircuit.__init__()` is called before the feature map circuit is generated, which ensures all `QuantumCircuit` class fields (e.g. `QuantumCircuit.qubits`) are properly initialized\n",
    " - After the `QuantumCircuit` constructor has been called, a class method `_generate_feature_map` generates the feature map circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:20.635109101Z",
     "start_time": "2023-11-16T04:37:20.620995488Z"
    },
    "id": "jw0YovV3Tvwg"
   },
   "outputs": [],
   "source": [
    "class ExampleFeatureMap(QuantumCircuit):\n",
    "    \"\"\"The Example Feature Map circuit\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dimension: int,\n",
    "        entanglement: Union[str, List[List[int]], Callable[[int], List[int]]] = None,\n",
    "        name: str = \"ExampleFeatureMap\",\n",
    "    ) -> None:\n",
    "        \"\"\"Create a new Example Feature Map circuit.\n",
    "        Args:\n",
    "            feature_dimension: The number of features\n",
    "            entanglement: Entanglement scheme to be used in second layer\n",
    "            name: Name of QuantumCircuit object\n",
    "\n",
    "        Raises:\n",
    "            ValueError: ExampleFeatureMap requires an even number of input features\n",
    "        \"\"\"\n",
    "        if (feature_dimension % 2) != 0:\n",
    "            raise ValueError(\n",
    "                \"\"\"\n",
    "            Example feature map requires an even number of input features.\n",
    "                \"\"\"\n",
    "            )\n",
    "        self.feature_dimension = feature_dimension\n",
    "        self.entanglement = entanglement\n",
    "        self.training_parameters = None\n",
    "\n",
    "        # Call the QuantumCircuit initialization\n",
    "        num_qubits = feature_dimension / 2\n",
    "        super().__init__(\n",
    "            num_qubits,\n",
    "            name=name,\n",
    "        )\n",
    "\n",
    "        # Build the feature map circuit\n",
    "        self._generate_feature_map()\n",
    "\n",
    "    def _generate_feature_map(self):\n",
    "        # If no entanglement scheme specified, use linear entanglement\n",
    "        if self.entanglement is None:\n",
    "            self.entanglement = [[i, i + 1] for i in range(self.num_qubits - 1)]\n",
    "\n",
    "        # Vector of data parameters\n",
    "        input_params = ParameterVector(\"x\", self.feature_dimension)\n",
    "\n",
    "        training_params = ParameterVector(\"θ\", self.num_qubits)\n",
    "        # Create an initial rotation layer of trainable parameters\n",
    "        for i in range(self.num_qubits):\n",
    "            self.ry(training_params[i], self.qubits[i])\n",
    "\n",
    "        self.training_parameters = training_params\n",
    "\n",
    "        # Create the entanglement layer\n",
    "        for source, target in self.entanglement:\n",
    "            self.cz(self.qubits[source], self.qubits[target])\n",
    "\n",
    "        self.barrier()\n",
    "\n",
    "        # Create a circuit representation of the data group\n",
    "        for i in range(self.num_qubits):\n",
    "            self.rz(input_params[2 * i + 1], self.qubits[i])\n",
    "            self.rx(input_params[2 * i], self.qubits[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUt8lnUCTvwg"
   },
   "source": [
    "### Instantiate and Inspect the Example Feature Map\n",
    "\n",
    "Finally, we will instantiate and inspect an `ExampleFeatureMap` object. We will use `feature_dimension=10` and the default linear entanglement, which should produce a 5-qubit feature map circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.162342241Z",
     "start_time": "2023-11-16T04:37:20.625733087Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "pLy4osXLTvwg",
    "outputId": "0c95ec08-276d-491b-d928-c892de8edb8f"
   },
   "outputs": [],
   "source": [
    "feature_map = ExampleFeatureMap(feature_dimension=10)\n",
    "feature_map.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5KCYOSjTvwg"
   },
   "source": [
    "## How To Assign Training Parameters to a Quantum Kernel\n",
    "\n",
    "In this guide, we show the ins and outs of assigning training parameters to a `TrainableKernel` instance using Qiskit Machine Learning.\n",
    "\n",
    "We can create a `TrainableFidelityQuantumKernel` (`QK`) and specify our feature map and trainable parameters. This can be done at initialization by passing an array of `Parameters` as the `training_parameters` argument to the `QK` constructor.\n",
    "\n",
    "After the `QK.training_parameters` field has been set, `QK.assign_training_parameters()` offers two ways to assign values to the training parameters\n",
    "\n",
    "1. Bind training parameters using a dictionary\n",
    "    - Keys to dict must be parameters within the feature map and must exist in `QK.training_parameters`\n",
    "    - Values in dict may be either numerical assignments or `ParameterExpression` objects\n",
    "2. Bind user parameters using a list of values\n",
    "    - If binding using a list of values, the list must be of same size and ordering as `QK.training_parameters`. Each input value will be bound to its corresponding ``training_parameters`` value.\n",
    "     \n",
    "We begin by importing a few packages and instantiating a feature map circuit with three trainable parameters, `θ`, and three input parameters, `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.165681515Z",
     "start_time": "2023-11-16T04:37:21.138267724Z"
    },
    "id": "HzFmLdIwTvwh"
   },
   "outputs": [],
   "source": [
    "# pylint: disable=import-error, wrong-import-position, pointless-statement\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit_machine_learning.kernels import TrainableFidelityQuantumKernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.444716598Z",
     "start_time": "2023-11-16T04:37:21.148314340Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "K8KqqfwWTvwh",
    "outputId": "0964a214-20bb-44ff-f8a4-5e08c01d9147"
   },
   "outputs": [],
   "source": [
    "NUM_QUBITS = 3\n",
    "fm = QuantumCircuit(NUM_QUBITS)\n",
    "input_params = ip = ParameterVector(\"x\", NUM_QUBITS)\n",
    "training_params = tp = ParameterVector(\"θ\", NUM_QUBITS)\n",
    "\n",
    "for i in range(NUM_QUBITS):\n",
    "    fm.h(i)\n",
    "    fm.ry(tp[i], i)\n",
    "\n",
    "for i in range(NUM_QUBITS):\n",
    "    fm.crx(ip[i], (i) % NUM_QUBITS, (i + 1) % NUM_QUBITS)\n",
    "\n",
    "# Define a Quantum Kernel using our trainable feature map\n",
    "qk = TrainableFidelityQuantumKernel(\n",
    "    feature_map=fm, training_parameters=training_params[:NUM_QUBITS]\n",
    ")\n",
    "\n",
    "print(\"input_params:\", input_params)\n",
    "print(\"training_params:\", training_params)\n",
    "qk.feature_map.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "oYir5SkkTvwh",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Option  1: Bind User Parameters with a Dictionary\n",
    "\n",
    "Here, we will use a dictionary of the form `{Parameter : Value}` that maps training parameters to either numeric values or `ParameterExpression` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.446343826Z",
     "start_time": "2023-11-16T04:37:21.422518508Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nINCUEwtTvwh",
    "outputId": "e0fbade5-34d3-456c-81c4-1da544a7cdf5"
   },
   "outputs": [],
   "source": [
    "# Bind parameters to numeric values\n",
    "param_binds = {tp[0]: np.pi / 2, tp[1]: np.pi / 3, tp[2]: np.pi / 4}\n",
    "\n",
    "qk.assign_training_parameters(param_binds)\n",
    "qk.parameter_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sChAt--1Tvwh"
   },
   "source": [
    "We are free to bind a subset of our training parameters and re-bind parameters to new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.446952699Z",
     "start_time": "2023-11-16T04:37:21.423045969Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IzZJpGjITvwh",
    "outputId": "52a0a9b1-c63a-48c5-e229-8556a2f8b37a"
   },
   "outputs": [],
   "source": [
    "# Create incomplete training param bindings\n",
    "param_binds = {tp[0]: np.pi / 6, tp[1]: np.pi / 5}\n",
    "\n",
    "qk.assign_training_parameters(param_binds)\n",
    "qk.parameter_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k27Iy1-0Tvwh"
   },
   "source": [
    "We can  un-bind our training parameters or assign training parameters to different `ParameterExpression` objects. This is done in in the same way that we would bind numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.673627241Z",
     "start_time": "2023-11-16T04:37:21.423472380Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gwFsO8p7Tvwh",
    "outputId": "fd6481fd-ac5b-4579-c278-2c3bea8d83ad"
   },
   "outputs": [],
   "source": [
    "# Create incomplete user param bindings\n",
    "param_binds = {tp[0]: tp[0], tp[1]: tp[0] + tp[2], tp[2]: tp[2]}\n",
    "\n",
    "qk.assign_training_parameters(param_binds)\n",
    "qk.parameter_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xvf2uyB-Tvwh"
   },
   "source": [
    "### Option 2: Bind Training Parameters with a List\n",
    "\n",
    "If the `training_parameters` have been specified in the `QuantumKernel`, we may bind and unbind those parameters using only lists of parameter values. Note that the list of values must always be equal in size to the `QuantumKernel.training_parameters` array, and the values will be assigned in order.\n",
    "\n",
    "Here we instantiate a new quantum kernel with the three training parameters unbound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.844504915Z",
     "start_time": "2023-11-16T04:37:21.507275996Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "Wroq87XhTvwh",
    "outputId": "26685ece-824e-4ab9-c25e-de38bcc8d32a"
   },
   "outputs": [],
   "source": [
    "qk = TrainableFidelityQuantumKernel(feature_map=fm, training_parameters=training_params)\n",
    "qk.feature_map.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsSXeNN_Tvwi"
   },
   "source": [
    "We may want to assign numerical values to parameters 0 and 2, while leaving parameter 1 unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.845213852Z",
     "start_time": "2023-11-16T04:37:21.730941180Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNQaN3mfTvwi",
    "outputId": "b95b7d39-5c0c-4f0e-f62a-255d3cc35894"
   },
   "outputs": [],
   "source": [
    "param_values = [np.pi / 7, tp[1], np.pi / 9]\n",
    "qk.assign_training_parameters(param_values)\n",
    "qk.parameter_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3tb5F58Tvwi"
   },
   "source": [
    "To assign parameter 1 to a numerical value, while leaving parameters 0 and 2 unchaged, we pass in a full list of the new values (values 0 and 2 will remain the same.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.845778677Z",
     "start_time": "2023-11-16T04:37:21.731586870Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RzyFWNV6Tvwi",
    "outputId": "3523a405-4b74-4927-8334-15f3648441be"
   },
   "outputs": [],
   "source": [
    "param_values = [np.pi / 7, np.pi / 6, np.pi / 9]\n",
    "qk.assign_training_parameters(param_values)\n",
    "qk.parameter_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZTfk5STTTvwi"
   },
   "source": [
    "Finally, if we want to unbind all of our parameters, we may just pass in a list of the parameters themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.852352699Z",
     "start_time": "2023-11-16T04:37:21.732389124Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3mcajITTvwi",
    "outputId": "fdca29c8-0235-42e1-c2aa-9a694fccc9c6"
   },
   "outputs": [],
   "source": [
    "param_values = [tp[0], tp[1], tp[2]]\n",
    "qk.assign_training_parameters(param_values)\n",
    "qk.parameter_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kNt8OVVETvwi"
   },
   "source": [
    "## How To Build Trainable Feature Maps from the Qiskit Circuit Library\n",
    "\n",
    "In this guide, we will show how to build trainable feature maps from existing circuits in the Qiskit circuit library. Each approach will involve reassigning some parameters originally reserved for input data to instead be trainable parameters.\n",
    "\n",
    "To build a trainable feature map, we require the following:\n",
    "\n",
    "1. A circuit containing parameterized gates\n",
    "2. A partition of circuit parameters into two sets: input parameters (encode the data) and user (trainable) parameters\n",
    "3. After partitioning parameters, the dimensionality of the input data must equal the number of input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUVcjzG8Tvwi"
   },
   "source": [
    "### Option 1: Partition the Parameters of a Single Circuit\n",
    "\n",
    "The main distinction between a feature map and a _parameterized_ feature map is the presence of parameters not associated to our input data. In other words, a feature map contains input parameters (encoding the dataset), and a parameterized feature map contains both input parameters as well as user parameters (which are trainable).\n",
    "\n",
    "One way to generate a parameterized feature map from an existing Qiskit feature map is to reassign some of the input parameters to be user parameters instead. If you go down this path, take care to ensure that you retain enough input parameters to match the dimensionality of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:21.852853161Z",
     "start_time": "2023-11-16T04:37:21.732962921Z"
    },
    "id": "l3zpByBBTvwi"
   },
   "outputs": [],
   "source": [
    "# pylint: disable=protected-access\n",
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "from qiskit.circuit import ParameterVector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLS5BgDFTvwj"
   },
   "source": [
    "Let's start with a two-qubit feature map from the Qiskit circuit library. By default, this is not parametrized and contains two input parameters `x[0]` and `x[1]` encoding the components of each data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:22.146510311Z",
     "start_time": "2023-11-16T04:37:21.765715191Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "bdwRD5FBTvwj",
    "outputId": "696a65ad-fc51-4568-b5f7-3d23d7eda9f3"
   },
   "outputs": [],
   "source": [
    "# Define a (non-parameterized) feature map from the Qiskit circuit library\n",
    "fm = ZZFeatureMap(2)\n",
    "input_params = fm.parameters\n",
    "fm.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0pa1HrkJTvwj"
   },
   "source": [
    "Let's partition the input parameters into two sets such that the second one is reassigned to be a user (trainable) parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:22.194138986Z",
     "start_time": "2023-11-16T04:37:22.000885028Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NUYFVdjsTvwj",
    "outputId": "f20fd82e-0004-4762-8ec6-b8f5cbd5e691"
   },
   "outputs": [],
   "source": [
    "# split params into two disjoint sets\n",
    "input_params = fm.parameters[::2]\n",
    "training_params = fm.parameters[1::2]\n",
    "print(\"input_params:\", input_params)\n",
    "print(\"training_params:\", training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOVUQFnLTvwj"
   },
   "source": [
    "For clarity, we will manually reassign the feature map parameters such that the new parameters are properly named. (Renaming is not strictly required in this example; however, in the example below it will be necessary to prevent name collisions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:22.434391451Z",
     "start_time": "2023-11-16T04:37:22.042979647Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "W_JqvvlqTvwj",
    "outputId": "30f981b9-4ab5-43f6-e1fd-d200584d0088"
   },
   "outputs": [],
   "source": [
    "# define new parameter vectors for the input and user parameters\n",
    "new_input_params = ParameterVector(\"x\", len(input_params))\n",
    "new_training_params = ParameterVector(\"θ\", len(training_params))\n",
    "\n",
    "# resassign the origin feature map parameters\n",
    "param_reassignments = {}\n",
    "for i, p in enumerate(input_params):\n",
    "    param_reassignments[p] = new_input_params[i]\n",
    "for i, p in enumerate(training_params):\n",
    "    param_reassignments[p] = new_training_params[i]\n",
    "\n",
    "fm.assign_parameters(param_reassignments, inplace=True)\n",
    "\n",
    "input_params = new_input_params\n",
    "training_params = new_training_params\n",
    "\n",
    "print(\"input_params:\", input_params)\n",
    "print(\"training_params:\", training_params)\n",
    "fm.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArnYeRVTTvwj"
   },
   "source": [
    "### Option 2: Compose Multiple Circuits\n",
    "\n",
    "We can build a parameterized feature map out of existing Qiskit library circuits by composing them to form a larger composite circuit. However, if two circuits have (different) parameters that share the same name, Qiskit will not allow us to compose them.\n",
    "\n",
    "To resolve this issue, we will simply rename our user parameters to prevent name collisions. As a nice side effect, our parameter names will also be more accurate and helpful. Again, note that our parameter names are automatically updated in our feature map circuit.\n",
    "\n",
    "*Note: although both options we show in this guide use two qubits, Option 2 results in a feature map that accepts two-dimensional data while Option 1 results in a feature map for one-dimensional data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-16T04:37:22.518011478Z",
     "start_time": "2023-11-16T04:37:22.202344019Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "mOzjlPqUTvwj",
    "outputId": "151821c4-62f3-4506-c951-7e6a9d20d212"
   },
   "outputs": [],
   "source": [
    "# Define two circuits\n",
    "circ1 = ZZFeatureMap(2)\n",
    "circ2 = ZZFeatureMap(2)\n",
    "input_params = circ1.parameters\n",
    "training_params = ParameterVector(\"θ\", 2)\n",
    "\n",
    "# Reassign new parameters to circ2 so there are no name collisions\n",
    "circ2.assign_parameters(training_params, inplace=True)\n",
    "\n",
    "# Compose to build a parameterized feature map\n",
    "fm = circ2.compose(circ1)\n",
    "print(\"input_params:\", list(input_params))\n",
    "print(\"training_params:\", training_params)\n",
    "fm.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tO1xEP7GTvwk",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Send it after class part 1\n",
    "\n",
    "SVMs, or \"Support-vector machines\", are used for many artificial intelligence tasks because they don't require a lot of computational power to achieve acceptable results.  Quantum SVMs are interesting in the era of quantum computing since, for certain types of data, a quantum kernel can provide a significant speed-up. Of course, the speed-up only occurs for certain types of data that require complex kernels. (A linear kernel will remain faster). In some cases, it's even possible to achieve better prediction accuracy since one can convert into a space that is not classically accessible.\n",
    "\n",
    "## Classification of the \"Iris datasets\" data\n",
    "\n",
    "To provide us with a comparison point that's simpler than a dataset from language processing, this will prevent the main issue from being dimensionality reduction.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "The data will be a small subset of the well known handwritten digits dataset, which is available through scikit-learn. We will be aiming to differentiate between '0' and '1'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "glCnWBhbTvwk",
    "outputId": "72c7ac9e-3125-4320-82a4-7660704def8e"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn import datasets\n",
    "# Load digits dataset\n",
    "digits = datasets.load_digits(n_class=2)\n",
    "\n",
    "# Plot example '0' and '1'\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6,3))\n",
    "axs[0].set_axis_off()\n",
    "axs[0].imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "axs[1].set_axis_off()\n",
    "axs[1].imshow(digits.images[1], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "syqBHZ1kTvwk",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "\n",
    "Data Preprocessing\n",
    "\n",
    "There are a total of 360 datapoints in the dataset. Each datapoint is a 8x8 image of a digit, collapsed into an array, where each element is an integer between 0 (white) and 16 (black). As per classical classification, we need to split the dataset into training (100) and testing (20) samples, and normalise it. To use the dataset for quantum classification, we need to scale the range to between -1 and 1, and reduce the dimensionality to the number of qubits we want to use (4).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CswUU0HBTvwk"
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "sample_train, sample_test, label_train, label_test = train_test_split(\n",
    "     digits.data, digits.target, test_size=0.2, random_state=22)\n",
    "\n",
    "# Reduce dimensions\n",
    "n_dim = 4\n",
    "pca = PCA(n_components=n_dim).fit(sample_train)\n",
    "sample_train = pca.transform(sample_train)\n",
    "sample_test = pca.transform(sample_test)\n",
    "\n",
    "# Normalise\n",
    "std_scale = StandardScaler().fit(sample_train)\n",
    "sample_train = std_scale.transform(sample_train)\n",
    "sample_test = std_scale.transform(sample_test)\n",
    "\n",
    "# Scale\n",
    "samples = np.append(sample_train, sample_test, axis=0)\n",
    "minmax_scale = MinMaxScaler((-1, 1)).fit(samples)\n",
    "sample_train = minmax_scale.transform(sample_train)\n",
    "sample_test = minmax_scale.transform(sample_test)\n",
    "\n",
    "# Select\n",
    "train_size = 100\n",
    "sample_train = sample_train[:train_size]\n",
    "label_train = label_train[:train_size]\n",
    "\n",
    "test_size = 20\n",
    "sample_test = sample_test[:test_size]\n",
    "label_test = label_test[:test_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KVMHSvDTvwk",
    "outputId": "e4313114-0229-47fb-fe51-f25d4c5e1173"
   },
   "outputs": [],
   "source": [
    "print(sample_train[0], label_train[0])\n",
    "print(sample_test[0], label_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Dol8MK5DTvwk",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Data Encoding\n",
    "\n",
    "We will take the classical data and encode it to the quantum state space using a quantum feature map. The choice of which feature map to use is important and may depend on the given dataset we want to classify. Here we'll look at the feature maps available in Qiskit, before selecting and customising one to encode our data.\n",
    "\n",
    "### Quantum Feature Maps\n",
    "\n",
    "As the name suggests, a quantum feature map $\\phi(\\mathbf{x})$ is a map from the classical feature vector $\\mathbf{x}$ to the quantum state $|\\Phi(\\mathbf{x})\\rangle\\langle\\Phi(\\mathbf{x})|$. This is faciliated by applying the unitary operation $\\mathcal{U}_{\\Phi(\\mathbf{x})}$ on the initial state $|0\\rangle^{n}$ where _n_ is the number of qubits being used for encoding.\n",
    "\n",
    "\n",
    "The feature maps currently available in Qiskit `PauliFeatureMap`, `ZZFeatureMap` and `ZFeatureMap` are those introduced in [_Havlicek et al_.  Nature **567**, 209-212 (2019)](https://www.nature.com/articles/s41586-019-0980-2), in particular the `ZZFeatureMap` is conjectured to be hard to simulate classically and can be implemented as short-depth circuits on near-term quantum devices.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "Tg4UI6Y2Tvwk",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "The `PauliFeatureMap` is describes the unitary operator of depth $d$:\n",
    "\n",
    "$$ \\mathcal{U}_{\\Phi(\\mathbf{x})}=\\prod_d U_{\\Phi(\\mathbf{x})}H^{\\otimes n},\\ U_{\\Phi(\\mathbf{x})}=\\exp\\left(i\\sum_{S\\subseteq[n]}\\phi_S(\\mathbf{x})\\prod_{k\\in S} P_i\\right), $$\n",
    "\n",
    "which contains layers of Hadamard gates interleaved with entangling blocks, $U_{\\Phi(\\mathbf{x})}$, encoding the classical data as shown in circuit diagram below for $d=2$.\n",
    "\n",
    "Within the entangling blocks, $U_{\\Phi(\\mathbf{x})}$: $P_i \\in \\{ I, X, Y, Z \\}$ denotes the Pauli matrices, the index $S$ describes connectivities between different qubits or datapoints: $S \\in \\{\\binom{n}{k}\\ combinations,\\ k = 1,... n \\}$, and by default the data mapping function $\\phi_S(\\mathbf{x})$ is\n",
    "$$\\phi_S:\\mathbf{x}\\mapsto \\Bigg\\{\\begin{array}{ll}\n",
    "    x_i & \\mbox{if}\\ S=\\{i\\} \\\\\n",
    "        (\\pi-x_i)(\\pi-x_j) & \\mbox{if}\\ S=\\{i,j\\}\n",
    "    \\end{array}$$\n",
    "\n",
    "when $k = 1, P_0 = Z$, this is the `ZFeatureMap`:\n",
    "$$\\mathcal{U}_{\\Phi(\\mathbf{x})} = \\left( \\exp\\left(i\\sum_j \\phi_{\\{j\\}}(\\mathbf{x}) \\, Z_j\\right) \\, H^{\\otimes n} \\right)^d.$$\n",
    "\n",
    "note the lack of entanglement in this feature map, this means that this feature map is simple to simulate classically and will not provide quantum advantage.\n",
    "\n",
    "and when $k = 2, P_0 = Z, P_1 = ZZ$, this is the `ZZFeatureMap`:\n",
    "$$\\mathcal{U}_{\\Phi(\\mathbf{x})} = \\left( \\exp\\left(i\\sum_{jk} \\phi_{\\{j,k\\}}(\\mathbf{x}) \\, Z_j \\otimes Z_k\\right) \\, \\exp\\left(i\\sum_j \\phi_{\\{j\\}}(\\mathbf{x}) \\, Z_j\\right) \\, H^{\\otimes n} \\right)^d.$$\n",
    "\n",
    "The `NLocal` and `TwoLocal` functions in Qiskit's circuit library can also be used to create parameterised quantum circuits as feature maps.\n",
    "\n",
    "Implement all these feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bClwkCzZTvwk"
   },
   "outputs": [],
   "source": [
    "map_z = # code me\n",
    "map_z.draw('mpl')\n",
    "# and the rest ...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "T8een73lTvwk",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Encode the data point $x = (-0.1,0.2)$ using the `ZZFeatureMap` with 4 repetitions and default data mapping function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmtcmTKmTvwl"
   },
   "outputs": [],
   "source": [
    "#code me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GFZhVNWETvwl",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Quantum Kernel Estimation\n",
    "\n",
    "A quantum feature map, $\\phi(\\mathbf{x})$, naturally gives rise to a quantum kernel, $k(\\mathbf{x}_i,\\mathbf{x}_j)= \\phi(\\mathbf{x}_j)^\\dagger\\phi(\\mathbf{x}_i)$, which can be seen as a measure of similarity: $k(\\mathbf{x}_i,\\mathbf{x}_j)$ is large when $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are close.\n",
    "\n",
    "When considering finite data, we can represent the quantum kernel as a matrix:\n",
    "$K_{ij} = \\left| \\langle \\phi^\\dagger(\\mathbf{x}_j)| \\phi(\\mathbf{x}_i) \\rangle \\right|^{2}$. We can calculate each element of this kernel matrix on a quantum computer by calculating the transition amplitude:\n",
    "$$\n",
    "\\left| \\langle \\phi^\\dagger(\\mathbf{x}_j)| \\phi(\\mathbf{x}_i) \\rangle \\right|^{2} =\n",
    "\\left| \\langle 0^{\\otimes n} | \\mathbf{U_\\phi^\\dagger}(\\mathbf{x}_j) \\mathbf{U_\\phi}(\\mathbf{x_i}) | 0^{\\otimes n} \\rangle \\right|^{2}\n",
    "$$\n",
    "assuming the feature map is a parameterized quantum circuit, which can be described as a unitary transformation $\\mathbf{U_\\phi}(\\mathbf{x})$ on $n$ qubits.\n",
    "\n",
    "This provides us with an estimate of the quantum kernel matrix, which we can then use in a kernel machine learning algorithm, such as support vector classification.\n",
    "\n",
    "As discussed in [*Havlicek et al*.  Nature 567, 209-212 (2019)](https://www.nature.com/articles/s41586-019-0980-2), quantum kernel machine algorithms only have the potential of quantum advantage over classical approaches if the corresponding quantum kernel is hard to estimate classically.\n",
    "\n",
    "As we will see later, the hardness of estimating the kernel with classical resources is of course only a necessary and not always sufficient condition to obtain a quantum advantage.\n",
    "\n",
    "However, it was proven recently in [*Liu et al.* arXiv:2010.02174 (2020)](https://arxiv.org/abs/2010.02174) that learning problems exist for which learners with access to quantum kernel methods have a quantum advantage over all classical learners.\n",
    "\n",
    "Compute and plot the training and testing kernel matrices like above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRKIQj3zTvwl"
   },
   "outputs": [],
   "source": [
    "#code me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "GykXTouUTvwl",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Calculate the transition amplitute between $x=(−0.1,0.2)$ and $y=(0.4,-0.6)$ using the `ZZFeatureMap` with 4 repetitions and default data mapping function. Use the `qasm_simulator` with `shots = 8192`, `seed_simulator = 1024` and `seed_transpiler = 1024`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JvaW391vTvwl"
   },
   "outputs": [],
   "source": [
    "x = [-0.1,0.2]\n",
    "y = [0.4,-0.6]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "zz_map =\n",
    "zz_kernel =\n",
    "zz_circuit =\n",
    "backend =\n",
    "job =\n",
    "counts =\n",
    "\n",
    "amplitude = counts['00']/sum(counts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "id": "tIY_kVzJTvwl",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Quantum Support Vector Classification\n",
    "\n",
    "\n",
    "Introduced in [*Havlicek et al*.  Nature 567, 209-212 (2019)](https://www.nature.com/articles/s41586-019-0980-2), the quantum kernel support vector classification algorithm consists of these steps:\n",
    "\n",
    "1. Build the train and test quantum kernel matrices.\n",
    "    1. For each pair of datapoints in the training dataset $\\mathbf{x}_{i},\\mathbf{x}_j$, apply the feature map and measure the transition probability: $ K_{ij} = \\left| \\langle 0 | \\mathbf{U}^\\dagger_{\\Phi(\\mathbf{x_j})} \\mathbf{U}_{\\Phi(\\mathbf{x_i})} | 0 \\rangle \\right|^2 $.\n",
    "    2. For each training datapoint $\\mathbf{x_i}$ and testing point $\\mathbf{y_i}$, apply the feature map and measure the transition probability: $ K_{ij} = \\left| \\langle 0 | \\mathbf{U}^\\dagger_{\\Phi(\\mathbf{y_i})} \\mathbf{U}_{\\Phi(\\mathbf{x_i})} | 0 \\rangle \\right|^2 $.\n",
    "2. Use the train and test quantum kernel matrices in a classical support vector machine classification algorithm.\n",
    "\n",
    "Try running Quantum Support Vector Classification with different feature maps and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZXoh2CWXTvwl"
   },
   "outputs": [],
   "source": [
    "# code me"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "rise": {
   "height": "90%",
   "scroll": true,
   "start_slideshow_at": "beginning",
   "theme": "white",
   "transition": "zoom",
   "width": "90%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
