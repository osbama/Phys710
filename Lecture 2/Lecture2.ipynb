{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hpLlD336vdpa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hpLlD336vdpa",
    "outputId": "e3df55e3-938f-4f60-d8fc-502e9b6447b4"
   },
   "outputs": [],
   "source": [
    "!pip install qiskit\n",
    "!pip install qiskit-algorithms\n",
    "!pip install qiskit-machine-learning\n",
    "!pip install qiskit-Aer\n",
    "!pip install qiskit-qulacs\n",
    "!pip install matplotlib\n",
    "!pip install pylatexenc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e09aee605ee1b",
   "metadata": {
    "id": "792e09aee605ee1b"
   },
   "source": [
    "# The new Qiskit\n",
    "\n",
    "Notice that you'll need latest development version of the `qiskit_machine_learning` to proceed. Please follow the instructions [here](https://qiskit.org/ecosystem/machine-learning/getting_started.html#installation) to install the library from source in your Conda environment. This will also install a newer version of Qiskit where the new `primitives`, in combination with the `quantum_info` module, have superseded functionality of opflow. Thus, the latter is being deprecated.\n",
    "\n",
    "## Primitives\n",
    "\n",
    "[Primitives](https://qiskit.org/ecosystem/ibm-runtime/primitives.html) are core functions that make it easier to build modular algorithms and applications.\n",
    "\n",
    "The initial release of Qiskit Runtime includes two primitives:\n",
    "\n",
    "**Sampler**: Generates quasi-probability distribution from input circuits.\n",
    "\n",
    "**Estimator**: Calculates expectation values from input circuits and observables.\n",
    "\n",
    "## Using the Estimator primitive\n",
    "\n",
    "Similar to the `Backend` base class, there is an `Estimator` base class defined in Qiskit Terra that standardizes the way users interact with all `Estimator` implementations. This allows users to easily change their choice of simulator or device for performing expectation value calculations, even if the underlying implementation is different.\n",
    "\n",
    "In this section we will be using the default implementation in Qiskit Terra, which uses a local state vector simulator.\n",
    "\n",
    "### 1. Create a circuit\n",
    "\n",
    "For a basic expectation value calculation you will need at least one quantum circuit to prepare our system in a precise quantum state for study. Here, `Decompose` is a \"shallow\" unroller, it unrolls every instruction one level down (so that you end up with a more readable circuit).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66876f4bf2ac98cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:39:45.026762055Z",
     "start_time": "2023-11-02T05:39:44.789898668Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "66876f4bf2ac98cc",
    "outputId": "f5679a34-e6e7-4e51-99a2-b7f569cd440c"
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit.random import random_circuit\n",
    "\n",
    "circuit = random_circuit(2, 2, seed=0).decompose(reps=1)\n",
    "display(circuit.draw(\"mpl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764eb45e2b1dfec3",
   "metadata": {
    "id": "764eb45e2b1dfec3"
   },
   "source": [
    "### 2. Create an observable to measure\n",
    "\n",
    "You will also need at least one observable to measure. Observables represent physical properties of a quantum system (such as energy or spin), and allow said properties to be measured (such as their expectation values) for a given state of our system. For simplicity, you can use the SparsePauliOp class in Qiskit to define them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5755763128e689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:39:47.761906268Z",
     "start_time": "2023-11-02T05:39:47.679121066Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ad5755763128e689",
    "outputId": "61263fe0-87c0-47e9-cdeb-09ec59ed52f6"
   },
   "outputs": [],
   "source": [
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "observable = SparsePauliOp(\"XZ\")\n",
    "print(f\">>> Observable: {observable.paulis}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482727cb2313de1e",
   "metadata": {
    "id": "482727cb2313de1e"
   },
   "source": [
    "### 3. Initialize an Estimator class\n",
    "\n",
    "The next step is to create an instance of an `Estimator` class, which can be any of the subclasses that comply with the base specification. For simplicity, we will use Qiskit Terra's  `qiskit.primitives.StatevectorEstimator` class, based on the [Statevector construct](https://qiskit.org/documentation/stubs/qiskit.quantum_info.Statevector.html?highlight=statevector#qiskit.quantum_info.Statevector) (algebraic simulation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c4845d4dcea15c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:39:50.395327389Z",
     "start_time": "2023-11-02T05:39:50.350287701Z"
    },
    "id": "84c4845d4dcea15c"
   },
   "outputs": [],
   "source": [
    "from qiskit.primitives import StatevectorEstimator\n",
    "\n",
    "estimator = StatevectorEstimator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2576b1b7c30b7d4f",
   "metadata": {
    "id": "2576b1b7c30b7d4f"
   },
   "source": [
    "### 4. Invoke the Estimator and get results\n",
    "\n",
    "To calculate the expectation values, invoke the `run()` method of the `Estimator` instance you just created and pass in the circuit and observable as input parameters. This method call is asynchronous, and you will get a `Job` object back. You can use this object to query for information like `job_id()` and `status()`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607fc086f8a71191",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:39:53.461626953Z",
     "start_time": "2023-11-02T05:39:53.416267416Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "607fc086f8a71191",
    "outputId": "f8b9b6d1-ebe6-4bd0-e7f0-01b6cc5d9b83"
   },
   "outputs": [],
   "source": [
    "pub = (circuit,observable)\n",
    "job = estimator.run([pub])\n",
    "print(f\">>> Job ID: {job.job_id()}\")\n",
    "print(f\">>> Job Status: {job.status()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a4f1e2603feb6",
   "metadata": {
    "id": "cd3a4f1e2603feb6"
   },
   "source": [
    "The `result()` method of the job will return the `EstimatorResult`, which includes both the expectation values and job metadata. Error-bar information is also available, but the error is 0 for this StatevectorEstimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642e63b784f639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:39:55.949508722Z",
     "start_time": "2023-11-02T05:39:55.910624416Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6642e63b784f639",
    "outputId": "1e333517-e357-4a2e-8287-9993d4226dea"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "result = job.result()[0]\n",
    "print(f\">>> {result}\")\n",
    "print(f\">>> {result.data.evs}\")\n",
    "print(f\">>> {result.data.stds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d646cf3ef6dc59",
   "metadata": {
    "id": "d8d646cf3ef6dc59"
   },
   "source": [
    "### Do it yourself\n",
    "\n",
    "Create a random circuit to prepare a state, create an observable, invoke `run()` method to  get expectation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123c1c24555f4ff3",
   "metadata": {
    "id": "123c1c24555f4ff3"
   },
   "outputs": [],
   "source": [
    "### Code me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb836cd59625b1b",
   "metadata": {
    "id": "3fb836cd59625b1b"
   },
   "source": [
    "### Do it yourself\n",
    "\n",
    "The `RealAmplitudes` circuit is a heuristic trial wave function used as Ansatz in chemistry applications or classification circuits in machine learning. The circuit consists of alternating layers of $Y$ rotations and $CX$ entanglements. The entanglement pattern can be user-defined or selected from a predefined set (hence it is parametric). It is called `RealAmplitudes` since the prepared quantum states will only have real amplitudes, the complex part is always 0.\n",
    "\n",
    "Use this parametrized circuit and an observable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588d0de6c747aa4e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "id": "588d0de6c747aa4e",
    "outputId": "98bae871-279b-43cf-c98d-1293f62a8ef8"
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import RealAmplitudes\n",
    "\n",
    "circuit = RealAmplitudes(num_qubits=2, reps=2).decompose(reps=1)\n",
    "observable = ## code me\n",
    "parameter_values = []## code me\n",
    "\n",
    "job = ## code me\n",
    "result = job.result()\n",
    "\n",
    "display(circuit.draw(\"mpl\"))\n",
    "print(f\">>> Observable: {observable.paulis}\")\n",
    "print(f\">>> Parameter values: {parameter_values}\")\n",
    "print(f\">>> Expectation value: {result.values[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brilliant-cross",
   "metadata": {
    "id": "brilliant-cross"
   },
   "source": [
    "# Quantum Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9083a",
   "metadata": {
    "id": "efb9083a"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "### 1.1. Quantum vs. Classical Neural Networks\n",
    "\n",
    "Classical neural networks are algorithmic models inspired by the human brain that can be trained to recognize patterns in data and learn to solve complex problems. They are based on a series of interconnected nodes, or *neurons*, organized in a layered structure, with parameters that can be learned by applying machine or deep learning training strategies.\n",
    "\n",
    "The motivation behind quantum machine learning (QML) is to integrate notions from quantum computing and classical machine learning to open the way for new and improved learning schemes. QNNs apply this generic principle by combining classical neural networks and parametrized quantum circuits. Because they lie at an intersection between two fields, QNNs can be viewed from two perspectives:\n",
    "\n",
    "- From a **machine learning perspective**, QNNs are, once again, algorithmic models that can be trained to find hidden patterns in data in a similar manner to their classical counterparts. These models can **load** classical data (**inputs**) into a quantum state, and later **process** it with quantum gates parametrized by **trainable weights**. Figure 1 shows a generic QNN example including the data loading and processing steps. The output from measuring this state can then be plugged into a loss function to train the weights through backpropagation.\n",
    "\n",
    "- From a **quantum computing perspective**, QNNs are quantum algorithms based on parametrized quantum circuits that can be trained in a variational manner using classical optimizers. These circuits contain a **feature map** (with input parameters) and an **ansatz** (with trainable weights), as seen in Figure 1.\n",
    "\n",
    "![new_qnn-3.jpg](https://github.com/osbama/Phys710/blob/master/Lecture%202/new_qnn-3.jpg?raw=1)\n",
    "\n",
    "\n",
    "*Figure 1. Generic quantum neural network (QNN) structure.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47d2070",
   "metadata": {
    "id": "f47d2070"
   },
   "source": [
    "As you can see, these two perspectives are complementary, and do not necessarily rely on strict definitions of concepts such as \"quantum neuron\" or what constitutes a QNN's \"layer\".\n",
    "\n",
    "### 1.2. Implementation in `qiskit-machine-learning`\n",
    "\n",
    "The QNNs in `qiskit-machine-learning` are meant as application-agnostic computational units that can be used for different use cases, and their setup will depend on the application they are needed for. The module contains an interface for the QNNs and two specific implementations:\n",
    "\n",
    "1. [NeuralNetwork](https://qiskit.org/ecosystem/machine-learning/stubs/qiskit_machine_learning.neural_networks.NeuralNetwork.html): The interface for neural networks. This is an abstract class all QNNs inherit from.\n",
    "2. [EstimatorQNN](https://qiskit.org/ecosystem/machine-learning/stubs/qiskit_machine_learning.neural_networks.EstimatorQNN.html): A network based on the evaluation of quantum mechanical observables.\n",
    "3. [SamplerQNN](https://qiskit.org/ecosystem/machine-learning/locale/fr_FR/stubs/qiskit_machine_learning.neural_networks.SamplerQNN.html): A network based on the samples resulting from measuring a quantum circuit.\n",
    "\n",
    "\n",
    "These implementations are based on the [qiskit primitives](https://qiskit.org/documentation/apidoc/primitives.html). The primitives are the entry point to run QNNs on either a simulator or real quantum hardware. Each implementation, `EstimatorQNN` and `SamplerQNN`, takes in an optional instance of its corresponding primitive, which can be any subclass of `BaseEstimator` and `BaseSampler`, respectively.\n",
    "\n",
    "\n",
    "The `NeuralNetwork` class is the interface for all QNNs available in `qiskit-machine-learning`.\n",
    "It exposes a forward and a backward pass that take data samples and trainable weights as input.\n",
    "\n",
    "It's important to note that `NeuralNetwork`s are \"stateless\". They do not contain any training capabilities (these are pushed to the actual algorithms or applications: [classifiers](https://qiskit.org/ecosystem/machine-learning/apidocs/qiskit_machine_learning.algorithms.html#classifiers), [regressors](https://qiskit.org/ecosystem/machine-learning/apidocs/qiskit_machine_learning.algorithms.html#regressors), etc), nor do they store the values for trainable weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba316207",
   "metadata": {
    "id": "ba316207"
   },
   "source": [
    "***\n",
    "\n",
    "Let's now look into specific examples for the two `NeuralNetwork` implementations. But first, let's set the algorithmic seed to ensure that the results don't change between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-engine",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:03.772318336Z",
     "start_time": "2023-11-02T05:40:03.371296006Z"
    },
    "id": "annual-engine"
   },
   "outputs": [],
   "source": [
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "\n",
    "algorithm_globals.random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-uniform",
   "metadata": {
    "id": "billion-uniform"
   },
   "source": [
    "## 2. How to Instantiate QNNs\n",
    "\n",
    "### 2.1. `EstimatorQNN`\n",
    "\n",
    "The `EstimatorQNN` takes in a parametrized quantum circuit as input, as well as an optional quantum mechanical observable, and outputs expectation value computations for the forward pass. The `EstimatorQNN` also accepts lists of observables to construct more complex QNNs.\n",
    "\n",
    "Let's see an `EstimatorQNN` in action with a simple example. We start by constructing the parametrized circuit. This quantum circuit has two parameters, one represents a QNN input and the other represents a trainable weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "popular-artwork",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:06.501402303Z",
     "start_time": "2023-11-02T05:40:06.246115100Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "popular-artwork",
    "outputId": "ee80b117-fc71-4753-8ce0-d756cc59f90a"
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit import Parameter\n",
    "from qiskit import QuantumCircuit\n",
    "\n",
    "params1 = [Parameter(\"input1\"), Parameter(\"weight1\")]\n",
    "qc1 = QuantumCircuit(1)\n",
    "qc1.h(0)\n",
    "qc1.ry(params1[0], 0)\n",
    "qc1.rx(params1[1], 0)\n",
    "qc1.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-aquatic",
   "metadata": {
    "id": "crucial-aquatic"
   },
   "source": [
    "We can now create an observable to define the expectation value computation. If not set, then the `EstimatorQNN` will automatically create the default observable $Z^{\\otimes n}$. Here, $n$ is the number of qubits of the quantum circuit.\n",
    "\n",
    "In this example, we will change things up and use the $Y^{\\otimes n}$ observable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-magnitude",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:09.200336110Z",
     "start_time": "2023-11-02T05:40:09.145194124Z"
    },
    "id": "encouraging-magnitude"
   },
   "outputs": [],
   "source": [
    "from qiskit.quantum_info import SparsePauliOp\n",
    "\n",
    "observable1 = SparsePauliOp.from_list([(\"Y\" * qc1.num_qubits, 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1fb7ea",
   "metadata": {
    "id": "dd1fb7ea"
   },
   "source": [
    "Together with the quantum circuit defined above, and the observable we have created, the `EstimatorQNN` constructor takes in the following keyword arguments:\n",
    "\n",
    "- `estimator`: optional primitive instance\n",
    "- `input_params`: list of quantum circuit parameters that should be treated as \"network inputs\"\n",
    "- `weight_params`: list of quantum circuit parameters that should be treated as \"network weights\"\n",
    "\n",
    "In this example, we previously decided that the first parameter of `params1` should be the input, while the second should be the weight. As we are performing a local statevector simulation, we will not set the `estimator` parameter; the network will create an instance of the reference `Estimator` primitive for us. If we needed to access cloud resources or `Aer` simulators, we would have to define the respective `Estimator` instances and pass them to the `EstimatorQNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-clear",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:12.156716342Z",
     "start_time": "2023-11-02T05:40:12.119994673Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "italian-clear",
    "outputId": "eef3a1a2-27e5-46ac-ea96-74324bb6b1d1"
   },
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "\n",
    "estimator_qnn = EstimatorQNN(\n",
    "    circuit=qc1, observables=observable1, input_params=[params1[0]], weight_params=[params1[1]]\n",
    ")\n",
    "estimator_qnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ecacdb",
   "metadata": {
    "id": "16ecacdb"
   },
   "source": [
    "We'll see how to use the QNN in the following sections, but before that, let's check out the `SamplerQNN` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac3811",
   "metadata": {
    "id": "f1ac3811"
   },
   "source": [
    "### 2.2. `SamplerQNN`\n",
    "\n",
    "The `SamplerQNN` is instantiated in a similar way to the `EstimatorQNN`, but because it directly consumes samples from measuring the quantum circuit, it does not require a custom observable.\n",
    "\n",
    "These output samples are interpreted by default as the probabilities of measuring the integer index corresponding to a bitstring. However, the `SamplerQNN` also allows us to specify an `interpret` function to post-process the samples. This function should be defined so that it takes a measured integer (from a bitstring) and maps it to a new value, i.e. non-negative integer.\n",
    "\n",
    "**(!)** It's important to note that if a custom `interpret` function is defined, the `output_shape` cannot be inferred by the network, and **needs to be provided explicitly**.\n",
    "\n",
    "**(!)** It's also important to keep in mind that if no `interpret` function is used, the dimension of the probability vector will scale exponentially with the number of qubits. With a custom `interpret` function, this scaling can change. If, for instance, an index is mapped to the parity of the corresponding bitstring, i.e., to 0 or 1, the result will be a probability vector of length 2 independently of the number of qubits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cc2353",
   "metadata": {
    "id": "82cc2353"
   },
   "source": [
    "Let's create a different quantum circuit for the `SamplerQNN`. In this case, we will have two input parameters and four trainable weights that parametrize a  two-local circuit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acceptable-standing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:15.945069449Z",
     "start_time": "2023-11-02T05:40:15.714400913Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "id": "acceptable-standing",
    "outputId": "62971195-4608-4fe8-ff55-56504a703501"
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit import ParameterVector\n",
    "\n",
    "inputs2 = ParameterVector(\"input\", 2)\n",
    "weights2 = ParameterVector(\"weight\", 4)\n",
    "print(f\"input parameters: {[str(item) for item in inputs2.params]}\")\n",
    "print(f\"weight parameters: {[str(item) for item in weights2.params]}\")\n",
    "\n",
    "qc2 = QuantumCircuit(2)\n",
    "qc2.ry(inputs2[0], 0)\n",
    "qc2.ry(inputs2[1], 1)\n",
    "qc2.cx(0, 1)\n",
    "qc2.ry(weights2[0], 0)\n",
    "qc2.ry(weights2[1], 1)\n",
    "qc2.cx(0, 1)\n",
    "qc2.ry(weights2[2], 0)\n",
    "qc2.ry(weights2[3], 1)\n",
    "\n",
    "qc2.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e86defb",
   "metadata": {
    "id": "1e86defb"
   },
   "source": [
    "Similarly to the `EstimatorQNN`, we must specify inputs and weights when instantiating the `SamplerQNN`. In this case, the keyword arguments will be:\n",
    "- `sampler`: optional primitive instance\n",
    "- `input_params`: list of quantum circuit parameters that should be treated as \"network inputs\"\n",
    "- `weight_params`: list of quantum circuit parameters that should be treated as \"network weights\"\n",
    "\n",
    "Please note that, once again, we are choosing not to set the `Sampler` instance to the QNN and relying on the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c007d10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:20.176077591Z",
     "start_time": "2023-11-02T05:40:20.109235119Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5c007d10",
    "outputId": "86b9e661-d547-410a-e05c-d7a3d2ad402a"
   },
   "outputs": [],
   "source": [
    "from qiskit_machine_learning.neural_networks import SamplerQNN\n",
    "\n",
    "sampler_qnn = SamplerQNN(circuit=qc2, input_params=inputs2, weight_params=weights2)\n",
    "sampler_qnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c56e76",
   "metadata": {
    "id": "e8c56e76"
   },
   "source": [
    "In addition to the basic arguments shown above, the `SamplerQNN` accepts three more settings: `input_gradients`, `interpret`, and `output_shape`. These will be introduced in sections 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac89b99",
   "metadata": {
    "id": "0ac89b99"
   },
   "source": [
    "## 3. How to Run a Forward Pass\n",
    "\n",
    "The \"forward pass\" refers to calculation process, values of the output layers from the inputs data. It's traversing through all \"neurons\" from first to last layer. A loss function is calculated from the output values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8589abbd",
   "metadata": {
    "id": "8589abbd"
   },
   "source": [
    "### 3.1. Set-Up\n",
    "In a real setting, the inputs would be defined by the dataset, and the weights would be defined by the training algorithm or as part of a pre-trained model. However, for the sake of this tutorial, we will specify random sets of input and weights of the right dimension:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2698406f",
   "metadata": {
    "id": "2698406f"
   },
   "source": [
    "#### 3.1.1. `EstimatorQNN` Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-summary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:23.454496736Z",
     "start_time": "2023-11-02T05:40:23.411367953Z"
    },
    "id": "beneficial-summary"
   },
   "outputs": [],
   "source": [
    "estimator_qnn_input = algorithm_globals.random.random(estimator_qnn.num_inputs)\n",
    "estimator_qnn_weights = algorithm_globals.random.random(estimator_qnn.num_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5c27e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:25.840705491Z",
     "start_time": "2023-11-02T05:40:25.813470821Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4d5c27e2",
    "outputId": "5751bc69-47cd-49e6-9a28-735f3c60bb47"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Number of input features for EstimatorQNN: {estimator_qnn.num_inputs} \\nInput: {estimator_qnn_input}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of trainable weights for EstimatorQNN: {estimator_qnn.num_weights} \\nWeights: {estimator_qnn_weights}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d07341",
   "metadata": {
    "id": "81d07341"
   },
   "source": [
    "#### 3.1.2. `SamplerQNN` Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0fd6253",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:29.610324161Z",
     "start_time": "2023-11-02T05:40:29.564752821Z"
    },
    "id": "a0fd6253",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampler_qnn_input = algorithm_globals.random.random(sampler_qnn.num_inputs)\n",
    "sampler_qnn_weights = algorithm_globals.random.random(sampler_qnn.num_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008cebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:31.563204965Z",
     "start_time": "2023-11-02T05:40:31.496412342Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a008cebc",
    "outputId": "a7e1c5e8-b93d-4455-f567-2ef78d1a0e99"
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Number of input features for SamplerQNN: {sampler_qnn.num_inputs} \\nInput: {sampler_qnn_input}\"\n",
    ")\n",
    "print(\n",
    "    f\"Number of trainable weights for SamplerQNN: {sampler_qnn.num_weights} \\nWeights: {sampler_qnn_weights}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1500df",
   "metadata": {
    "id": "7f1500df"
   },
   "source": [
    "Once we have the inputs and the weights, let's see the results for batched and non-batched passes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e7302",
   "metadata": {
    "id": "de7e7302"
   },
   "source": [
    "### 3.2. Non-batched Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c5fcc5",
   "metadata": {
    "id": "52c5fcc5"
   },
   "source": [
    "#### 3.2.1. `EstimatorQNN` Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba01a3",
   "metadata": {
    "id": "7fba01a3"
   },
   "source": [
    "For the `EstimatorQNN`, the expected output shape for the forward pass is `(1, num_qubits * num_observables)` where `1` in our case is the number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bed89e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:34.757570212Z",
     "start_time": "2023-11-02T05:40:34.734254440Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54bed89e",
    "outputId": "f5a5d580-e453-4070-f664-2906532a2352"
   },
   "outputs": [],
   "source": [
    "estimator_qnn_forward = estimator_qnn.forward(estimator_qnn_input, estimator_qnn_weights)\n",
    "\n",
    "print(\n",
    "    f\"Forward pass result for EstimatorQNN: {estimator_qnn_forward}. \\nShape: {estimator_qnn_forward.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea4f85e",
   "metadata": {
    "id": "4ea4f85e"
   },
   "source": [
    "#### 3.2.2. `SamplerQNN` Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94473b35",
   "metadata": {
    "id": "94473b35"
   },
   "source": [
    "For the `SamplerQNN` (without a custom interpret function), the expected output shape for the forward pass is `(1, 2**num_qubits)`. With a custom interpret function, the output shape will be `(1, output_shape)`, where `1` in our case is the number of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb847a75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:37.553610277Z",
     "start_time": "2023-11-02T05:40:37.470207956Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb847a75",
    "outputId": "b6897582-13f7-4d4a-fb12-e9491764e744"
   },
   "outputs": [],
   "source": [
    "sampler_qnn_forward = sampler_qnn.forward(sampler_qnn_input, sampler_qnn_weights)\n",
    "\n",
    "print(\n",
    "    f\"Forward pass result for SamplerQNN: {sampler_qnn_forward}.  \\nShape: {sampler_qnn_forward.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c843c95",
   "metadata": {
    "id": "1c843c95"
   },
   "source": [
    "### 3.3. Batched Forward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c51e2fc",
   "metadata": {
    "id": "9c51e2fc"
   },
   "source": [
    "#### 3.3.1. `EstimatorQNN` Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3612ff46",
   "metadata": {
    "id": "3612ff46"
   },
   "source": [
    "For the `EstimatorQNN`, the expected output shape for the forward pass is `(batch_size, num_qubits * num_observables)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2629892e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:40.438980901Z",
     "start_time": "2023-11-02T05:40:40.396835316Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2629892e",
    "outputId": "95d95fea-7cd8-4762-b5e0-935540bba636"
   },
   "outputs": [],
   "source": [
    "estimator_qnn_forward_batched = estimator_qnn.forward(\n",
    "    [estimator_qnn_input, estimator_qnn_input], estimator_qnn_weights\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Forward pass result for EstimatorQNN: {estimator_qnn_forward_batched}.  \\nShape: {estimator_qnn_forward_batched.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7b0a7",
   "metadata": {
    "id": "acb7b0a7"
   },
   "source": [
    "#### 3.3.2. `SamplerQNN` Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7b7bb",
   "metadata": {
    "id": "48f7b7bb"
   },
   "source": [
    "For the `SamplerQNN` (without custom interpret function), the expected output shape for the forward pass is `(batch_size, 2**num_qubits)`. With a custom interpret function, the output shape will be `(batch_size, output_shape)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eb2151",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:43.045615156Z",
     "start_time": "2023-11-02T05:40:42.978139526Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "29eb2151",
    "outputId": "f13536bd-01a9-4561-c5dd-5c87d2015771"
   },
   "outputs": [],
   "source": [
    "sampler_qnn_forward_batched = sampler_qnn.forward(\n",
    "    [sampler_qnn_input, sampler_qnn_input], sampler_qnn_weights\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Forward pass result for SamplerQNN: {sampler_qnn_forward_batched}.  \\nShape: {sampler_qnn_forward_batched.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171b8ee1",
   "metadata": {
    "id": "171b8ee1"
   },
   "source": [
    "## 4. How to Run a Backward Pass\n",
    "\n",
    "\"Backward pass\" refers to process of counting changes in weights (de facto learning), using gradient descent algorithm (or similar). Computation is made from last layer, backward to the first layer.\n",
    "\n",
    "Backward and forward pass makes together one \"iteration\".\n",
    "\n",
    "Let's take advantage of the inputs and weights defined above to show how the backward pass works. This pass returns a tuple `(input_gradients, weight_gradients)`. By default, the backward pass will only calculate gradients with respect to the weight parameters.\n",
    "\n",
    "If you want to enable gradients with respect to the input parameters, you should set the following flag during the QNN instantiation:\n",
    "\n",
    "```\n",
    "qnn = ... QNN(..., input_gradients=True)\n",
    "```\n",
    "\n",
    "Please remember that input gradients are **required** for the use of `TorchConnector` for PyTorch integration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b90338",
   "metadata": {
    "id": "e5b90338"
   },
   "source": [
    "### 4.1. Backward Pass without Input Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1a6c32",
   "metadata": {
    "id": "fe1a6c32"
   },
   "source": [
    "#### 4.1.1. `EstimatorQNN` Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387c9700",
   "metadata": {
    "id": "387c9700"
   },
   "source": [
    "For the `EstimatorQNN`, the expected output shape for the weight gradients is `(batch_size, num_qubits * num_observables, num_weights)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-reaction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:46.206457426Z",
     "start_time": "2023-11-02T05:40:46.181397282Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "entitled-reaction",
    "outputId": "f9d04bdd-b411-4ffa-9567-88abbf5504f3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "estimator_qnn_input_grad, estimator_qnn_weight_grad = estimator_qnn.backward(\n",
    "    estimator_qnn_input, estimator_qnn_weights\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Input gradients for EstimatorQNN: {estimator_qnn_input_grad}.  \\nShape: {estimator_qnn_input_grad}\"\n",
    ")\n",
    "print(\n",
    "    f\"Weight gradients for EstimatorQNN: {estimator_qnn_weight_grad}.  \\nShape: {estimator_qnn_weight_grad.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5feb04",
   "metadata": {
    "id": "6d5feb04"
   },
   "source": [
    "#### 4.1.2. `SamplerQNN` Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebaa404",
   "metadata": {
    "id": "bebaa404"
   },
   "source": [
    "For the `SamplerQNN` (without custom interpret function), the expected output shape for the forward pass is `(batch_size, 2**num_qubits, num_weights)`. With a custom interpret function, the output shape will be `(batch_size, output_shape, num_weights)`.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefacefe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:49.146666162Z",
     "start_time": "2023-11-02T05:40:49.035503657Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eefacefe",
    "outputId": "bd359465-8c4e-4b20-ae23-fae07d9eca47"
   },
   "outputs": [],
   "source": [
    "sampler_qnn_input_grad, sampler_qnn_weight_grad = sampler_qnn.backward(\n",
    "    sampler_qnn_input, sampler_qnn_weights\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Input gradients for SamplerQNN: {sampler_qnn_input_grad}.  \\nShape: {sampler_qnn_input_grad}\"\n",
    ")\n",
    "print(\n",
    "    f\"Weight gradients for SamplerQNN: {sampler_qnn_weight_grad}.  \\nShape: {sampler_qnn_weight_grad.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d28a00",
   "metadata": {
    "id": "74d28a00"
   },
   "source": [
    "### 4.2. Backward Pass with Input Gradients\n",
    "\n",
    "Let's enable the `input_gradients` to show what the expected output sizes are for this option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccc4641",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:54.256587181Z",
     "start_time": "2023-11-02T05:40:54.227369298Z"
    },
    "id": "9ccc4641"
   },
   "outputs": [],
   "source": [
    "estimator_qnn.input_gradients = True\n",
    "sampler_qnn.input_gradients = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d2cd57",
   "metadata": {
    "id": "f5d2cd57"
   },
   "source": [
    "#### 4.2.1. `EstimatorQNN` Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ebc80",
   "metadata": {
    "id": "9d1ebc80"
   },
   "source": [
    "For the `EstimatorQNN`, the expected output shape for the input gradients is `(batch_size, num_qubits * num_observables, num_inputs)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4332f42b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:40:59.406950147Z",
     "start_time": "2023-11-02T05:40:59.354664953Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4332f42b",
    "outputId": "14ea8b46-2844-4c1b-e335-0f3b92918e8d"
   },
   "outputs": [],
   "source": [
    "estimator_qnn_input_grad, estimator_qnn_weight_grad = estimator_qnn.backward(\n",
    "    estimator_qnn_input, estimator_qnn_weights\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Input gradients for EstimatorQNN: {estimator_qnn_input_grad}.  \\nShape: {estimator_qnn_input_grad.shape}\"\n",
    ")\n",
    "print(\n",
    "    f\"Weight gradients for EstimatorQNN: {estimator_qnn_weight_grad}.  \\nShape: {estimator_qnn_weight_grad.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e50ff8",
   "metadata": {
    "id": "d3e50ff8"
   },
   "source": [
    "#### 4.2.2. `SamplerQNN` Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76da18a",
   "metadata": {
    "id": "b76da18a"
   },
   "source": [
    "For the `SamplerQNN` (without custom interpret function), the expected output shape for the input gradients is `(batch_size, 2**num_qubits, num_inputs)`. With a custom interpret function, the output shape will be `(batch_size, output_shape, num_inputs)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3339f869",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:02.550005706Z",
     "start_time": "2023-11-02T05:41:02.484581837Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3339f869",
    "outputId": "2a74687e-2e80-4d4e-a924-5978855ee36c"
   },
   "outputs": [],
   "source": [
    "sampler_qnn_input_grad, sampler_qnn_weight_grad = sampler_qnn.backward(\n",
    "    sampler_qnn_input, sampler_qnn_weights\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"Input gradients for SamplerQNN: {sampler_qnn_input_grad}.  \\nShape: {sampler_qnn_input_grad.shape}\"\n",
    ")\n",
    "print(\n",
    "    f\"Weight gradients for SamplerQNN: {sampler_qnn_weight_grad}.  \\nShape: {sampler_qnn_weight_grad.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45871b6d",
   "metadata": {
    "id": "45871b6d"
   },
   "source": [
    "## 5. Advanced Functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fb829",
   "metadata": {
    "id": "4e1fb829"
   },
   "source": [
    "### 5.1. `EstimatorQNN` with Multiple Observables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c86fd7",
   "metadata": {
    "id": "18c86fd7"
   },
   "source": [
    "The `EstimatorQNN` allows to pass lists of observables for more complex QNN architectures. For example (note the change in output shape):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e1e2f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:07.090784561Z",
     "start_time": "2023-11-02T05:41:07.057333351Z"
    },
    "id": "34e1e2f0"
   },
   "outputs": [],
   "source": [
    "observable2 = SparsePauliOp.from_list([(\"Z\" * qc1.num_qubits, 1)])\n",
    "\n",
    "estimator_qnn2 = EstimatorQNN(\n",
    "    circuit=qc1,\n",
    "    observables=[observable1, observable2],\n",
    "    input_params=[params1[0]],\n",
    "    weight_params=[params1[1]],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e801632d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:09.808088396Z",
     "start_time": "2023-11-02T05:41:09.684021505Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e801632d",
    "outputId": "01a8d01d-34b6-46e7-f74e-962d33f848c1"
   },
   "outputs": [],
   "source": [
    "estimator_qnn_forward2 = estimator_qnn2.forward(estimator_qnn_input, estimator_qnn_weights)\n",
    "estimator_qnn_input_grad2, estimator_qnn_weight_grad2 = estimator_qnn2.backward(\n",
    "    estimator_qnn_input, estimator_qnn_weights\n",
    ")\n",
    "\n",
    "print(f\"Forward output for EstimatorQNN1: {estimator_qnn_forward.shape}\")\n",
    "print(f\"Forward output for EstimatorQNN2: {estimator_qnn_forward2.shape}\")\n",
    "print(f\"Backward output for EstimatorQNN1: {estimator_qnn_weight_grad.shape}\")\n",
    "print(f\"Backward output for EstimatorQNN2: {estimator_qnn_weight_grad2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788ec9f1",
   "metadata": {
    "id": "788ec9f1"
   },
   "source": [
    "### 5.2. `SamplerQNN` with custom `interpret`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378ef3ba",
   "metadata": {
    "id": "378ef3ba"
   },
   "source": [
    "One common `interpret` method for `SamplerQNN` is the `parity` function, which allows it to perform binary classification. As explained in the instantiation section, using interpret functions will modify the output shape of the forward and backward passes. In the case of the parity interpret function, `output_shape` is fixed to `2`. Therefore, the expected forward and weight gradient shapes are `(batch_size, 2)` and `(batch_size, 2, num_weights)`, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed68d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:13.085532292Z",
     "start_time": "2023-11-02T05:41:12.973139335Z"
    },
    "id": "eed68d1a"
   },
   "outputs": [],
   "source": [
    "parity = lambda x: \"{:b}\".format(x).count(\"1\") % 2\n",
    "output_shape = 2  # parity = 0, 1\n",
    "\n",
    "sampler_qnn2 = SamplerQNN(\n",
    "    circuit=qc2,\n",
    "    input_params=inputs2,\n",
    "    weight_params=weights2,\n",
    "    interpret=parity,\n",
    "    output_shape=output_shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2888195",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:15.671853306Z",
     "start_time": "2023-11-02T05:41:15.542838435Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2888195",
    "outputId": "431c03f0-75a9-4baa-e74c-7abc118b5f8d"
   },
   "outputs": [],
   "source": [
    "sampler_qnn_forward2 = sampler_qnn2.forward(sampler_qnn_input, sampler_qnn_weights)\n",
    "sampler_qnn_input_grad2, sampler_qnn_weight_grad2 = sampler_qnn2.backward(\n",
    "    sampler_qnn_input, sampler_qnn_weights\n",
    ")\n",
    "\n",
    "print(f\"Forward output for SamplerQNN1: {sampler_qnn_forward.shape}\")\n",
    "print(f\"Forward output for SamplerQNN2: {sampler_qnn_forward2.shape}\")\n",
    "print(f\"Backward output for SamplerQNN1: {sampler_qnn_weight_grad.shape}\")\n",
    "print(f\"Backward output for SamplerQNN2: {sampler_qnn_weight_grad2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-ecology",
   "metadata": {
    "id": "intense-ecology"
   },
   "source": [
    "# Neural Network Classifier & Regressor\n",
    "\n",
    "This section shows how `NeuralNetworkClassifier` and `NeuralNetworkRegressor` are used.\n",
    "Both take as an input a (Quantum) `NeuralNetwork` and leverage it in a specific context.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-sword",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:18.453026315Z",
     "start_time": "2023-11-02T05:41:18.221854016Z"
    },
    "id": "functioning-sword"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.circuit.library import RealAmplitudes, ZZFeatureMap\n",
    "from qiskit_algorithms.optimizers import COBYLA, L_BFGS_B\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "\n",
    "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier, VQC\n",
    "from qiskit_machine_learning.algorithms.regressors import NeuralNetworkRegressor, VQR\n",
    "from qiskit_machine_learning.neural_networks import SamplerQNN, EstimatorQNN\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit\n",
    "\n",
    "algorithm_globals.random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-divide",
   "metadata": {
    "id": "compact-divide"
   },
   "source": [
    "## Classification\n",
    "\n",
    "We prepare a simple classification dataset to illustrate the following algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-pierre",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:21.512056982Z",
     "start_time": "2023-11-02T05:41:21.279771856Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450
    },
    "id": "short-pierre",
    "outputId": "083d71ba-bdd4-4c73-eef1-ba9ca159b40f",
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "num_inputs = 2\n",
    "num_samples = 20\n",
    "X = 2 * algorithm_globals.random.random([num_samples, num_inputs]) - 1\n",
    "y01 = 1 * (np.sum(X, axis=1) >= 0)  # in { 0,  1}\n",
    "y = 2 * y01 - 1  # in {-1, +1}\n",
    "y_one_hot = np.zeros((num_samples, 2))\n",
    "for i in range(num_samples):\n",
    "    y_one_hot[i, y01[i]] = 1\n",
    "\n",
    "for x, y_target in zip(X, y):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[1], \"bo\")\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], \"go\")\n",
    "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "religious-history",
   "metadata": {
    "id": "religious-history"
   },
   "source": [
    "### Classification with an `EstimatorQNN`\n",
    "\n",
    "First we show how an `EstimatorQNN` can be used for classification within a `NeuralNetworkClassifier`. In this context, the `EstimatorQNN` is expected to return one-dimensional output in $[-1, +1]$. This only works for binary classification and we assign the two classes to $\\{-1, +1\\}$. To simplify the composition of parameterized quantum circuit from a feature map and an ansatz we can use the `QNNCircuit` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed90df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:25.674084098Z",
     "start_time": "2023-11-02T05:41:25.514238006Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "ceed90df",
    "outputId": "4b693e8c-066f-4b8d-8648-48ae73c07424"
   },
   "outputs": [],
   "source": [
    "# construct QNN with the QNNCircuit's default ZZFeatureMap feature map and RealAmplitudes ansatz.\n",
    "qc = QNNCircuit(num_qubits=2)\n",
    "qc.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-animal",
   "metadata": {
    "id": "formed-animal"
   },
   "source": [
    "Create a quantum neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "determined-hands",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:29.047932846Z",
     "start_time": "2023-11-02T05:41:28.954335106Z"
    },
    "id": "determined-hands"
   },
   "outputs": [],
   "source": [
    "estimator_qnn = EstimatorQNN(circuit=qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-casting",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:31.984138402Z",
     "start_time": "2023-11-02T05:41:31.841039130Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "acute-casting",
    "outputId": "47478726-7976-470a-8c24-007156da0494"
   },
   "outputs": [],
   "source": [
    "# QNN maps inputs to [-1, +1]\n",
    "estimator_qnn.forward(X[0, :], algorithm_globals.random.random(estimator_qnn.num_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-holiday",
   "metadata": {
    "id": "stone-holiday"
   },
   "source": [
    "We will add a callback function called `callback_graph`. This will be called for each iteration of the optimizer and will be passed two parameters: the current weights and the value of the objective function at those weights. For our function, we append the value of the objective function to an array so we can plot iteration versus objective function value and update the graph with each iteration. However, you can do whatever you want with a callback function as long as it gets the two parameters mentioned passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "similar-controversy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:34.514778249Z",
     "start_time": "2023-11-02T05:41:34.473358861Z"
    },
    "id": "similar-controversy"
   },
   "outputs": [],
   "source": [
    "# callback function that draws a live plot when the .fit() method is called\n",
    "def callback_graph(weights, obj_func_eval):\n",
    "    clear_output(wait=True)\n",
    "    objective_func_vals.append(obj_func_eval)\n",
    "    plt.title(\"Objective function value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-receiver",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:41:37.049587222Z",
     "start_time": "2023-11-02T05:41:37.008492054Z"
    },
    "id": "lesser-receiver"
   },
   "outputs": [],
   "source": [
    "# construct neural network classifier\n",
    "estimator_classifier = NeuralNetworkClassifier(\n",
    "    estimator_qnn, optimizer=COBYLA(maxiter=60), callback=callback_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-editor",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "adopted-editor",
    "outputId": "78e64770-9d6f-4acb-fea8-94f4662fcf93"
   },
   "outputs": [],
   "source": [
    "# create empty array for callback to store evaluations of the objective function\n",
    "objective_func_vals = []\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# fit classifier to data\n",
    "estimator_classifier.fit(X, y)\n",
    "\n",
    "# return to default figsize\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "# score classifier\n",
    "estimator_classifier.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "civilian-analysis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:42:03.709233394Z",
     "start_time": "2023-11-02T05:42:03.260886178Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "civilian-analysis",
    "outputId": "72336708-3dc8-42b1-e9e7-f664ec6570f7"
   },
   "outputs": [],
   "source": [
    "# evaluate data points\n",
    "y_predict = estimator_classifier.predict(X)\n",
    "\n",
    "# plot results\n",
    "# red == wrongly classified\n",
    "for x, y_target, y_p in zip(X, y, y_predict):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[1], \"bo\")\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], \"go\")\n",
    "    if y_target != y_p:\n",
    "        plt.scatter(x[0], x[1], s=200, facecolors=\"none\", edgecolors=\"r\", linewidths=2)\n",
    "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-seattle",
   "metadata": {
    "id": "japanese-seattle"
   },
   "source": [
    "Now, when the model is trained, we can explore the weights of the neural network. Please note, the number of weights is defined by ansatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-basket",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:42:07.314201107Z",
     "start_time": "2023-11-02T05:42:07.285108742Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "offshore-basket",
    "outputId": "b1ae833a-377a-4f5a-bdd6-aed16ef2adac"
   },
   "outputs": [],
   "source": [
    "estimator_classifier.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220213bcc0819f5a",
   "metadata": {
    "id": "220213bcc0819f5a"
   },
   "source": [
    "### Do it yourself\n",
    "\n",
    "There are a number of ways you can play with the above \"model\" QNN. First start with the `optimizer`.  This interface is based on [SciPy’s optimize module](https://qiskit.org/documentation/stubs/qiskit.algorithms.optimizers.Minimizer.html#qiskit.algorithms.optimizers.Minimizer). Try different optimizers, do you see any difference?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63e043122bd2915",
   "metadata": {
    "id": "f63e043122bd2915"
   },
   "outputs": [],
   "source": [
    "# Code me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "determined-standing",
   "metadata": {
    "id": "determined-standing"
   },
   "source": [
    "### Classification with a `SamplerQNN`\n",
    "\n",
    "Next we show how a `SamplerQNN` can be used for classification within a `NeuralNetworkClassifier`. In this context, the `SamplerQNN` is expected to return $d$-dimensional probability vector as output, where $d$ denotes the number of classes.\n",
    "The underlying `Sampler` primitive returns quasi-distributions of bit strings and we just need to define a mapping from the measured bitstrings to the different classes. For binary classification we use the parity mapping. Again we can use the `QNNCircuit` class to set up a parameterized quantum circuit from a feature map and ansatz of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ff56f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:42:12.137478349Z",
     "start_time": "2023-11-02T05:42:12.039391685Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "d1ff56f4",
    "outputId": "fd62589f-f1b9-4adf-b178-39dc68242e5a"
   },
   "outputs": [],
   "source": [
    "# construct a quantum circuit from the default ZZFeatureMap feature map and a customized RealAmplitudes ansatz\n",
    "qc = QNNCircuit(ansatz=RealAmplitudes(num_inputs, reps=1))\n",
    "qc.draw(output=\"mpl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-sensitivity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:42:17.031354234Z",
     "start_time": "2023-11-02T05:42:16.989271346Z"
    },
    "id": "young-sensitivity"
   },
   "outputs": [],
   "source": [
    "# parity maps bitstrings to 0 or 1\n",
    "def parity(x):\n",
    "    return \"{:b}\".format(x).count(\"1\") % 2\n",
    "\n",
    "\n",
    "output_shape = 2  # corresponds to the number of classes, possible outcomes of the (parity) mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "statutory-mercury",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:42:20.062308854Z",
     "start_time": "2023-11-02T05:42:20.028213742Z"
    },
    "id": "statutory-mercury"
   },
   "outputs": [],
   "source": [
    "# construct QNN\n",
    "sampler_qnn = SamplerQNN(\n",
    "    circuit=qc,\n",
    "    interpret=parity,\n",
    "    output_shape=output_shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-orlando",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:42:23.019658934Z",
     "start_time": "2023-11-02T05:42:22.975655824Z"
    },
    "id": "hybrid-orlando"
   },
   "outputs": [],
   "source": [
    "# construct classifier\n",
    "sampler_classifier = NeuralNetworkClassifier(\n",
    "    neural_network=sampler_qnn, optimizer=COBYLA(maxiter=30), callback=callback_graph\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-newman",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "adult-newman",
    "outputId": "2ed8ab6c-c08f-4204-b175-8a53f46b123e"
   },
   "outputs": [],
   "source": [
    "# create empty array for callback to store evaluations of the objective function\n",
    "objective_func_vals = []\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# fit classifier to data\n",
    "sampler_classifier.fit(X, y01)\n",
    "\n",
    "# return to default figsize\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "# score classifier\n",
    "sampler_classifier.score(X, y01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "angry-bulgarian",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:42:37.992105463Z",
     "start_time": "2023-11-02T05:42:37.727629457Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "angry-bulgarian",
    "outputId": "f46c3dde-5074-437d-d045-45a9d2a477d9"
   },
   "outputs": [],
   "source": [
    "# evaluate data points\n",
    "y_predict = sampler_classifier.predict(X)\n",
    "\n",
    "# plot results\n",
    "# red == wrongly classified\n",
    "for x, y_target, y_p in zip(X, y01, y_predict):\n",
    "    if y_target == 1:\n",
    "        plt.plot(x[0], x[1], \"bo\")\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], \"go\")\n",
    "    if y_target != y_p:\n",
    "        plt.scatter(x[0], x[1], s=200, facecolors=\"none\", edgecolors=\"r\", linewidths=2)\n",
    "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-individual",
   "metadata": {
    "id": "assisted-individual"
   },
   "source": [
    "Again, once the model is trained we can take a look at the weights. As we set `reps=1` explicitly in our ansatz, we can see less parameters than in the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-bulletin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:42:41.697096807Z",
     "start_time": "2023-11-02T05:42:41.643396432Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "indonesian-bulletin",
    "outputId": "9c1820ab-5634-4d54-f1a8-dfec758bf4a5"
   },
   "outputs": [],
   "source": [
    "sampler_classifier.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "champion-approval",
   "metadata": {
    "id": "champion-approval"
   },
   "source": [
    "### Variational Quantum Classifier (`VQC`)\n",
    "\n",
    "The `VQC` is a special variant of the `NeuralNetworkClassifier` with a `SamplerQNN`. It applies a parity mapping (or extensions to multiple classes) to map from the bitstring to the classification, which results in a probability vector, which is interpreted as a one-hot encoded result. By default, it applies this the `CrossEntropyLoss` function that expects labels given in one-hot encoded format and will return predictions in that format too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-dublin",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:42:44.468521207Z",
     "start_time": "2023-11-02T05:42:44.429738308Z"
    },
    "id": "legislative-dublin"
   },
   "outputs": [],
   "source": [
    "# construct feature map, ansatz, and optimizer\n",
    "feature_map = ZZFeatureMap(num_inputs)\n",
    "ansatz = RealAmplitudes(num_inputs, reps=1)\n",
    "\n",
    "# construct variational quantum classifier\n",
    "vqc = VQC(\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz,\n",
    "    loss=\"cross_entropy\",\n",
    "    optimizer=COBYLA(maxiter=30),\n",
    "    callback=callback_graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geographic-adjustment",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "geographic-adjustment",
    "outputId": "8e6bb0a7-070d-473e-9c8d-8b253c785699"
   },
   "outputs": [],
   "source": [
    "# create empty array for callback to store evaluations of the objective function\n",
    "objective_func_vals = []\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# fit classifier to data\n",
    "vqc.fit(X, y_one_hot)\n",
    "\n",
    "# return to default figsize\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "# score classifier\n",
    "vqc.score(X, y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-heavy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:00.574103020Z",
     "start_time": "2023-11-02T05:43:00.384106427Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "stopped-heavy",
    "outputId": "eb5c6313-c181-44d7-c9a6-3aa1b9e0c2b8"
   },
   "outputs": [],
   "source": [
    "# evaluate data points\n",
    "y_predict = vqc.predict(X)\n",
    "\n",
    "# plot results\n",
    "# red == wrongly classified\n",
    "for x, y_target, y_p in zip(X, y_one_hot, y_predict):\n",
    "    if y_target[0] == 1:\n",
    "        plt.plot(x[0], x[1], \"bo\")\n",
    "    else:\n",
    "        plt.plot(x[0], x[1], \"go\")\n",
    "    if not np.all(y_target == y_p):\n",
    "        plt.scatter(x[0], x[1], s=200, facecolors=\"none\", edgecolors=\"r\", linewidths=2)\n",
    "plt.plot([-1, 1], [1, -1], \"--\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-testament",
   "metadata": {
    "id": "grave-testament"
   },
   "source": [
    "### Multiple classes with VQC\n",
    "In this section we generate an artificial dataset that contains samples of three classes and show how to train a model to classify this dataset. This example shows how to tackle more interesting problems in machine learning. Of course, for a sake of short training time we prepare a tiny dataset. We employ `make_classification` from SciKit-Learn to generate a dataset. There 10 samples in the dataset, 2 features, that means we can still have a nice plot of the dataset, as well as no redundant features, these are features are generated as a combinations of the other features. Also, we have 3 different classes in the dataset, each classes one kind of centroid and we set class separation to `2.0`, a slight increase from the default value of `1.0` to ease the classification problem.\n",
    "\n",
    "Once the dataset is generated we scale the features into the range `[0, 1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plastic-dividend",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:03.963577576Z",
     "start_time": "2023-11-02T05:43:03.875307803Z"
    },
    "id": "plastic-dividend"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=10,\n",
    "    n_features=2,\n",
    "    n_classes=3,\n",
    "    n_redundant=0,\n",
    "    n_clusters_per_class=1,\n",
    "    class_sep=2.0,\n",
    "    random_state=algorithm_globals.random_seed,\n",
    ")\n",
    "X = MinMaxScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-disclosure",
   "metadata": {
    "id": "forced-disclosure"
   },
   "source": [
    "Let's see how our dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premier-drill",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:08.021218749Z",
     "start_time": "2023-11-02T05:43:07.837067057Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "id": "premier-drill",
    "outputId": "eb345749-3903-46c7-c3d2-7efdf9628d01"
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deadly-response",
   "metadata": {
    "id": "deadly-response"
   },
   "source": [
    "We also transform labels and make them categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-bailey",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:11.362243879Z",
     "start_time": "2023-11-02T05:43:11.335334369Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exposed-bailey",
    "outputId": "747a7960-50d6-41f4-be84-f9d64811718b"
   },
   "outputs": [],
   "source": [
    "y_cat = np.empty(y.shape, dtype=str)\n",
    "y_cat[y == 0] = \"A\"\n",
    "y_cat[y == 1] = \"B\"\n",
    "y_cat[y == 2] = \"C\"\n",
    "print(y_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-headquarters",
   "metadata": {
    "id": "instructional-headquarters"
   },
   "source": [
    "We create an instance of `VQC` similar to the previous example, but in this case we pass a minimal set of parameters. Instead of feature map and ansatz we pass just the number of qubits that is equal to the number of features in the dataset, an optimizer with a low number of iteration to reduce training time, a quantum instance, and a callback to observe progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-result",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:13.631704793Z",
     "start_time": "2023-11-02T05:43:13.571566625Z"
    },
    "id": "latin-result"
   },
   "outputs": [],
   "source": [
    "vqc = VQC(\n",
    "    num_qubits=2,\n",
    "    optimizer=COBYLA(maxiter=30),\n",
    "    callback=callback_graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-bookmark",
   "metadata": {
    "id": "proper-bookmark"
   },
   "source": [
    "Start the training process in the same way as in previous examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reported-pioneer",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "reported-pioneer",
    "outputId": "a8e75f6e-58da-4691-b4ca-9e1d197162d5"
   },
   "outputs": [],
   "source": [
    "# create empty array for callback to store evaluations of the objective function\n",
    "objective_func_vals = []\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# fit classifier to data\n",
    "vqc.fit(X, y_cat)\n",
    "\n",
    "# return to default figsize\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "# score classifier\n",
    "vqc.score(X, y_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-renaissance",
   "metadata": {
    "id": "weighted-renaissance"
   },
   "source": [
    "Despite we had the low number of iterations, we achieved quite a good score. Let see the output of the `predict` method and compare the output with the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-patient",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:25.062484722Z",
     "start_time": "2023-11-02T05:43:25.005207786Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "employed-patient",
    "outputId": "05aaa3f3-788f-49c6-960d-e344b546b4df"
   },
   "outputs": [],
   "source": [
    "predict = vqc.predict(X)\n",
    "print(f\"Predicted labels: {predict}\")\n",
    "print(f\"Ground truth:     {y_cat}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-secret",
   "metadata": {
    "id": "guided-secret"
   },
   "source": [
    "## Regression\n",
    "\n",
    "We prepare a simple regression dataset to illustrate the following algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iraqi-flavor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:34.842676915Z",
     "start_time": "2023-11-02T05:43:34.731581809Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "iraqi-flavor",
    "outputId": "f05b2f01-1173-47f1-e029-cc1e6d536957"
   },
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "eps = 0.2\n",
    "lb, ub = -np.pi, np.pi\n",
    "X_ = np.linspace(lb, ub, num=50).reshape(50, 1)\n",
    "f = lambda x: np.sin(x)\n",
    "\n",
    "X = (ub - lb) * algorithm_globals.random.random([num_samples, 1]) + lb\n",
    "y = f(X[:, 0]) + eps * (2 * algorithm_globals.random.random(num_samples) - 1)\n",
    "\n",
    "plt.plot(X_, f(X_), \"r--\")\n",
    "plt.plot(X, y, \"bo\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-capitol",
   "metadata": {
    "id": "talented-capitol"
   },
   "source": [
    "### Regression with an `EstimatorQNN`\n",
    "\n",
    "Here we restrict to regression with an `EstimatorQNN` that returns values in $[-1, +1]$. More complex and also multi-dimensional models could be constructed, also based on `SamplerQNN` but that exceeds the scope of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-kelly",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:37.853630466Z",
     "start_time": "2023-11-02T05:43:37.768471840Z"
    },
    "id": "perfect-kelly"
   },
   "outputs": [],
   "source": [
    "# construct simple feature map\n",
    "param_x = Parameter(\"x\")\n",
    "feature_map = QuantumCircuit(1, name=\"fm\")\n",
    "feature_map.ry(param_x, 0)\n",
    "\n",
    "# construct simple ansatz\n",
    "param_y = Parameter(\"y\")\n",
    "ansatz = QuantumCircuit(1, name=\"vf\")\n",
    "ansatz.ry(param_y, 0)\n",
    "\n",
    "# construct a circuit\n",
    "qc = QNNCircuit(feature_map=feature_map, ansatz=ansatz)\n",
    "\n",
    "# construct QNN\n",
    "regression_estimator_qnn = EstimatorQNN(circuit=qc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-marks",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:41.387623971Z",
     "start_time": "2023-11-02T05:43:41.357381163Z"
    },
    "id": "velvet-marks"
   },
   "outputs": [],
   "source": [
    "# construct the regressor from the neural network\n",
    "regressor = NeuralNetworkRegressor(\n",
    "    neural_network=regression_estimator_qnn,\n",
    "    loss=\"squared_error\",\n",
    "    optimizer=L_BFGS_B(maxiter=5),\n",
    "    callback=callback_graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-mongolia",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "working-mongolia",
    "outputId": "1a4060dc-0143-4771-c9b4-7b283119aa3e"
   },
   "outputs": [],
   "source": [
    "# create empty array for callback to store evaluations of the objective function\n",
    "objective_func_vals = []\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# fit to data\n",
    "regressor.fit(X, y)\n",
    "\n",
    "# return to default figsize\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "# score the result\n",
    "regressor.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-conservative",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:47.959875535Z",
     "start_time": "2023-11-02T05:43:47.809139476Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "diverse-conservative",
    "outputId": "14745c95-fb18-43db-d774-a0b45e68ee5c"
   },
   "outputs": [],
   "source": [
    "# plot target function\n",
    "plt.plot(X_, f(X_), \"r--\")\n",
    "\n",
    "# plot data\n",
    "plt.plot(X, y, \"bo\")\n",
    "\n",
    "# plot fitted line\n",
    "y_ = regressor.predict(X_)\n",
    "plt.plot(X_, y_, \"g-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "false-india",
   "metadata": {
    "id": "false-india"
   },
   "source": [
    "Similarly to the classification models, we can obtain an array of trained weights by querying a corresponding property of the model. In this model we have only one parameter defined as `param_y` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-turner",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:51.110157655Z",
     "start_time": "2023-11-02T05:43:51.068042418Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "terminal-turner",
    "outputId": "735aec2e-1418-4321-d1f9-2174f4c79e62"
   },
   "outputs": [],
   "source": [
    "regressor.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-legislation",
   "metadata": {
    "id": "offensive-legislation"
   },
   "source": [
    "### Regression with the Variational Quantum Regressor (`VQR`)\n",
    "\n",
    "Similar to the `VQC` for classification, the `VQR` is a special variant of the `NeuralNetworkRegressor` with a `EstimatorQNN`. By default it considers the `L2Loss` function to minimize the mean squared error between predictions and targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-entry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:43:54.046637804Z",
     "start_time": "2023-11-02T05:43:53.985055290Z"
    },
    "id": "offensive-entry"
   },
   "outputs": [],
   "source": [
    "vqr = VQR(\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz,\n",
    "    optimizer=L_BFGS_B(maxiter=5),\n",
    "    callback=callback_graph,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cooperative-helmet",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 526
    },
    "id": "cooperative-helmet",
    "outputId": "d10bd963-9f51-4da7-fcc4-fd463ecc23b7"
   },
   "outputs": [],
   "source": [
    "# create empty array for callback to store evaluations of the objective function\n",
    "objective_func_vals = []\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# fit regressor\n",
    "vqr.fit(X, y)\n",
    "\n",
    "# return to default figsize\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "\n",
    "# score result\n",
    "vqr.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-cambridge",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:44:05.089188298Z",
     "start_time": "2023-11-02T05:44:04.938416179Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "genetic-cambridge",
    "outputId": "3e8da9f1-7399-4631-897a-a5e383f85df1"
   },
   "outputs": [],
   "source": [
    "# plot target function\n",
    "plt.plot(X_, f(X_), \"r--\")\n",
    "\n",
    "# plot data\n",
    "plt.plot(X, y, \"bo\")\n",
    "\n",
    "# plot fitted line\n",
    "y_ = vqr.predict(X_)\n",
    "plt.plot(X_, y_, \"g-\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc483278092755",
   "metadata": {
    "id": "43bc483278092755"
   },
   "source": [
    "# Barren Plateus (do it yourself)\n",
    "We've seen that training with gradients works well on the small example model. But can we expect the same if we increase the number of qubits? To investigate that, we measure the variance (A measure of how much a variable deviates from the mean.) of the gradients for different model sizes. The idea is simple: if the variance is really small, we don't have enough information to update our parameters. You'll see that as the number of QNN layers increase, you'll have exponentially vanishing gradients (barren plateaus). This means our gradients contain less and less information and we'll have a hard time to train the model. This is where the loss landscape becomes increasingly flat (and thus hard to determine the direction to the minimum). This issue is discussed in detail in these two references: Jarrod R. McClean, Sergio Boixo, Vadim N. Smelyanskiy, Ryan Babbush and Hartmut Neven, Barren plateaus in quantum neural network training landscapes, Nature Communications, Volume 9, 4812 (2018), doi:10.1038/s41467-018-07090-4, arXiv:1803.11173 and M. Cerezo, Akira Sone, Tyler Volkoff, Lukasz Cincio and Patrick J. Coles, Cost Function Dependent Barren Plateaus in Shallow Parametrized Quantum Circuits, Nature Communications 12, 1791 (2021), doi:10.1038/s41467-021-21728-w, arXiv:2001.00550.\n",
    "\n",
    "Create a neural network for either a regression or classification problem that varies between 2 to 12 layers, and plot variance to observe the barren plateau problem.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4bc420d011e31f",
   "metadata": {
    "id": "1a4bc420d011e31f"
   },
   "outputs": [],
   "source": [
    "#Code me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8183d2073e8b614",
   "metadata": {
    "id": "e8183d2073e8b614"
   },
   "source": [
    "\n",
    "To help us visualize the problem and get a sense of what is happening in the cost landscape let’s look at two circuits with 50 layers each and containing 4 and 12 qubits. We will use the circuit shown below consisting of only 2 parameters $\\theta$ and $\\phi$. We use the observable ZZ on the first two qubits and visualize the landscape for different values of $\\theta$ and $\\phi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc83c89f279c746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:44:10.215812086Z",
     "start_time": "2023-11-02T05:44:09.955111316Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "id": "3dc83c89f279c746",
    "outputId": "588f0849-b57c-495d-bbe9-d2cf3a5b09f1"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from qiskit.circuit import Parameter\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit_qulacs.qulacs_estimator import QulacsEstimator\n",
    "import os\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n",
    "estimator = QulacsEstimator()\n",
    "\n",
    "def generate_random_pqc(n_qubits, n_layers):\n",
    "\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "\n",
    "    theta, phi = Parameter('θ'), Parameter('φ')\n",
    "\n",
    "    for q in range(n_qubits):\n",
    "        qc.ry(np.pi / 4, q)\n",
    "\n",
    "    qc.barrier()\n",
    "\n",
    "    for l in range(n_layers):\n",
    "\n",
    "        for q in range(n_qubits):\n",
    "\n",
    "            if l == 0:\n",
    "                val = theta\n",
    "            elif l == 1:\n",
    "                val = phi\n",
    "            else:\n",
    "                val = np.random.uniform(0, 2 * np.pi)\n",
    "\n",
    "            num = np.random.randint(3)\n",
    "\n",
    "            if l % 3 == 0:\n",
    "                qc.rx(val, q)\n",
    "            elif l % 3 == 1:\n",
    "                qc.ry(val, q)\n",
    "            else:\n",
    "                qc.rz(val, q)\n",
    "\n",
    "        for q in range(0, n_qubits - 1, 2):\n",
    "            qc.cz(q, q + 1)\n",
    "        for q in range(1, n_qubits - 1, 2):\n",
    "            qc.cz(q, q + 1)\n",
    "\n",
    "        qc.barrier()\n",
    "\n",
    "    return qc\n",
    "\n",
    "\n",
    "circ = generate_random_pqc(4, 4)\n",
    "circ.draw('mpl')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3eb12b77becbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:44:14.678199389Z",
     "start_time": "2023-11-02T05:44:14.636350262Z"
    },
    "id": "8ad3eb12b77becbb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_surface(circuit, observable):\n",
    "    Z = []\n",
    "    Z_assembler = []\n",
    "\n",
    "    X = np.arange(-np.pi, np.pi, 0.25)\n",
    "    Y = np.arange(-np.pi, np.pi, 0.25)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    for x in X[0, :]:\n",
    "        for y in Y[:, 0]:\n",
    "            rotations = np.array([x, y])\n",
    "            Z_assembler.append(\n",
    "                estimator.run(circuit, observable,\n",
    "                              rotations).result().values[0])\n",
    "        Z.append(Z_assembler)\n",
    "        Z_assembler = []\n",
    "\n",
    "    Z = np.asarray(Z)\n",
    "    return Z\n",
    "\n",
    "\n",
    "def plot_surface(surface):\n",
    "    X = np.arange(-np.pi, np.pi, 0.25)\n",
    "    Y = np.arange(-np.pi, np.pi, 0.25)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "    surf = ax.plot_surface(X,\n",
    "                           Y,\n",
    "                           surface,\n",
    "                           rstride=1,\n",
    "                           cstride=1,\n",
    "                           cmap=plt.get_cmap('rainbow'),\n",
    "                           linewidth=0,\n",
    "                           antialiased=False,\n",
    "                           alpha=0.6)\n",
    "    ax.set_zlim(-1, 1)\n",
    "    ax.set_xlabel('θ')\n",
    "    ax.set_ylabel('φ')\n",
    "    ax.set_zlabel('Pauli-Z Expectation')\n",
    "    ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "    ax.zaxis.set_major_formatter(FormatStrFormatter(\"%.02f\"))\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d4fd18d66f9279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:44:24.347830715Z",
     "start_time": "2023-11-02T05:44:18.468983254Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 412
    },
    "id": "94d4fd18d66f9279",
    "outputId": "cbb91d73-835b-43be-fd96-a061d1f6f737"
   },
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "n_layers = 50\n",
    "op = SparsePauliOp(['I' * (n_qubits - 2) + 'ZZ'])\n",
    "\n",
    "small_qubit_qc = generate_random_pqc(n_qubits, n_layers)\n",
    "\n",
    "small_global_surface = generate_surface(small_qubit_qc, op)\n",
    "plot_surface(small_global_surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c174dde5111c0274",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:44:50.661258892Z",
     "start_time": "2023-11-02T05:44:28.265553902Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "id": "c174dde5111c0274",
    "outputId": "5ec96690-c1e2-4a62-f2f3-19639af119eb"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "n_qubits = 12\n",
    "n_layers = 50\n",
    "op = SparsePauliOp(['I' * (n_qubits - 2) + 'ZZ'])\n",
    "\n",
    "large_qubit_qc = generate_random_pqc(n_qubits, n_layers)\n",
    "\n",
    "large_global_surface = generate_surface(large_qubit_qc, op)\n",
    "plot_surface(large_global_surface)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f21ba74dc1ceece",
   "metadata": {
    "id": "5f21ba74dc1ceece"
   },
   "source": [
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%202/barren1.gif?raw=1)\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%202/barren2.gif?raw=1)\n",
    "For a 4-qubit circuit, the cost landscape is trainable. However, For a 12-qubit circuit, much of the cost landscape is flattening and becomes difficult to train. This effect will worsen as the number of qubits and layers increases.\n",
    "\n",
    "## Is there something we can do about these barren plateaus?\n",
    "\n",
    "It's a hot topic in current research and there are some proposals to mitigate barren plateaus. First look at how global and local cost functions and the depth of the ansatz influences the barren plateaus. Investigate single layer circuits with various depth using global operators (An operator acting on the entire circuit.) and with local operators. Can you find a scenario without barren plateu? Is it an easy to simulate circuit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d8a124eaeb19c0",
   "metadata": {
    "id": "73d8a124eaeb19c0"
   },
   "outputs": [],
   "source": [
    "#Code me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f404f610afa49e",
   "metadata": {
    "id": "b9f404f610afa49e"
   },
   "source": [
    "## Layerwise Learning\n",
    "\n",
    "The Layerwise learning algorithm proposed by Skolik, A., McClean, J.R., Mohseni, M. et al. Layerwise learning for quantum neural networks. Quantum Mach. Intell. 3, 5 (2021). https://doi.org/10.1007/s42484-020-00036-4 is one of the many attempts to tackle Barren Plateu problem. The traditional way we have seen up until now is called complete depth learning (CDL) where all parameters are trained together. However, in a noisy environment, a single unfavourable update can affect the entire circuit and can trap it within a barren plateau. In layerwise learning, the circuit is trained in two phases:\n",
    "\n",
    "** Phase I **\n",
    "\n",
    "In the initial phase of the algorithm, we build the ansatz by progressively adding. We first have a circuit with s number of layers whose parameters are initialized to zeros. We train this circuit for a fixed number of epochs, after which we add another set of layers and freeze the parameters of the previous layers. The parameters to optimize are dependent on two hyperparameters p and q. The value of p governs the number of layers added in each step, while q determines the layer interval after which the parameters of prior layers are frozen. For instance, with p = 2 and q = 4, two layers are appended in each step, and layers preceding the current one by more than four are frozen.\n",
    "\n",
    "** Phase II **\n",
    "\n",
    "The second phase of the algorithm involves further training of the pre-trained circuit from phase I. Here, larger contiguous partitions of layers are trained simultaneously. A hyperparameter r is introduced to specify the percentage of parameters trained within a single step. We train each partition alternatively until convergence. This way we are training on a larger partition (compared to phase I) at once. By constraining randomness to shallower sub-circuits throughout the entire training process, the algorithm also effectively reduces the likelihood of encountering barren plateaus, which could arise due to stochastic or hardware noise present during the sampling procedure.\n",
    "\n",
    "The following example implementation from [Layerwise learning for Quantum Neural Networks with Qiskit and PyTorch](https://github.com/Gopal-Dahale/ILearnQuantum/tree/main/layerwise_learning_with_qiskit_and_pytorch#layerwise-learning-for-quantum-neural-networks-with-qiskit-and-pytorch) Is rather large, and better run in a PC with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec8edf642b791f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:48:55.382075757Z",
     "start_time": "2023-11-02T05:48:55.057187595Z"
    },
    "id": "51ec8edf642b791f"
   },
   "outputs": [],
   "source": [
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import TwoLocal\n",
    "from qiskit.primitives import Estimator\n",
    "import qiskit_algorithms\n",
    "from qiskit.utils import algorithm_globals\n",
    "\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from qiskit_machine_learning.connectors import TorchConnector\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torch import manual_seed\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import shuffle\n",
    "from time import time\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "seed = 42\n",
    "algorithm_globals.random_seed = seed\n",
    "np.random.seed(seed)\n",
    "manual_seed(seed)\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18b943d271edc7f",
   "metadata": {
    "id": "b18b943d271edc7f"
   },
   "source": [
    "### Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3250a9940af10fcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:48:59.941342726Z",
     "start_time": "2023-11-02T05:48:59.897430337Z"
    },
    "id": "3250a9940af10fcc"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_workers = 2\n",
    "n_train_samples = 200\n",
    "n_test_samples = 2000\n",
    "n_components = 8\n",
    "\n",
    "n_qubits = n_components\n",
    "n_layer_steps = 8\n",
    "n_layers_to_add = 2\n",
    "n_layers_to_train = 2\n",
    "partition_percentage = 0.5\n",
    "n_sweeps = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2952594b78ca6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:49:04.085509175Z",
     "start_time": "2023-11-02T05:49:03.996406185Z"
    },
    "id": "2a2952594b78ca6c"
   },
   "outputs": [],
   "source": [
    "def min_max_scaling(x, new_min, new_max):\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    return (x - x_min) / (x_max - x_min) * (new_max - new_min) + new_min\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f8d4f5069b190b",
   "metadata": {
    "id": "37f8d4f5069b190b"
   },
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4495080736ce2897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:49:14.588506446Z",
     "start_time": "2023-11-02T05:49:07.351933227Z"
    },
    "id": "4495080736ce2897",
    "outputId": "b014c149-67d4-4afe-a732-6060bd6c3ea9"
   },
   "outputs": [],
   "source": [
    "# Train Dataset\n",
    "# -------------\n",
    "\n",
    "# Use pre-defined torchvision function to load MNIST train data\n",
    "X_train = datasets.MNIST(root=\"./data\",\n",
    "                         train=True,\n",
    "                         download=True,\n",
    "                         transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# Filter out labels (originally 0-9), leaving only labels 0 and 1\n",
    "idx = np.append(\n",
    "    np.where(X_train.targets == 0)[0][:n_train_samples // 2],\n",
    "    np.where(X_train.targets == 1)[0][:n_train_samples // 2])\n",
    "\n",
    "X_train.data = X_train.data[idx]\n",
    "X_train.targets = X_train.targets[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb5f81ba241c1bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:49:20.809420206Z",
     "start_time": "2023-11-02T05:49:20.724832750Z"
    },
    "id": "4cb5f81ba241c1bb"
   },
   "outputs": [],
   "source": [
    "# Test Dataset\n",
    "# -------------\n",
    "\n",
    "# Use pre-defined torchvision function to load MNIST test data\n",
    "X_test = datasets.MNIST(root=\"./data\",\n",
    "                        train=False,\n",
    "                        download=True,\n",
    "                        transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "# Filter out labels (originally 0-9), leaving only labels 0 and 1\n",
    "idx = np.append(\n",
    "    np.where(X_test.targets == 0)[0][:n_test_samples // 2],\n",
    "    np.where(X_test.targets == 1)[0][:n_test_samples // 2])\n",
    "\n",
    "X_test.data = X_test.data[idx]\n",
    "X_test.targets = X_test.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511b1657cd1a75c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:49:25.946460175Z",
     "start_time": "2023-11-02T05:49:25.827382669Z"
    },
    "id": "511b1657cd1a75c0",
    "outputId": "03aa7c31-3db8-4b73-93c2-4a8b32234059"
   },
   "outputs": [],
   "source": [
    "x_train = np.float32(X_train.data.numpy()) / 255.\n",
    "y_train = X_train.targets.numpy()\n",
    "\n",
    "x_test = np.float32(X_test.data.numpy()) / 255.\n",
    "y_test = X_test.targets.numpy()\n",
    "\n",
    "x_train, y_train = shuffle(x_train, y_train, random_state=seed)\n",
    "x_test, y_test = shuffle(x_test, y_test, random_state=seed)\n",
    "\n",
    "x_train = x_train.reshape(-1, 28 * 28)\n",
    "x_test = x_test.reshape(-1, 28 * 28)\n",
    "\n",
    "pca = PCA(n_components)\n",
    "\n",
    "x_train = pca.fit_transform(x_train)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_ * 100)[-1]\n",
    "print(\"Cumulative sum on train :\", cumsum)\n",
    "\n",
    "x_test = pca.transform(x_test)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_ * 100)[-1]\n",
    "print(\"Cumulative sum on train :\", cumsum)\n",
    "\n",
    "x_train = min_max_scaling(x_train, 0, 2 * np.pi)\n",
    "x_test = min_max_scaling(x_test, 0, 2 * np.pi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b46b2208ad2285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:49:29.600085136Z",
     "start_time": "2023-11-02T05:49:29.526126044Z"
    },
    "id": "84b46b2208ad2285"
   },
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.Tensor(x_train), torch.Tensor(y_train))\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              shuffle=True,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers)\n",
    "\n",
    "test_dataset = TensorDataset(torch.Tensor(x_test), torch.Tensor(y_test))\n",
    "test_dataloader = DataLoader(test_dataset,\n",
    "                             shuffle=True,\n",
    "                             batch_size=batch_size,\n",
    "                             num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb3fec864fc6ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:49:31.991536479Z",
     "start_time": "2023-11-02T05:49:31.896477716Z"
    },
    "id": "4acb3fec864fc6ec"
   },
   "outputs": [],
   "source": [
    "feature_map = TwoLocal(n_qubits, ['rx'],\n",
    "                       parameter_prefix='x',\n",
    "                       skip_final_rotation_layer=True,\n",
    "                       reps=1).decompose()\n",
    "\n",
    "ansatz = TwoLocal(n_qubits, ['ry', 'rz'],\n",
    "                  'cz',\n",
    "                  skip_final_rotation_layer=True,\n",
    "                  entanglement='linear',\n",
    "                  reps=1).decompose()\n",
    "\n",
    "input_params = feature_map.parameters\n",
    "weight_params = ansatz.parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827b35273da3b66c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:49:35.566664902Z",
     "start_time": "2023-11-02T05:49:35.028734361Z"
    },
    "id": "827b35273da3b66c",
    "outputId": "82f1e31f-3211-4c8d-9224-2cde1445d1fe"
   },
   "outputs": [],
   "source": [
    "temp_qc = feature_map.copy()\n",
    "temp_qc.barrier()\n",
    "temp_qc.compose(ansatz, inplace=True)\n",
    "temp_qc.draw('mpl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d722cd5330506b05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T05:49:47.625514122Z",
     "start_time": "2023-11-02T05:49:47.513311496Z"
    },
    "id": "d722cd5330506b05"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    \"\"\"Feedfoward neural network with 1 hidden layer\"\"\"\n",
    "\n",
    "    def __init__(self, qnn, **kwargs):\n",
    "        super().__init__()\n",
    "        self.qnn = TorchConnector(qnn, **kwargs)\n",
    "        self.loss_func = torch.nn.BCELoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Rescale output to lie between 0 and 1 instead of −1 and 1\n",
    "        return torch.clamp((self.qnn(x) + 1) / 2, min=0.0, max=1.0)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch\n",
    "        out = self(images)  # Generate predictions\n",
    "        loss = self.loss_func(out, labels.view(-1, 1))  # Calculate loss\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbbdcff2d38e90",
   "metadata": {
    "id": "efbbdcff2d38e90"
   },
   "source": [
    "### Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c8451242e941",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T12:24:08.299507024Z",
     "start_time": "2023-11-02T10:13:47.002349953Z"
    },
    "id": "235c8451242e941",
    "outputId": "a513f647-13f0-44c1-eb48-9ca929cefde8"
   },
   "outputs": [],
   "source": [
    "from qiskit_algorithms.gradients import ReverseEstimatorGradient\n",
    "\n",
    "num_weights = len(weight_params)\n",
    "\n",
    "# We use qiskit-qulacs to run qiskit circuits on qulacs backend\n",
    "# This speeds up the training process\n",
    "\n",
    "estimator = Estimator()\n",
    "gradient = ReverseEstimatorGradient(estimator)\n",
    "\n",
    "n_qubits = ansatz.num_qubits\n",
    "main_qc = QuantumCircuit(n_qubits)\n",
    "\n",
    "assert feature_map.num_qubits == ansatz.num_qubits\n",
    "main_qc.compose(feature_map, range(n_qubits), inplace=True)\n",
    "\n",
    "residual_qc = main_qc.copy()  # used in Phase II\n",
    "\n",
    "layer_params = []  # stores the parameters of layers\n",
    "layer_param_values = []  # stores the corresponding parameter values\n",
    "\n",
    "losses = []\n",
    "\n",
    "s = time()\n",
    "\n",
    "for layer_id in range(n_layer_steps):\n",
    "    print(\"\\nLayer:\", layer_id)\n",
    "\n",
    "    qc = QuantumCircuit(n_qubits)\n",
    "    temp_params = []\n",
    "\n",
    "    # Append layers\n",
    "    for i in range(n_layers_to_add):\n",
    "        params = ParameterVector(f\"θ_{layer_id}_{i}\", num_weights)\n",
    "        qc.compose(\n",
    "            ansatz.assign_parameters(dict(zip(weight_params, params))),\n",
    "            range(n_qubits),\n",
    "            inplace=True,\n",
    "        )\n",
    "        temp_params.append(params)\n",
    "\n",
    "    # Parameter and its values for current layer\n",
    "    # we perform a small deviation from all zeros\n",
    "    temp_params = np.asarray(temp_params).flatten()\n",
    "    temp_init = np.zeros(temp_params.shape) + 0.001\n",
    "\n",
    "    main_qc.compose(qc, range(n_qubits), inplace=True)\n",
    "    residual_qc.compose(qc, range(n_qubits), inplace=True)\n",
    "    residual_qc.barrier()\n",
    "\n",
    "    layer_params.append(temp_params)\n",
    "\n",
    "    # Freeze all parameters before `n_layers_to_train`\n",
    "    # To perform the freezing, we bind the parameter values\n",
    "    # and keep only the trainable parameters for `n_layers_to_train` layers\n",
    "    if layer_id >= n_layers_to_train:\n",
    "        freeze_params = layer_params[-n_layers_to_train - 1]\n",
    "        index = num_weights * n_layers_to_add * (n_layers_to_train - 1)\n",
    "        freeze_param_values = layer_param_values[-index - num_weights *\n",
    "                                                 n_layers_to_add:-index]\n",
    "        main_qc = main_qc.bind_parameters(\n",
    "            dict(zip(freeze_params, freeze_param_values)))\n",
    "        initial_point = np.concatenate(\n",
    "            (layer_param_values[-index:], temp_init))\n",
    "        initial_params = np.asarray(\n",
    "            layer_params[-n_layers_to_train:]).flatten()\n",
    "    else:\n",
    "        initial_point = np.concatenate((layer_param_values, temp_init))\n",
    "        initial_params = np.asarray(layer_params).flatten()\n",
    "\n",
    "    qnn = EstimatorQNN(\n",
    "        estimator=estimator,\n",
    "        circuit=main_qc,\n",
    "        input_params=input_params,\n",
    "        weight_params=initial_params,\n",
    "        gradient=gradient,\n",
    "        input_gradients=False,\n",
    "    )\n",
    "\n",
    "    # Model and Training\n",
    "    model = Net(qnn, initial_weights=initial_point)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    epochs = 20  # Set number of epochs\n",
    "    loss_list = []  # Store loss history\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch in train_dataloader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(\n",
    "            100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    losses.append(loss_list)\n",
    "\n",
    "    # Extract the current weights and store them\n",
    "    with torch.no_grad():\n",
    "        weights = [param for param in model.parameters()][0].numpy()\n",
    "\n",
    "    if layer_id >= n_layers_to_train:\n",
    "        layer_param_values = np.concatenate((layer_param_values, temp_init))\n",
    "        index = num_weights * n_layers_to_add * n_layers_to_train\n",
    "        layer_param_values[-index:] = weights.copy()\n",
    "    else:\n",
    "        layer_param_values = weights.copy()\n",
    "\n",
    "e = time()\n",
    "print(\"\\nDuration {:.4f} s\".format(e - s))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181999096a74d7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T13:49:32.221909090Z",
     "start_time": "2023-11-02T13:49:31.990477855Z"
    },
    "id": "5181999096a74d7f",
    "outputId": "7c012f68-6801-4f9c-f415-3680ab02ea6c"
   },
   "outputs": [],
   "source": [
    "# Plot loss convergence\n",
    "plt.plot(np.array(losses).flatten())\n",
    "plt.title(\"Phase I Training Convergence\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.ylabel(\"Binary Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebaaff37d0b3412",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T13:49:38.761870061Z",
     "start_time": "2023-11-02T13:49:36.169647936Z"
    },
    "id": "5ebaaff37d0b3412",
    "outputId": "eb905e8f-db67-4750-a0a8-fdc9fc5f6b23"
   },
   "outputs": [],
   "source": [
    "residual_qc.draw('mpl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d93c9bb75535a44",
   "metadata": {
    "id": "4d93c9bb75535a44"
   },
   "source": [
    "### Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77d2c45e7a64df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-02T18:08:57.854320907Z",
     "start_time": "2023-11-02T15:58:49.702802643Z"
    },
    "id": "4b77d2c45e7a64df",
    "outputId": "fcb912cd-8052-4459-a900-08c7c877f9dd"
   },
   "outputs": [],
   "source": [
    "qc = residual_qc.copy()\n",
    "\n",
    "# Number of weights in the first partition\n",
    "n_p1_weights = int(len(layer_param_values) * partition_percentage)\n",
    "\n",
    "# Partition 1 weights\n",
    "p1_weights = dict(\n",
    "    zip(\n",
    "        np.asarray(layer_params).flatten()[:n_p1_weights],\n",
    "        layer_param_values[:n_p1_weights],\n",
    "    ))\n",
    "\n",
    "# Partition 2 weights\n",
    "p2_weights = dict(\n",
    "    zip(\n",
    "        np.asarray(layer_params).flatten()[n_p1_weights:],\n",
    "        layer_param_values[n_p1_weights:],\n",
    "    ))\n",
    "\n",
    "losses2 = []\n",
    "\n",
    "s = time()\n",
    "\n",
    "print(\"\\nSweep over partitions\\n\")\n",
    "for sweep in range(n_sweeps):\n",
    "    # configure and train first partition\n",
    "    print(\"\\nSweep {}, partition 1\\n\".format(sweep))\n",
    "\n",
    "    # Freeze the partition 2 weights\n",
    "    train_qc = qc.bind_parameters(p2_weights)\n",
    "\n",
    "    # Train only partition 1 weights\n",
    "    qnn = EstimatorQNN(\n",
    "        estimator=estimator,\n",
    "        circuit=train_qc,\n",
    "        input_params=input_params,\n",
    "        weight_params=list(p1_weights.keys()),\n",
    "        gradient=gradient,\n",
    "        input_gradients=True,\n",
    "    )\n",
    "\n",
    "    model = Net(qnn, initial_weights=list(p1_weights.values()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "    epochs = 20  # Set number of epochs\n",
    "    loss_list = []  # Store loss history\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch in train_dataloader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(\n",
    "            100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    losses.append(loss_list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        trained_weights = [param for param in model.parameters()][0].numpy()\n",
    "\n",
    "    p1_weights = {\n",
    "        key: value\n",
    "        for key, value in zip(p1_weights.keys(), trained_weights)\n",
    "    }\n",
    "\n",
    "    # configure and train second partition\n",
    "    print(\"\\nSweep {}, partition 2\\n\".format(sweep))\n",
    "\n",
    "    # Freeze the partition 1 weights\n",
    "    train_qc = qc.bind_parameters(p1_weights)\n",
    "\n",
    "    # Train only partition 2 weights\n",
    "    qnn = EstimatorQNN(\n",
    "        estimator=estimator,\n",
    "        circuit=train_qc,\n",
    "        input_params=input_params,\n",
    "        weight_params=list(p2_weights.keys()),\n",
    "        gradient=gradient,\n",
    "        input_gradients=True,\n",
    "    )\n",
    "\n",
    "    model = Net(qnn, initial_weights=list(p2_weights.values()))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "    epochs = 20  # Set number of epochs\n",
    "    loss_list = []  # Store loss history\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        for batch in train_dataloader:\n",
    "            loss = model.training_step(batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.append(loss.item())  # Store loss\n",
    "        loss_list.append(sum(total_loss) / len(total_loss))\n",
    "        print(\"Training [{:.0f}%]\\tLoss: {:.4f}\".format(\n",
    "            100.0 * (epoch + 1) / epochs, loss_list[-1]))\n",
    "\n",
    "    losses2.append(loss_list)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        trained_weights = [param for param in model.parameters()][0].numpy()\n",
    "\n",
    "    p2_weights = {\n",
    "        key: value\n",
    "        for key, value in zip(p2_weights.keys(), trained_weights)\n",
    "    }\n",
    "\n",
    "e = time()\n",
    "\n",
    "print(\"\\nDuration {:.4f} s\".format(e - s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3b808dc3c03f88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T05:33:31.689785565Z",
     "start_time": "2023-11-03T05:33:31.486921247Z"
    },
    "id": "7f3b808dc3c03f88",
    "outputId": "475e74a2-c85f-4c79-d85a-9c0591d24347"
   },
   "outputs": [],
   "source": [
    "# Plot loss convergence\n",
    "plt.plot(np.array(losses2).flatten())\n",
    "plt.title(\"Phase II Training Convergence\")\n",
    "plt.xlabel(\"Training Iterations\")\n",
    "plt.ylabel(\"Binary Cross Entropy Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa22b61ffa08354",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T05:33:39.828520721Z",
     "start_time": "2023-11-03T05:33:39.488465188Z"
    },
    "id": "6aa22b61ffa08354",
    "outputId": "36c5ef15-d36b-499f-f0a9-a0394f0d986c"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
    "ax[0].plot(np.array(losses).flatten())\n",
    "ax[0].set_xlabel(\"Training Iterations\")\n",
    "ax[0].set_ylabel(\"Binary Cross Entropy Loss\")\n",
    "ax[0].set_title(\"Phase I Training Convergence\")\n",
    "\n",
    "ax[1].plot(np.array(losses2).flatten())\n",
    "ax[1].set_xlabel(\"Training Iterations\")\n",
    "ax[1].set_ylabel(\"Binary Cross Entropy Loss\")\n",
    "ax[1].set_title(\"Phase II Training Convergence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f40ea2d715f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T05:33:42.937876754Z",
     "start_time": "2023-11-03T05:33:42.863843237Z"
    },
    "id": "d03f40ea2d715f"
   },
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    preds = outputs > 0.5\n",
    "    return torch.tensor(\n",
    "        torch.sum(preds == labels.view(-1, 1)).item() / len(preds))\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    batch_accs = [accuracy(model(images), labels) for images, labels in loader]\n",
    "    return torch.stack(batch_accs).mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf694f7d1199aba4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-03T05:34:40.647223515Z",
     "start_time": "2023-11-03T05:33:51.303697338Z"
    },
    "id": "bf694f7d1199aba4",
    "outputId": "5d8f2f86-5d81-4187-eb5c-f312eaea8510"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "model.eval()\n",
    "train_acc = evaluate(model, train_dataloader)\n",
    "test_acc = evaluate(model, test_dataloader)\n",
    "\n",
    "train_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-strategy",
   "metadata": {
    "id": "ideal-strategy"
   },
   "source": [
    "# Training a Quantum Model on a Real Dataset\n",
    "\n",
    "This tutorial will demonstrate how to train a quantum machine learning model to tackle a classification problem. Previous tutorials have featured small, artificial datasets. Here we will increase the problem complexity by considering a real-life classical dataset. We decided to pick a very well-known – albeit still relatively small – problem: the Iris flower dataset. This dataset even has its own Wikipedia [page](https://en.wikipedia.org/wiki/Iris_flower_data_set). Although the Iris dataset is well known to data scientists, we will briefly introduce it to refresh our memories. For comparison, we'll first train a classical counterpart to the quantum model.\n",
    "\n",
    "So, let's get started:\n",
    "\n",
    "- First, we'll load the dataset and explore what it looks like.\n",
    "- Next, we'll train a classical model using [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) from [scikit-learn](https://scikit-learn.org/) to see how well the classification problem can be solved using classical methods.\n",
    "- After that, we'll introduce the Variational Quantum Classifier (VQC).\n",
    "- To conclude, we'll compare the results obtained from both models.\n",
    "\n",
    "## 1. Exploratory Data Analysis\n",
    "\n",
    "First, let us explore the Iris dataset this tutorial will use and see what it contains. For our convenience, this [dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset) is available in scikit-learn and can be loaded easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valued-leeds",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:23:53.462753911Z",
     "start_time": "2023-11-01T18:23:53.427992756Z"
    },
    "id": "valued-leeds"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "billion-advance",
   "metadata": {
    "id": "billion-advance"
   },
   "source": [
    "If no parameters are specified in the `load_iris` function, then a dictionary-like object is returned by scikit-learn. Let's print the description of the dataset and see what is inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-commission",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:24:00.564427414Z",
     "start_time": "2023-11-01T18:24:00.512503715Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "everyday-commission",
    "outputId": "632a8d70-8bc5-47aa-b61a-d05a65bd8520"
   },
   "outputs": [],
   "source": [
    "print(iris_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arctic-girlfriend",
   "metadata": {
    "id": "arctic-girlfriend"
   },
   "source": [
    "There are a few interesting observations we can find from this dataset description:\n",
    "\n",
    "- There are 150 samples (instances) in the dataset.\n",
    "- There are four features (attributes) in each sample.\n",
    "- There are three labels (classes) in the dataset.\n",
    "- The dataset is perfectly balanced, as there are the same number of samples (50) in each class.\n",
    "- We can see features are not normalized, and their value ranges are different, e.g., $[4.3, 7.9]$ and $[0.1, 2.5]$ for sepal length and petal width, respectively. So, transforming the features to the same scale may be helpful.\n",
    "- As stated in the table above, feature-to-class correlation in some cases is very high; this may lead us to think that our model should cope well with the dataset.\n",
    "\n",
    "We only examined the dataset description, but additional properties are available in the `iris_data` object. Now we are going to work with features and labels from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mobile-dictionary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:24:06.421256956Z",
     "start_time": "2023-11-01T18:24:06.363277149Z"
    },
    "id": "mobile-dictionary"
   },
   "outputs": [],
   "source": [
    "features = iris_data.data\n",
    "labels = iris_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-iraqi",
   "metadata": {
    "id": "signed-iraqi"
   },
   "source": [
    "Firstly, we'll normalize the features. Namely, we will apply a simple transformation to represent all features on the same scale. In our case, we squeeze all features onto the interval $[0, 1]$. Normalization is a common technique in machine learning and often leads to better numerical stability and convergence of an algorithm.\n",
    "\n",
    "We can use `MinMaxScaler` from scikit-learn to perform this. Without specifying parameters, this does exactly what is required: maps data onto $[0, 1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-preliminary",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:24:09.018780567Z",
     "start_time": "2023-11-01T18:24:08.962460309Z"
    },
    "id": "alternative-preliminary"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = MinMaxScaler().fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phantom-hollow",
   "metadata": {
    "id": "phantom-hollow"
   },
   "source": [
    "Let's see how our data looks. We plot the features pair-wise to see if there's an observable correlation between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-exhaust",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:24:23.392864054Z",
     "start_time": "2023-11-01T18:24:11.883402849Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "id": "whole-exhaust",
    "outputId": "b1c6e3a2-e824-43d7-8676-2f1053e344fe",
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "df[\"class\"] = pd.Series(iris_data.target)\n",
    "\n",
    "sns.pairplot(df, hue=\"class\", palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-adult",
   "metadata": {
    "id": "quarterly-adult"
   },
   "source": [
    "From the plots, we see that class `0` is easily separable from the other two classes, while classes `1` and `2` are sometimes intertwined, especially regarding the \"sepal width\" feature.\n",
    "\n",
    "Next, let's see how classical machine learning handles this dataset.\n",
    "\n",
    "## 2. Training a Classical Machine Learning Model\n",
    "\n",
    "Before we train a model, we should split the dataset into two parts: a training dataset and a test dataset. We'll use the former to train the model and the latter to verify how well our models perform on unseen data.\n",
    "\n",
    "As usual, we'll ask scikit-learn to do the boring job for us. We'll also fix the seed to ensure the results are reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-survival",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:25:02.079667916Z",
     "start_time": "2023-11-01T18:25:02.052254879Z"
    },
    "id": "pursuant-survival"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "\n",
    "algorithm_globals.random_seed = 123\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, train_size=0.8, random_state=algorithm_globals.random_seed\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-festival",
   "metadata": {
    "id": "close-festival"
   },
   "source": [
    "We train a classical Support Vector Classifier from scikit-learn. For the sake of simplicity, we don't tweak any parameters and rely on the default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proved-reviewer",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:25:05.346323496Z",
     "start_time": "2023-11-01T18:25:05.282263173Z"
    },
    "id": "proved-reviewer"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc = SVC()\n",
    "_ = svc.fit(train_features, train_labels)  # suppress printing the return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-destination",
   "metadata": {
    "id": "earned-destination"
   },
   "source": [
    "Now we check out how well our classical model performs. We will analyze the scores in the conclusion section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "veterinary-proxy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:25:08.843843820Z",
     "start_time": "2023-11-01T18:25:08.681216948Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "veterinary-proxy",
    "outputId": "37871353-c0f4-4afe-9e7d-e52bda085777"
   },
   "outputs": [],
   "source": [
    "train_score_c4 = svc.score(train_features, train_labels)\n",
    "test_score_c4 = svc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Classical SVC on the training dataset: {train_score_c4:.2f}\")\n",
    "print(f\"Classical SVC on the test dataset:     {test_score_c4:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-hybrid",
   "metadata": {
    "id": "limited-hybrid"
   },
   "source": [
    "As can be seen from the scores, the classical SVC algorithm performs very well. Next up, it's time to look at quantum machine learning models.\n",
    "\n",
    "## 3. Training a Quantum Machine Learning Model\n",
    "\n",
    "As an example of a quantum model, we'll train a variational quantum classifier (VQC). The VQC is the simplest classifier available in Qiskit Machine Learning and is a good starting point for newcomers to quantum machine learning who have a background in classical machine learning.\n",
    "\n",
    "But before we train a model, let's examine what comprises the `VQC` class. Two of its central elements are the feature map and ansatz. What these are will now be explained.\n",
    "\n",
    "Our data is classical, meaning it consists of a set of bits, not qubits. We need a way to encode the data as qubits. This process is crucial if we want to obtain an effective quantum model. We usually refer to this mapping as data encoding, data embedding, or data loading and this is the role of the feature map. While feature mapping is a common ML mechanism, this process of loading data into quantum states does not appear in classical machine learning as that only operates in the classical world.\n",
    "\n",
    "Once the data is loaded, we must immediately apply a parameterized quantum circuit. This circuit is a direct analog to the layers in classical neural networks. It has a set of tunable parameters or weights. The weights are optimized such that they minimize an objective function. This objective function characterizes the distance between the predictions and known labeled data. A parameterized quantum circuit is also called a parameterized trial state, variational form, or ansatz. Perhaps, the latter is the most widely used term.\n",
    "\n",
    "For more information, we direct the reader to the [Quantum Machine Learning Course](https://learn.qiskit.org/course/machine-learning).\n",
    "\n",
    "Our choice of feature map will be the ``ZZFeatureMap``. The ``ZZFeatureMap`` is one of the standard feature maps in the Qiskit circuit library. We pass `num_features` as `feature_dimension`, meaning the feature map will have `num_features` or `4` qubits.\n",
    "\n",
    "We decompose the feature map into its constituent gates to give the reader a flavor of how feature maps may look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-pocket",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:25:14.973295358Z",
     "start_time": "2023-11-01T18:25:13.322651130Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "id": "optional-pocket",
    "outputId": "86b91b47-fb58-4353-c6db-443cb1c91394"
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import ZZFeatureMap\n",
    "\n",
    "num_features = features.shape[1]\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
    "feature_map.decompose().draw(output=\"mpl\", fold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-airport",
   "metadata": {
    "id": "noticed-airport"
   },
   "source": [
    "If you look closely at the feature map diagram, you will notice parameters `x[0], ..., x[3]`. These are placeholders for our features.\n",
    "\n",
    "Now we create and plot our ansatz. Pay attention to the repetitive structure of the ansatz circuit. We define the number of these repetitions using the `reps` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elder-interaction",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:25:21.129273531Z",
     "start_time": "2023-11-01T18:25:20.240074274Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "elder-interaction",
    "outputId": "2abd3522-cdac-4ea7-e5be-69223cd7ea16"
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import RealAmplitudes\n",
    "\n",
    "ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
    "ansatz.decompose().draw(output=\"mpl\", fold=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-bumper",
   "metadata": {
    "id": "comic-bumper"
   },
   "source": [
    "This circuit has 16 parameters named `θ[0], ..., θ[15]`. These are the trainable weights of the classifier.\n",
    "\n",
    "We then choose an optimization algorithm to use in the training process. This step is similar to what you may find in classical deep learning frameworks. To make the training process faster, we choose a gradient-free optimizer. You may explore other optimizers available in Qiskit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intimate-doubt",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:25:24.271912068Z",
     "start_time": "2023-11-01T18:25:24.205039600Z"
    },
    "id": "intimate-doubt"
   },
   "outputs": [],
   "source": [
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "\n",
    "optimizer = COBYLA(maxiter=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-compound",
   "metadata": {
    "id": "integral-compound"
   },
   "source": [
    "In the next step, we define where to train our classifier. We can train on a simulator or a real quantum computer. Here, we will use a simulator. We create an instance of the `Sampler` primitive. This is the reference implementation that is statevector based. Using qiskit runtime services you can create a sampler that is backed by a quantum computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unauthorized-footwear",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:25:27.705231520Z",
     "start_time": "2023-11-01T18:25:27.629287241Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "unauthorized-footwear",
    "outputId": "d6a190bc-8bd5-4f5b-d8a3-dc05aef14201"
   },
   "outputs": [],
   "source": [
    "from qiskit.primitives import Sampler\n",
    "\n",
    "sampler = Sampler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-charles",
   "metadata": {
    "id": "seeing-charles"
   },
   "source": [
    "We will add a callback function called `callback_graph`. `VQC` will call this function for each evaluation of the objective function with two parameters: the current weights and the value of the objective function at those weights. Our callback will append the value of the objective function to an array so we can plot the iteration versus the objective function value. The callback will update the plot at each iteration. Note that you can do whatever you want inside a callback function, so long as it has the two-parameter signature we mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "connected-reach",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:25:30.694665167Z",
     "start_time": "2023-11-01T18:25:30.499969293Z"
    },
    "id": "connected-reach"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "objective_func_vals = []\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "\n",
    "def callback_graph(weights, obj_func_eval):\n",
    "    clear_output(wait=True)\n",
    "    objective_func_vals.append(obj_func_eval)\n",
    "    plt.title(\"Objective function value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "freelance-wesley",
   "metadata": {
    "id": "freelance-wesley"
   },
   "source": [
    "Now we are ready to construct the classifier and fit it.\n",
    "\n",
    "`VQC` stands for \"variational quantum classifier.\" It takes a feature map and an ansatz and constructs a quantum neural network automatically. In the simplest case it is enough to pass the number of qubits and a quantum instance to construct a valid classifier. You may omit the `sampler` parameter, in this case a `Sampler` instance will be created for you in the way we created it earlier. We created it manually for illustrative purposes only.\n",
    "\n",
    "Training may take some time. Please, be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-garbage",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "multiple-garbage",
    "outputId": "3d04bc50-9467-41ab-c840-c34249f7250d"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from qiskit_machine_learning.algorithms.classifiers import VQC\n",
    "\n",
    "vqc = VQC(\n",
    "    sampler=sampler,\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz,\n",
    "    optimizer=optimizer,\n",
    "    callback=callback_graph,\n",
    ")\n",
    "\n",
    "# clear objective value history\n",
    "objective_func_vals = []\n",
    "\n",
    "start = time.time()\n",
    "vqc.fit(train_features, train_labels)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Training time: {round(elapsed)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-regulation",
   "metadata": {
    "id": "laughing-regulation"
   },
   "source": [
    "Let's see how the quantum model performs on the real-life dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-mineral",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:28:12.187905146Z",
     "start_time": "2023-11-01T18:28:10.771579018Z"
    },
    "id": "formed-mineral",
    "outputId": "f5dfd5a6-fd0c-43a7-a17b-0cd287d40b48"
   },
   "outputs": [],
   "source": [
    "train_score_q4 = vqc.score(train_features, train_labels)\n",
    "test_score_q4 = vqc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Quantum VQC on the training dataset: {train_score_q4:.2f}\")\n",
    "print(f\"Quantum VQC on the test dataset:     {test_score_q4:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-deviation",
   "metadata": {
    "id": "minimal-deviation"
   },
   "source": [
    "As we can see, the scores are high, and the model can be used to predict labels on unseen data.\n",
    "\n",
    "Now let's see what we can tune to get even better models.\n",
    "\n",
    "- The key components are the feature map and the ansatz. You can tweak parameters. In our case, you may change the `reps` parameter that specifies how repetitions of a gate pattern we add to the circuit. Larger values lead to more entanglement operations and more parameters. Thus, the model can be more flexible, but the higher number of parameters also adds complexity, and training such a model usually takes more time. Furthermore, we may end up overfitting the model. You can try the other feature maps and ansatzes available in the [Qiskit circuit library](https://qiskit.org/documentation/apidoc/circuit_library.html#n-local-circuits), or you can come up with custom circuits.\n",
    "- You may try other optimizers. Qiskit contains a bunch of them. Some of them are gradient-free, others not. If you choose a gradient-based optimizer, e.g., `L_BFGS_B`, expect the training time to increase. Additionally to the objective function, these optimizers must evaluate the gradient with respect to the training parameters, which leads to an increased number of circuit executions per iteration.\n",
    "- Another option is to randomly (or deterministically) sample `initial_point` and fit the model several times.\n",
    "\n",
    "But what if a dataset contains more features than a modern quantum computer can handle? Recall, in this example, we had the same number of qubits as the number of features in the dataset, but this may not always be the case.\n",
    "\n",
    "## 4. Reducing the Number of Features\n",
    "\n",
    "In this section, we reduce the number of features in our dataset and train our models again. We'll move through faster this time as the steps are the same except for the first, where we apply a PCA transformation.\n",
    "\n",
    "We transform our four features into two features only. This dimensionality reduction is for educational purposes only. As you saw in the previous section, we can train a quantum model using all four features from the dataset.\n",
    "\n",
    "Now, we can easily plot these two features on a single figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-montreal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:28:17.728613321Z",
     "start_time": "2023-11-01T18:28:16.775806614Z"
    },
    "id": "painted-montreal",
    "outputId": "e67b9ecc-4e6e-433e-a709-68b27f91cd1f"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features = PCA(n_components=2).fit_transform(features)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (6, 6)\n",
    "sns.scatterplot(x=features[:, 0], y=features[:, 1], hue=labels, palette=\"tab10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modular-hometown",
   "metadata": {
    "id": "modular-hometown"
   },
   "source": [
    "As usual, we split the dataset first, then fit a classical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-agriculture",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:28:22.774272717Z",
     "start_time": "2023-11-01T18:28:22.540643111Z"
    },
    "id": "naval-agriculture",
    "outputId": "5bbf9cbb-e9b8-467c-9171-03a6e4027b09"
   },
   "outputs": [],
   "source": [
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    features, labels, train_size=0.8, random_state=algorithm_globals.random_seed\n",
    ")\n",
    "\n",
    "svc.fit(train_features, train_labels)\n",
    "\n",
    "train_score_c2 = svc.score(train_features, train_labels)\n",
    "test_score_c2 = svc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Classical SVC on the training dataset: {train_score_c2:.2f}\")\n",
    "print(f\"Classical SVC on the test dataset:     {test_score_c2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-subcommittee",
   "metadata": {
    "id": "chemical-subcommittee"
   },
   "source": [
    "The results are still good but slightly worse compared to the initial version. Let's see how a quantum model deals with them. As we now have two qubits, we must recreate the feature map and ansatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-novel",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:28:26.730749233Z",
     "start_time": "2023-11-01T18:28:26.642048645Z"
    },
    "id": "electric-novel"
   },
   "outputs": [],
   "source": [
    "num_features = features.shape[1]\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
    "ansatz = RealAmplitudes(num_qubits=num_features, reps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-johnston",
   "metadata": {
    "id": "competent-johnston"
   },
   "source": [
    "We also reduce the maximum number of iterations we run the optimization process for, as we expect it to converge faster because we now have fewer qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-louisiana",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:28:30.712349964Z",
     "start_time": "2023-11-01T18:28:30.604896360Z"
    },
    "id": "younger-louisiana"
   },
   "outputs": [],
   "source": [
    "optimizer = COBYLA(maxiter=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-cookbook",
   "metadata": {
    "id": "proprietary-cookbook"
   },
   "source": [
    "Now we construct a quantum classifier from the new parameters and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-capital",
   "metadata": {
    "id": "varied-capital",
    "outputId": "7ab502ca-1b8e-4e87-f96d-10bac30fe8e3"
   },
   "outputs": [],
   "source": [
    "vqc = VQC(\n",
    "    sampler=sampler,\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz,\n",
    "    optimizer=optimizer,\n",
    "    callback=callback_graph,\n",
    ")\n",
    "\n",
    "# clear objective value history\n",
    "objective_func_vals = []\n",
    "\n",
    "# make the objective function plot look nicer.\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "vqc.fit(train_features, train_labels)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Training time: {round(elapsed)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developmental-crazy",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-01T18:29:25.700167116Z",
     "start_time": "2023-11-01T18:29:24.983595229Z"
    },
    "id": "developmental-crazy",
    "outputId": "f0f99f7f-7349-471c-a5c6-783fa7652932"
   },
   "outputs": [],
   "source": [
    "train_score_q2_ra = vqc.score(train_features, train_labels)\n",
    "test_score_q2_ra = vqc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Quantum VQC on the training dataset using RealAmplitudes: {train_score_q2_ra:.2f}\")\n",
    "print(f\"Quantum VQC on the test dataset using RealAmplitudes:     {test_score_q2_ra:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-singing",
   "metadata": {
    "id": "quarterly-singing"
   },
   "source": [
    "Well, the scores are higher than a fair coin toss but could be better. The objective function is almost flat towards the end, meaning increasing the number of iterations won't help, and model performance will stay the same. Let's see what we can do with another ansatz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-seven",
   "metadata": {
    "id": "convinced-seven",
    "outputId": "09bab61d-29ab-44ce-e932-8cccf4b0417e"
   },
   "outputs": [],
   "source": [
    "from qiskit.circuit.library import EfficientSU2\n",
    "\n",
    "ansatz = EfficientSU2(num_qubits=num_features, reps=3)\n",
    "optimizer = COBYLA(maxiter=40)\n",
    "\n",
    "vqc = VQC(\n",
    "    sampler=sampler,\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz,\n",
    "    optimizer=optimizer,\n",
    "    callback=callback_graph,\n",
    ")\n",
    "\n",
    "# clear objective value history\n",
    "objective_func_vals = []\n",
    "\n",
    "start = time.time()\n",
    "vqc.fit(train_features, train_labels)\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Training time: {round(elapsed)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-reverse",
   "metadata": {
    "id": "painted-reverse"
   },
   "outputs": [],
   "source": [
    "train_score_q2_eff = vqc.score(train_features, train_labels)\n",
    "test_score_q2_eff = vqc.score(test_features, test_labels)\n",
    "\n",
    "print(f\"Quantum VQC on the training dataset using EfficientSU2: {train_score_q2_eff:.2f}\")\n",
    "print(f\"Quantum VQC on the test dataset using EfficientSU2:     {test_score_q2_eff:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-norway",
   "metadata": {
    "id": "alike-norway"
   },
   "source": [
    "The scores are better than in the previous setup. Perhaps if we had used more iterations, we could do even better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fluid-truck",
   "metadata": {
    "id": "fluid-truck"
   },
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "In this tutorial, we have built two classical and three quantum machine learning models. Let's print an overall table with our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-snake",
   "metadata": {
    "id": "educated-snake"
   },
   "outputs": [],
   "source": [
    "print(f\"Model                           | Test Score | Train Score\")\n",
    "print(f\"SVC, 4 features                 | {train_score_c4:10.2f} | {test_score_c4:10.2f}\")\n",
    "print(f\"VQC, 4 features, RealAmplitudes | {train_score_q4:10.2f} | {test_score_q4:10.2f}\")\n",
    "print(f\"----------------------------------------------------------\")\n",
    "print(f\"SVC, 2 features                 | {train_score_c2:10.2f} | {test_score_c2:10.2f}\")\n",
    "print(f\"VQC, 2 features, RealAmplitudes | {train_score_q2_ra:10.2f} | {test_score_q2_ra:10.2f}\")\n",
    "print(f\"VQC, 2 features, EfficientSU2   | {train_score_q2_eff:10.2f} | {test_score_q2_eff:10.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moderate-nashville",
   "metadata": {
    "id": "moderate-nashville"
   },
   "source": [
    "Unsurprisingly, the classical models perform better than their quantum counterparts, but classical ML has come a long way, and quantum ML has yet to reach that level of maturity. As we can see,  we achieved the best results using a classical support vector machine. But the quantum model trained on four features was also quite good. When we reduced the number of features, the performance of all models went down as expected. So, if resources permit training a model on a full-featured dataset without any reduction, you should train such a model. If not, you may expect to compromise between dataset size, training time, and score.\n",
    "\n",
    "Another observation is that even a simple ansatz change can lead to better results. The two-feature model with the `EfficientSU2` ansatz performs better than the one with `RealAmplitudes`. That means the choice of hyperparameters plays the same critical role in quantum ML as in classical ML, and searching for optimal hyperparameters may take a long time. You may apply the same techniques we use in classical ML, such as random/grid or more sophisticated approaches.\n",
    "\n",
    "### Do it after\n",
    "\n",
    "Scikit learn offers tools for [generated datasets](https://scikit-learn.org/stable/datasets/sample_generators.html). Use VQC and VQR with generated regression and classification datasets in a python script. [Can you use other providers?](https://qiskit.org/providers). Can you implement Layerwise learning?   "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "e2eee1ec3b7b75618be3bcd737c6b000914c302a788483aeea47c6724501a27e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
