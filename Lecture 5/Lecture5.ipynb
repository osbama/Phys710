{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9279519965c57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:12.952310040Z",
     "start_time": "2023-11-22T16:40:12.496637214Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7a9279519965c57",
    "outputId": "99d52960-c46e-440d-e0bd-fcfcdd1461b1"
   },
   "outputs": [],
   "source": [
    "!pip install qiskit-algorithms\n",
    "!pip install qiskit-machine-learning\n",
    "!pip install qiskit-Aer\n",
    "!pip install qiskit-qulacs\n",
    "!pip install matplotlib\n",
    "!pip install pylatexenc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed021640",
   "metadata": {
    "collapsed": false,
    "id": "ed021640",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# The Quantum Convolution Neural Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc439a",
   "metadata": {
    "id": "16dc439a"
   },
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In this lecture we will talk about Quantum Convolutional neural networks introduced in [1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52dabaed6ec3d7f",
   "metadata": {
    "collapsed": false,
    "id": "e52dabaed6ec3d7f",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 2. What are convolutional neural networks?\n",
    "\n",
    "Neural networks are a subset of machine learning, and they are at the heart of deep learning algorithms. They are comprised of node layers, containing an input layer, one or more hidden layers, and an output layer. Each node connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.\n",
    "\n",
    "Convolutional neural networks (ConvNets or CNNs) are more often utilized for classification and computer vision tasks. Prior to CNNs, manual, time-consuming feature extraction methods were used to identify objects in images. However, convolutional neural networks now provide a more scalable approach to image classification and object recognition tasks, leveraging principles from linear algebra, specifically matrix multiplication, to identify patterns within an image.\n",
    "\n",
    "### 2.1 What is convolution ? (discrete)\n",
    "\n",
    "For complex-valued functions $f$, $g$ defined on the set $Z$ of integers, the discrete convolution of $f$ and $g$ is given by:\n",
    "\n",
    "$$\n",
    "(f*g)[n]=\\sum _{m=-\\infty }^{\\infty }f[m]g[n-m],\n",
    "$$\n",
    "\n",
    "or equivalently (see commutativity) by:\n",
    "\n",
    "$$\n",
    "(f*g)[n]=\\sum _{m=-\\infty }^{\\infty }f[n-m]g[m].\n",
    "$$\n",
    "\n",
    "The convolution of two finite sequences is defined by extending the sequences to finitely supported functions on the set of integers. When the sequences are the coefficients of two polynomials, then the coefficients of the ordinary product of the two polynomials are the convolution of the original two sequences. This is known as the Cauchy product of the coefficients of the sequences.\n",
    "\n",
    "Thus when $g$ has finite support in the set ${\\displaystyle (f*g)[n]=\\sum _{m=-\\infty }^{\\infty }f[n-m]g[m].}$  (representing, for instance, a finite impulse response), a finite summation may be used:\n",
    "$$\n",
    "(f*g)[n]=\\sum _{m=-M}^{M}f[n-m]g[m].\n",
    "$$\n",
    "\n",
    "\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%205/2D_Convolution_Animation.gif?raw=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec88bf340a64c3f7",
   "metadata": {
    "collapsed": false,
    "id": "ec88bf340a64c3f7",
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### 2.2 How do convolutional neural networks work?\n",
    "\n",
    "\n",
    "\n",
    "Convolutional neural networks are distinguished from other neural networks by their superior performance with image, speech, or audio signal inputs. They have three main types of layers, which are:\n",
    "\n",
    "    Convolutional layer\n",
    "    Pooling layer\n",
    "    Fully-connected (FC) layer\n",
    "\n",
    "The convolutional layer is the first layer of a convolutional network. While convolutional layers can be followed by additional convolutional layers or pooling layers, the fully-connected layer is the final layer. With each layer, the CNN increases in its complexity, identifying greater portions of the image. Earlier layers focus on simple features, such as colors and edges. As the image data progresses through the layers of the CNN, it starts to recognize larger elements or shapes of the object until it finally identifies the intended object.\n",
    "\n",
    "####  Convolutional layer\n",
    "\n",
    "The convolutional layer is the core building block of a CNN, and it is where the majority of computation occurs. It requires a few components, which are input data, a filter, and a feature map. Let’s assume that the input will be a color image, which is made up of a matrix of pixels in 3D. This means that the input will have three dimensions—a height, width, and depth—which correspond to RGB in an image. We also have a feature detector, also known as a kernel or a filter, which will move across the receptive fields of the image, checking if the feature is present. This process is known as a convolution.\n",
    "\n",
    "The feature detector is a two-dimensional (2-D) array of weights, which represents part of the image. While they can vary in size, the filter size is typically a 3x3 matrix; this also determines the size of the receptive field. The filter is then applied to an area of the image, and a dot product is calculated between the input pixels and the filter. This dot product is then fed into an output array. Afterwards, the filter shifts by a stride, repeating the process until the kernel has swept across the entire image. The final output from the series of dot products from the input and the filter is known as a feature map, activation map, or a convolved feature.\n",
    "\n",
    "Note that the weights in the feature detector remain fixed as it moves across the image, which is also known as parameter sharing. Some parameters, like the weight values, adjust during training through the process of backpropagation and gradient descent. However, there are three hyperparameters which affect the volume size of the output that need to be set before the training of the neural network begins. These include:\n",
    "\n",
    "1. The number of filters affects the depth of the output. For example, three distinct filters would yield three different feature maps, creating a depth of three.\n",
    "\n",
    "2. Stride is the distance, or number of pixels, that the kernel moves over the input matrix. While stride values of two or greater is rare, a larger stride yields a smaller output.\n",
    "\n",
    "3. Zero-padding is usually used when the filters do not fit the input image. This sets all elements that fall outside of the input matrix to zero, producing a larger or equally sized output. There are three types of padding:\n",
    "\n",
    "    Valid padding: This is also known as no padding. In this case, the last convolution is dropped if dimensions do not align.\n",
    "    Same padding: This padding ensures that the output layer has the same size as the input layer\n",
    "    Full padding: This type of padding increases the size of the output by adding zeros to the border of the input.\n",
    "\n",
    "After each convolution operation, a CNN applies a Rectified Linear Unit (ReLU) transformation to the feature map, introducing nonlinearity to the model.\n",
    "\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%205/iclh-diagram-convolutional-neural-networks.png?raw=1)\n",
    "\n",
    "#### Additional convolutional layer\n",
    "\n",
    "As we mentioned earlier, another convolution layer can follow the initial convolution layer. When this happens, the structure of the CNN can become hierarchical as the later layers can see the pixels within the receptive fields of prior layers.  As an example, let’s assume that we’re trying to determine if an image contains a bicycle. You can think of the bicycle as a sum of parts. It is comprised of a frame, handlebars, wheels, pedals, et cetera. Each individual part of the bicycle makes up a lower-level pattern in the neural net, and the combination of its parts represents a higher-level pattern, creating a feature hierarchy within the CNN. Ultimately, the convolutional layer converts the image into numerical values, allowing the neural network to interpret and extract relevant patterns.\n",
    "\n",
    "![](https://github.com/osbama/Phys710/blob/master/Lecture%205/hierarchy.png?raw=1)\n",
    "\n",
    "#### Pooling layer\n",
    "\n",
    "Pooling layers, also known as downsampling, conducts dimensionality reduction, reducing the number of parameters in the input. Similar to the convolutional layer, the pooling operation sweeps a filter across the entire input, but the difference is that this filter does not have any weights. Instead, the kernel applies an aggregation function to the values within the receptive field, populating the output array. There are two main types of pooling:\n",
    "\n",
    "    Max pooling: As the filter moves across the input, it selects the pixel with the maximum value to send to the output array. As an aside, this approach tends to be used more often compared to average pooling.\n",
    "    Average pooling: As the filter moves across the input, it calculates the average value within the receptive field to send to the output array.\n",
    "\n",
    "While a lot of information is lost in the pooling layer, it also has a number of benefits to the CNN. They help to reduce complexity, improve efficiency, and limit risk of overfitting.\n",
    "\n",
    "#### Fully-connected layer\n",
    "\n",
    "The name of the full-connected layer aptly describes itself. As mentioned earlier, the pixel values of the input image are not directly connected to the output layer in partially connected layers. However, in the fully-connected layer, each node in the output layer connects directly to a node in the previous layer.\n",
    "\n",
    "This layer performs the task of classification based on the features extracted through the previous layers and their different filters. While convolutional and pooling layers tend to use ReLu functions, FC layers usually leverage a softmax activation function to classify inputs appropriately, producing a probability from 0 to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceca583",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:13.240273871Z",
     "start_time": "2023-11-22T16:40:12.954691064Z"
    },
    "id": "3ceca583",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "from qiskit import QuantumCircuit\n",
    "from qiskit.circuit import ParameterVector\n",
    "from qiskit.circuit.library import ZFeatureMap\n",
    "from qiskit.quantum_info import SparsePauliOp\n",
    "from qiskit_algorithms.optimizers import COBYLA\n",
    "from qiskit_algorithms.utils import algorithm_globals\n",
    "from qiskit_machine_learning.algorithms.classifiers import NeuralNetworkClassifier\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "algorithm_globals.random_seed = 12345"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8875c12",
   "metadata": {
    "id": "a8875c12"
   },
   "source": [
    "## 3. Differences between a QCNN and CCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a65761",
   "metadata": {
    "id": "e3a65761"
   },
   "source": [
    "### 3.1 Classical Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d397710c",
   "metadata": {
    "id": "d397710c"
   },
   "source": [
    "Classical Convolutional Neural Networks (CCNNs) are a subclass of artificial neural networks which have the ability to determine particular features and patterns of a given input. Because of this, they are commonly used in image recognition and audio processing.\n",
    "\n",
    "The capability of determining features is a result of the two types of layers used in a CCNN, the convolutional layer and pooling layer.\n",
    "\n",
    "An example of a CCNN can be seen in Figure 1, where a CCNN is trained to determine whether an input image either contains a cat or a dog. To do so, the input image passes through a series of alternating convolutional (C) and pooling layers (P), all of which detect patterns and associate each pattern to a cat or a dog. The fully connected layer (FC) provides us with an output which allows us to determine whether the input image was a cat or dog.\n",
    "\n",
    "The convolutional layer makes  use of a kernel, which can determine features and patterns of a particular input. An example of this is feature detection in an image, where different layers detect particular patterns in the input image. This is demonstrated in Figure 1, where the $l^{th}$ layer recognizes features and patterns along the $ij$ plane. It can then associate such features with a given output in the training process, and can use this process to train the dataset.\n",
    "\n",
    "On the other hand, a pooling layer reduces the dimensionality of the input data, reducing the computational cost and amount of learning parameters in the CCNN. A schematic of a CCNN can be seen below.\n",
    "\n",
    "For further information on CCNN, see [2]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18d774",
   "metadata": {
    "id": "1f18d774"
   },
   "source": [
    "![CCNN.png](https://github.com/osbama/Phys710/blob/master/Lecture%205/CCNN.png?raw=1)\n",
    "Figure 1. A schematic demonstration of the use of a CCNN to classify between images of a cat and dog. Here, we see the several convolutional and pooling layers being applied, all of which are decreasing in dimensionality due to the use of the pooling layers. The output of the CCNN determines whether the input image was a cat or dog. Image obtained form [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18ecb7",
   "metadata": {
    "id": "7b18ecb7"
   },
   "source": [
    "### 3.2 Quantum Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15711fe7",
   "metadata": {
    "id": "15711fe7"
   },
   "source": [
    "Quantum Convolutional Neural Networks (QCNN) behave in a similar manner to CCNNs. First, we encode our pixelated image into a quantum circuit using a given feature map, such Qiskit's ZFeatureMap or ZZFeatureMap or others available in the circuit library.\n",
    "\n",
    "After encoding our image, we apply alternating convolutional and pooling layers, as defined in the next section. By applying these alternating layers, we reduce the dimensionality of our circuit until we are left with one qubit. We can then classify our input image by measuring the output of this one remaining qubit.\n",
    "\n",
    "The Quantum Convolutional Layer will consist of a series of two qubit unitary operators, which recognize and determine relationships between the qubits in our circuit. This unitary gates are defined below in the next section.\n",
    "\n",
    "For the Quantum Pooling Layer, we cannot do the same as is done classically to reduce the dimension, i.e. the number of qubits in our circuit. Instead, we reduce the number of qubits by performing operations upon each until a specific point and then disregard certain qubits in a specific layer. It is these layers where we stop performing operations on certain qubits that we call our 'pooling layer'. Details of the pooling layer is discussed further in the next section.\n",
    "\n",
    "In the QCNN, each layer contains parametrized circuits, meaning we alter our output result by adjusting the parameters of each layer. When training our QCNN, it is these parameters that are adjusted to reduce the loss function of our QCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894b249a",
   "metadata": {
    "id": "894b249a"
   },
   "source": [
    "A simple example of four qubit QCNN can be seen below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2777d65",
   "metadata": {
    "id": "c2777d65"
   },
   "source": [
    "![QCNN.png](https://github.com/osbama/Phys710/blob/master/Lecture%205/QCNN.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159a69bf",
   "metadata": {
    "id": "159a69bf"
   },
   "source": [
    "\n",
    "Figure 2: Example QCNN containing four qubits. The first Convolutional Layer acts on all the qubits. This is followed by the first pooling layer, which reduces the dimensionality of the QCNN from four qubits to two qubits by disregarding the first two. The second Convolutional layer then detects features between the two qubits still in use in the QCNN, followed by another pooling layer, which reduces the dimensionality from two qubits to one, which will be our output qubit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb2541e",
   "metadata": {
    "id": "cdb2541e"
   },
   "source": [
    "## 4. Components of a QCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5c01c6",
   "metadata": {
    "id": "6f5c01c6"
   },
   "source": [
    "As discussed in Section 1 of this tutorial, a CCNN will contain both convolutional and pooling layers. Here, we define these layers for the QCNN in terms of gates applied to a Quantum Circuit and demonstrate an example for each layer for 4 qubits.\n",
    "\n",
    "Each of these layers will contain parameters which are tuned throughout the training process to minimize the loss function and train the QCNN to classify between horizontal and vertical lines.\n",
    "\n",
    "In theory, one could apply any parametrized circuit for both the convolutional and pooling layers of our network. For example the Gellmann Matrices (which are the three dimensional generalization of the Pauli Matrices) are used as generators for each unitary gate acting on a pair of qubits.\n",
    "\n",
    "Here, we take a different approach and form our parametrized circuit based on the two qubit unitary as proposed in [2]. This states that every unitary matrix in $U(4)$ can be decomposed such that\n",
    "\n",
    "$$U = (A_1 \\otimes A_2) \\cdot N(\\alpha, \\beta, \\gamma) \\cdot (A_3 \\otimes A_4)$$\n",
    "\n",
    "where $A_j \\in \\text{SU}(2)$, $\\otimes$ is the tensor product, and $N(\\alpha, \\beta, \\gamma) = exp(i[\\alpha \\sigma_x\\sigma_x + \\beta \\sigma_y\\sigma_y + \\gamma \\sigma_z\\sigma_z ])$, where $\\alpha, \\beta, \\gamma$ are the parameters that we can adjust.\n",
    "\n",
    "From this, it is evident that each unitary depends on 15 parameters and implies that in order for the QCNN to be able to span the whole Hilbert space, each unitary in our QCNN must contain 15 parameters each.\n",
    "\n",
    "Tuning this large amount of parameters would be difficult and would lead to long training times. To overcome this problem, we restrict our ansatz to a particular subspace of the Hilbert space and define the two qubit unitary gate as $N(\\alpha, \\beta, \\gamma)$. These two qubit unitaries, as seen in [2] can be seen below and are applied to all neighboring qubits each of the layers in the QCNN.\n",
    "\n",
    "Note that by only using $N(\\alpha, \\beta, \\gamma)$ as our two qubit unitary for the parametrized layers, we are restricting our QCNN to a particular subspace, one in which the optimal solution may not be contained in and reducing the accuracy of the QCNN. For the purpose of this tutorial, we will use this parametrized circuit to decrease the training time of our QCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b474663",
   "metadata": {
    "id": "8b474663"
   },
   "source": [
    "![QCNN2.png](https://github.com/osbama/Phys710/blob/master/Lecture%205/QCNN2.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d16e806",
   "metadata": {
    "id": "9d16e806"
   },
   "source": [
    "Figure 3:\n",
    "Parametrized two qubit unitary circuit for $N(\\alpha, \\beta, \\gamma) = exp(i[\\alpha \\sigma_x\\sigma_x + \\beta \\sigma_y\\sigma_y + \\gamma \\sigma_z\\sigma_z ])$ as seen in [2], where $\\alpha =  \\frac{\\pi}{2} - 2\\theta$, $\\beta = 2\\phi - \\frac{\\pi}{2}$ and $\\gamma =  \\frac{\\pi}{2} - 2\\lambda$ as seen in the circuit. This two qubit unitary will be applied to all neighboring qubits in our feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4972aa4",
   "metadata": {
    "id": "d4972aa4"
   },
   "source": [
    "### 4.1 Convolutional Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7332be0a",
   "metadata": {
    "id": "7332be0a"
   },
   "source": [
    "The next step in this tutorial is to define the Convolutional Layers of our QCNN. These layers are then applied to the qubits after the data has been encoded through use of the feature map.\n",
    "\n",
    "To do so we first need to determine a parametrized unitary gate, which will be used to create our convolutional and pooling layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809524ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:13.556120180Z",
     "start_time": "2023-11-22T16:40:13.243848498Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "809524ce",
    "outputId": "3ca5fb20-248d-440d-d49d-0d2ed32431e5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# We now define a two qubit unitary as defined in [2]\n",
    "def conv_circuit(params):\n",
    "    target = QuantumCircuit(2)\n",
    "    target.rz(-np.pi / 2, 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(params[0], 0)\n",
    "    target.ry(params[1], 1)\n",
    "    target.cx(0, 1)\n",
    "    target.ry(params[2], 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(np.pi / 2, 0)\n",
    "    return target\n",
    "\n",
    "\n",
    "# Let's draw this circuit and see what it looks like\n",
    "params = ParameterVector(\"θ\", length=3)\n",
    "circuit = conv_circuit(params)\n",
    "circuit.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1b7140",
   "metadata": {
    "id": "6c1b7140"
   },
   "source": [
    "Now that we have defined these unitaries, it is time to create a function for the convolutional layer in our QCNN. To do so, we apply the two qubit unitary to neighboring qubits as seen in the ``conv_layer`` function below.\n",
    "\n",
    "Note that we first apply the two qubit unitary to all even pairs of qubits followed by applying to odd pairs of qubits in a circular coupling manner, i.e. the as well as neighboring qubits being coupled, the first and final qubit are also coupled through a unitary gate.\n",
    "\n",
    "Note that we add barriers into our quantum circuits for convenience when plotting, however they are not required for the actual QCNN and can be extracted from the following circuits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68562ff2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:13.773953250Z",
     "start_time": "2023-11-22T16:40:13.552465266Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 420
    },
    "id": "68562ff2",
    "outputId": "19bccce9-3ceb-47b4-f26b-8c80b36faff1"
   },
   "outputs": [],
   "source": [
    "def conv_layer(num_qubits, param_prefix):\n",
    "    qc = QuantumCircuit(num_qubits, name=\"Convolutional Layer\")\n",
    "    qubits = list(range(num_qubits))\n",
    "    param_index = 0\n",
    "    params = ParameterVector(param_prefix, length=num_qubits * 3)\n",
    "    for q1, q2 in zip(qubits[0::2], qubits[1::2]):\n",
    "        qc = qc.compose(conv_circuit(params[param_index : (param_index + 3)]), [q1, q2])\n",
    "        qc.barrier()\n",
    "        param_index += 3\n",
    "    for q1, q2 in zip(qubits[1::2], qubits[2::2] + [0]):\n",
    "        qc = qc.compose(conv_circuit(params[param_index : (param_index + 3)]), [q1, q2])\n",
    "        qc.barrier()\n",
    "        param_index += 3\n",
    "\n",
    "    qc_inst = qc.to_instruction()\n",
    "\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qc.append(qc_inst, qubits)\n",
    "    return qc\n",
    "\n",
    "\n",
    "circuit = conv_layer(4, \"θ\")\n",
    "circuit.decompose().draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59677b5",
   "metadata": {
    "id": "f59677b5"
   },
   "source": [
    "### 4.2 Pooling Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23856709",
   "metadata": {
    "id": "23856709"
   },
   "source": [
    "The purpose of a pooling layer is to reduce the dimensions of our Quantum Circuit, i.e. reduce the number of qubits in our circuit, while retaining as much information as possible from previously learned data. Reducing the amount of qubits also reduces the computational cost of the overall circuit, as the number of parameters that the QCNN needs to learn decreases.\n",
    "\n",
    "However, one cannot simply decrease the amount of qubits in our quantum circuit. Because of this, we must define the pooling layer in a different manner compared with the classical approach.\n",
    "\n",
    "To 'artificially' reduce the number of qubits in our circuit, we first begin by creating pairs of the $N$ qubits in our system.\n",
    "\n",
    "After initially pairing all the qubits, we apply our generalized 2 qubit unitary to each pair, as described previously. After applying this two qubit unitary, we then ignore one qubit from each pair of qubits for the remainder of the neural network.\n",
    "\n",
    "This layer therefore has the overall effect of 'combining' the information of the two qubits into one qubit by first applying the unitary circuit, encoding information from one qubit into another, before disregarding one of qubits for the remainder of the circuit and not performing any operations or measurements on it.\n",
    "\n",
    "We note that one could also apply a dynamic circuit to reduce the dimensionality in the pooling layers. This would involve performing measurements on certain qubits in the circuit and having an intermediate classical feedback loop in our pooling layers. By applying these measurements, one would also be reducing the dimensionality of the circuit.\n",
    "\n",
    "In this tutorial, we apply the former approach, and disregard qubits in each pooling layer. Using this approach, we thus create a QCNN Pooling Layer which transforms the dimensions of our $N$ qubit Quantum Circuit to $N/2$.\n",
    "\n",
    "To do so, we first define a two qubit unitary, which transforms the two qubit system to one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c742cc9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:13.933253090Z",
     "start_time": "2023-11-22T16:40:13.778544493Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "id": "3c742cc9",
    "outputId": "39c7a013-9cda-4bc2-99fb-ba9d060c3d90",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pool_circuit(params):\n",
    "    target = QuantumCircuit(2)\n",
    "    target.rz(-np.pi / 2, 1)\n",
    "    target.cx(1, 0)\n",
    "    target.rz(params[0], 0)\n",
    "    target.ry(params[1], 1)\n",
    "    target.cx(0, 1)\n",
    "    target.ry(params[2], 1)\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "params = ParameterVector(\"θ\", length=3)\n",
    "circuit = pool_circuit(params)\n",
    "circuit.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ff63b",
   "metadata": {
    "id": "8b9ff63b"
   },
   "source": [
    "After applying this two qubit unitary circuit, we neglect the first qubit (q0) in future layers and only use the second qubit (q1) in our QCNN\n",
    "\n",
    "We apply this two qubit pooling layer to different pairs of qubits to create our pooling layer for N qubits. As an example we then plot it for four qubits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b37f922",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:14.028917334Z",
     "start_time": "2023-11-22T16:40:13.860755534Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "8b37f922",
    "outputId": "e8059759-0c64-4fdf-9129-928b3c10a3f6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def pool_layer(sources, sinks, param_prefix):\n",
    "    num_qubits = len(sources) + len(sinks)\n",
    "    qc = QuantumCircuit(num_qubits, name=\"Pooling Layer\")\n",
    "    param_index = 0\n",
    "    params = ParameterVector(param_prefix, length=num_qubits // 2 * 3)\n",
    "    for source, sink in zip(sources, sinks):\n",
    "        qc = qc.compose(pool_circuit(params[param_index : (param_index + 3)]), [source, sink])\n",
    "        qc.barrier()\n",
    "        param_index += 3\n",
    "\n",
    "    qc_inst = qc.to_instruction()\n",
    "\n",
    "    qc = QuantumCircuit(num_qubits)\n",
    "    qc.append(qc_inst, range(num_qubits))\n",
    "    return qc\n",
    "\n",
    "\n",
    "sources = [0, 1]\n",
    "sinks = [2, 3]\n",
    "circuit = pool_layer(sources, sinks, \"θ\")\n",
    "circuit.decompose().draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafef540",
   "metadata": {
    "id": "bafef540"
   },
   "source": [
    "In this particular example, we reduce the dimensionality of our four qubit circuit to the last two qubits, i.e. the last two qubits in this particular example. These qubits are then used in the next layer, while the first two are neglected for the remainder of the QCNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d0497d",
   "metadata": {
    "id": "03d0497d"
   },
   "source": [
    "## 5. Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3fbdc",
   "metadata": {
    "id": "88c3fbdc"
   },
   "source": [
    "One common use of a CCNN is an image classifier, where a CCNN detects particular features and patterns (such as straight lines or curves) of the pixelated images through the use of the feature maps in the convolutional layer. By learning the relationship between these features, it can then classify and label handwritten digits with ease.\n",
    "\n",
    "Because of a classical CNN's ability to recognize features and patterns easily, we will train our QCNN to also determine patterns and features of a given set of pixelated images, and classify between two different patterns.\n",
    "\n",
    "To simplify the dataset, we only consider 2 x 4 pixelated images. The patterns we will train the QCNN to distinguish will be a horizontal or vertical line, which can be placed anywhere in the image, alongside a noisy background.\n",
    "\n",
    "We first begin by generating this dataset. To create a 'horizontal' or 'vertical' line, we assign pixels value to be $\\frac{\\pi}{2}$ which will represent the line in our pixelated image. We create a noisy background by assigning every other pixel a random value between $0$ and $\\frac{\\pi}{4}$ which will create a noisy background.\n",
    "\n",
    "Note that when we create our dataset, we need to split it into the training set and testing set of images, the datasets we train and test our neural network respectively.\n",
    "\n",
    "We also need to label our datasets such that the QCNN can learn to differentiate between the two patterns. In this example we label images with a horizontal line with -1 and images with a vertical line +1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a674ebf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:14.034385431Z",
     "start_time": "2023-11-22T16:40:14.032417893Z"
    },
    "id": "3a674ebf"
   },
   "outputs": [],
   "source": [
    "def generate_dataset(num_images):\n",
    "    images = []\n",
    "    labels = []\n",
    "    hor_array = np.zeros((6, 8))\n",
    "    ver_array = np.zeros((4, 8))\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0, 7):\n",
    "        if i != 3:\n",
    "            hor_array[j][i] = np.pi / 2\n",
    "            hor_array[j][i + 1] = np.pi / 2\n",
    "            j += 1\n",
    "\n",
    "    j = 0\n",
    "    for i in range(0, 4):\n",
    "        ver_array[j][i] = np.pi / 2\n",
    "        ver_array[j][i + 4] = np.pi / 2\n",
    "        j += 1\n",
    "\n",
    "    for n in range(num_images):\n",
    "        rng = algorithm_globals.random.integers(0, 2)\n",
    "        if rng == 0:\n",
    "            labels.append(-1)\n",
    "            random_image = algorithm_globals.random.integers(0, 6)\n",
    "            images.append(np.array(hor_array[random_image]))\n",
    "        elif rng == 1:\n",
    "            labels.append(1)\n",
    "            random_image = algorithm_globals.random.integers(0, 4)\n",
    "            images.append(np.array(ver_array[random_image]))\n",
    "\n",
    "        # Create noise\n",
    "        for i in range(8):\n",
    "            if images[-1][i] == 0:\n",
    "                images[-1][i] = algorithm_globals.random.uniform(0, np.pi / 4)\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3ca82",
   "metadata": {
    "id": "a5b3ca82"
   },
   "source": [
    "Let's now create our dataset below and split it into our test and training datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1828c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:14.090510756Z",
     "start_time": "2023-11-22T16:40:14.034971753Z"
    },
    "id": "ed1828c5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "images, labels = generate_dataset(50)\n",
    "\n",
    "train_images, test_images, train_labels, test_labels = train_test_split(\n",
    "    images, labels, test_size=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f6952d",
   "metadata": {
    "id": "e6f6952d"
   },
   "source": [
    "Let's see some examples in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afeaa5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:14.151933913Z",
     "start_time": "2023-11-22T16:40:14.053930522Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "0afeaa5f",
    "outputId": "ccd4a715-4f86-468d-ab83-6b616bd0b0b8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(10, 6), subplot_kw={\"xticks\": [], \"yticks\": []})\n",
    "for i in range(4):\n",
    "    ax[i // 2, i % 2].imshow(\n",
    "        train_images[i].reshape(2, 4),  # Change back to 2 by 4\n",
    "        aspect=\"equal\",\n",
    "    )\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.025)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a67058",
   "metadata": {
    "id": "85a67058"
   },
   "source": [
    "As we can see each image contains either a vertical or horizontal line, that the QCNN will learn how to differentiate. Now that we have built our dataset, it is time to discuss the components of the QCNN and build our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed5d2d6",
   "metadata": {
    "id": "eed5d2d6"
   },
   "source": [
    "## 6. Modeling our QCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64efb8d8",
   "metadata": {
    "id": "64efb8d8"
   },
   "source": [
    "Now that we have defined both the convolutional layers it is now time to build our QCNN, which will consist of alternating pooling and convolutional layers.\n",
    "\n",
    "As the images in our dataset contains 8 pixels, we will use 8 qubits in our QCNN.\n",
    "\n",
    "We encode our dataset into our QCNN by applying a feature map. One can create a feature map using one of Qiskit's built in feature maps, such as ZFeatureMap or ZZFeatureMap.\n",
    "\n",
    "After analyzing several different Feature maps for this dataset, it was found that QCNN obtains the greatest accuracy when the Z feature map is used. Therefore, throughout the remainder of the tutorial we will use the Z feature Map, of which can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840db7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:14.467592838Z",
     "start_time": "2023-11-22T16:40:14.160736105Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 577
    },
    "id": "0840db7b",
    "outputId": "5befe9cc-ee39-4abd-b396-20c88d8a1453"
   },
   "outputs": [],
   "source": [
    "feature_map = ZFeatureMap(8)\n",
    "feature_map.decompose().draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee9549c",
   "metadata": {
    "id": "cee9549c"
   },
   "source": [
    "We create a function for our QCNN, which will contain three sets of alternating convolutional and pooling layers, which can be seen in the schematic below. Through the use of the pooling layers, we thus reduce the dimensionality of our QCNN from eight qubits to one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ddcb80",
   "metadata": {
    "id": "83ddcb80"
   },
   "source": [
    "![QCNN3.png](https://github.com/osbama/Phys710/blob/master/Lecture%205/QCNN3.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e31982",
   "metadata": {
    "id": "e7e31982"
   },
   "source": [
    "To classify our image dataset of horizontal and vertical lines, we measure the expectation value of the Pauli Z operator of the final qubit. Based on the obtained value being +1 or -1, we can conclude that the input image contained either a horizontal or vertical line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930ed08",
   "metadata": {
    "id": "e930ed08"
   },
   "source": [
    "## 7. Training our QCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04e424",
   "metadata": {
    "id": "4e04e424"
   },
   "source": [
    "The next step is to build our model using our training data.\n",
    "\n",
    "To classify our system, we perform a measurement from the output circuit. The value we obtain will thus classify whether our input data contains either a vertical line or horizontal line.\n",
    "\n",
    "The measurement we have chosen in this tutorial is $<Z>$, i.e. the expectation value of the Pauli Z qubit for the final qubit. Measuring this expectation value, we obtain +1 or -1, which correspond to a vertical or horizontal line respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc478975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:14.467806321Z",
     "start_time": "2023-11-22T16:40:14.442339606Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cc478975",
    "outputId": "0d10b76e-3a86-403f-9589-c2dde96bdf9d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_map = ZFeatureMap(8)\n",
    "\n",
    "ansatz = QuantumCircuit(8, name=\"Ansatz\")\n",
    "\n",
    "# First Convolutional Layer\n",
    "ansatz.compose(conv_layer(8, \"с1\"), list(range(8)), inplace=True)\n",
    "\n",
    "# First Pooling Layer\n",
    "ansatz.compose(pool_layer([0, 1, 2, 3], [4, 5, 6, 7], \"p1\"), list(range(8)), inplace=True)\n",
    "\n",
    "# Second Convolutional Layer\n",
    "ansatz.compose(conv_layer(4, \"c2\"), list(range(4, 8)), inplace=True)\n",
    "\n",
    "# Second Pooling Layer\n",
    "ansatz.compose(pool_layer([0, 1], [2, 3], \"p2\"), list(range(4, 8)), inplace=True)\n",
    "\n",
    "# Third Convolutional Layer\n",
    "ansatz.compose(conv_layer(2, \"c3\"), list(range(6, 8)), inplace=True)\n",
    "\n",
    "# Third Pooling Layer\n",
    "ansatz.compose(pool_layer([0], [1], \"p3\"), list(range(6, 8)), inplace=True)\n",
    "\n",
    "# Combining the feature map and ansatz\n",
    "circuit = QuantumCircuit(8)\n",
    "circuit.compose(feature_map, range(8), inplace=True)\n",
    "circuit.compose(ansatz, range(8), inplace=True)\n",
    "\n",
    "observable = SparsePauliOp.from_list([(\"Z\" + \"I\" * 7, 1)])\n",
    "\n",
    "# we decompose the circuit for the QNN to avoid additional data copying\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=circuit.decompose(),\n",
    "    observables=observable,\n",
    "    input_params=feature_map.parameters,\n",
    "    weight_params=ansatz.parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6b6e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:14.665133769Z",
     "start_time": "2023-11-22T16:40:14.442514511Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 753
    },
    "id": "a4f6b6e7",
    "outputId": "e1b483ca-0f88-4c61-9ccd-4c4465d1a588"
   },
   "outputs": [],
   "source": [
    "circuit.draw(\"mpl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ec49c",
   "metadata": {
    "id": "db7ec49c"
   },
   "source": [
    "We will also define a callback function to use when training our model. This allows us to view and plot the loss function per each iteration in our training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97cc662",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:14.672305752Z",
     "start_time": "2023-11-22T16:40:14.666414189Z"
    },
    "id": "d97cc662",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def callback_graph(weights, obj_func_eval):\n",
    "    clear_output(wait=True)\n",
    "    objective_func_vals.append(obj_func_eval)\n",
    "    plt.title(\"Objective function value against iteration\")\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Objective function value\")\n",
    "    plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371b5a7",
   "metadata": {
    "id": "4371b5a7"
   },
   "source": [
    "In this example, we will use the COBYLA optimizer to train our classifier, which is a numerical optimization method commonly used for classification machine learning algorithms.\n",
    "\n",
    "We then place the the callback function, optimizer and operator of our QCNN created above into Qiskit Machine Learning's built in Neural Network Classifier, which we can then use to train our model.\n",
    "\n",
    "Since model training may take a long time we have already pre-trained the model for some iterations and saved the pre-trained weights. We'll continue training from that point by setting `initial_point` to a vector of pre-trained weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39Gb3BGYTJdm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "39Gb3BGYTJdm",
    "outputId": "6dd2330d-1ce9-4017-85f9-fd6c56009be7"
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "from google.colab import files\n",
    "try :\n",
    "  uploaded = files.view(qcnn_initial_point.json)\n",
    "except :\n",
    "  HTML(\"\"\"\n",
    "    <div class=\"alert\">\n",
    "      <p>Please upload the qcnn_initial_point.json from github to instance if you are using colab</p>\n",
    "    </div>\n",
    "\n",
    "    <style>\n",
    "    .alert {\n",
    "      padding: 20px;\n",
    "      background-color: #f44336;\n",
    "      color: white;\n",
    "      margin-bottom: 15px;\n",
    "    }\n",
    "    </style>\n",
    "    \"\"\")\n",
    "finally:\n",
    "  print(\"qcnn_initial_point.json successfully\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2949fc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:40:14.681220030Z",
     "start_time": "2023-11-22T16:40:14.669048135Z"
    },
    "id": "f2949fc6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(\"qcnn_initial_point.json\", \"r\") as f:\n",
    "    initial_point = json.load(f)\n",
    "\n",
    "classifier = NeuralNetworkClassifier(\n",
    "    qnn,\n",
    "    optimizer=COBYLA(maxiter=200),  # Set max iterations here\n",
    "    callback=callback_graph,\n",
    "    initial_point=initial_point,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9061806",
   "metadata": {
    "id": "c9061806"
   },
   "source": [
    "After creating this classifier, we can train our QCNN using our training dataset and each image's corresponding label. Because we previously defined the callback function, we plot the overall loss of our system per iteration.\n",
    "\n",
    "It may take some time to train the QCNN so be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0219ff4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "0219ff4a",
    "outputId": "badac0e2-d0b9-4478-dfad-71ffa758cf0e"
   },
   "outputs": [],
   "source": [
    "x = np.asarray(train_images)\n",
    "y = np.asarray(train_labels)\n",
    "\n",
    "objective_func_vals = []\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "classifier.fit(x, y)\n",
    "\n",
    "# score classifier\n",
    "print(f\"Accuracy from the train data : {np.round(100 * classifier.score(x, y), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95d1100",
   "metadata": {
    "id": "e95d1100"
   },
   "source": [
    "As we can see from above, the QCNN converges slowly, hence our `initial_point` was already close to an optimal solution. The next step is to determine whether our QCNN can classify data seen in our test image data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b3cf40",
   "metadata": {
    "id": "72b3cf40"
   },
   "source": [
    "## 8. Testing our QCNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571b1b32",
   "metadata": {
    "id": "571b1b32"
   },
   "source": [
    "After building and training our dataset we now test whether our QCNN can predict images that are not from our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a34ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-22T16:41:42.714530964Z",
     "start_time": "2023-11-22T16:41:42.217115788Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "7f2a34ae",
    "outputId": "96d0f374-fbad-4654-e54f-766ae3acd1f1",
    "tags": [
     "nbsphinx-thumbnail"
    ]
   },
   "outputs": [],
   "source": [
    "y_predict = classifier.predict(test_images)\n",
    "x = np.asarray(test_images)\n",
    "y = np.asarray(test_labels)\n",
    "print(f\"Accuracy from the test data : {np.round(100 * classifier.score(x, y), 2)}%\")\n",
    "\n",
    "# Let's see some examples in our dataset\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 6), subplot_kw={\"xticks\": [], \"yticks\": []})\n",
    "for i in range(0, 4):\n",
    "    ax[i // 2, i % 2].imshow(test_images[i].reshape(2, 4), aspect=\"equal\")\n",
    "    if y_predict[i] == -1:\n",
    "        ax[i // 2, i % 2].set_title(\"The QCNN predicts this is a Horizontal Line\")\n",
    "    if y_predict[i] == +1:\n",
    "        ax[i // 2, i % 2].set_title(\"The QCNN predicts this is a Vertical Line\")\n",
    "plt.subplots_adjust(wspace=0.1, hspace=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f296307",
   "metadata": {
    "id": "7f296307"
   },
   "source": [
    "From above, we can indeed see that our QCNN can classify horizontal and vertical lines! Congratulations! Through the use of quantum circuits and quantum convolutional and pooling layers, you have built a Quantum Convolutional Neural Network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1d8370",
   "metadata": {
    "id": "4a1d8370"
   },
   "source": [
    "## 9. References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16174c0",
   "metadata": {
    "id": "a16174c0"
   },
   "source": [
    "[1] Cong, I., Choi, S. & Lukin, M.D. Quantum convolutional neural networks. Nat. Phys. 15, 1273–1278 (2019). https://doi.org/10.1038/s41567-019-0648-8\n",
    "\n",
    "[2] Vatan, Farrokh, and Colin Williams. \"Optimal quantum circuits for general two-qubit gates.\" Physical Review A 69.3 (2004): 032315."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
